	.file	"dirac_wilson.cpp"
	.section	.text._ZN5DiracD2Ev,"axG",@progbits,_ZN5DiracD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN5DiracD2Ev
	.type	_ZN5DiracD2Ev, @function
_ZN5DiracD2Ev:
.LFB2646:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	ret
	.cfi_endproc
.LFE2646:
	.size	_ZN5DiracD2Ev, .-_ZN5DiracD2Ev
	.section	.text._ZN15DiracWilsonLikeD2Ev,"axG",@progbits,_ZN15DiracWilsonLikeD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN15DiracWilsonLikeD2Ev
	.type	_ZN15DiracWilsonLikeD2Ev, @function
_ZN15DiracWilsonLikeD2Ev:
.LFB2650:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	ret
	.cfi_endproc
.LFE2650:
	.size	_ZN15DiracWilsonLikeD2Ev, .-_ZN15DiracWilsonLikeD2Ev
	.section	.text._ZN12Dirac_WilsonD2Ev,"axG",@progbits,_ZN12Dirac_WilsonD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN12Dirac_WilsonD2Ev
	.type	_ZN12Dirac_WilsonD2Ev, @function
_ZN12Dirac_WilsonD2Ev:
.LFB2703:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	ret
	.cfi_endproc
.LFE2703:
	.size	_ZN12Dirac_WilsonD2Ev, .-_ZN12Dirac_WilsonD2Ev
	.section	.text._ZNK12Dirac_Wilson5fsizeEv,"axG",@progbits,_ZNK12Dirac_Wilson5fsizeEv,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson5fsizeEv
	.type	_ZNK12Dirac_Wilson5fsizeEv, @function
_ZNK12Dirac_Wilson5fsizeEv:
.LFB2706:
	.cfi_startproc
	movq	80(%rdi), %rax
	ret
	.cfi_endproc
.LFE2706:
	.size	_ZNK12Dirac_Wilson5fsizeEv, .-_ZNK12Dirac_Wilson5fsizeEv
	.section	.text._ZNK12Dirac_Wilson5gsizeEv,"axG",@progbits,_ZNK12Dirac_Wilson5gsizeEv,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson5gsizeEv
	.type	_ZNK12Dirac_Wilson5gsizeEv, @function
_ZNK12Dirac_Wilson5gsizeEv:
.LFB2707:
	.cfi_startproc
	movq	88(%rdi), %rax
	ret
	.cfi_endproc
.LFE2707:
	.size	_ZNK12Dirac_Wilson5gsizeEv, .-_ZNK12Dirac_Wilson5gsizeEv
	.section	.text._ZNK12Dirac_Wilson17get_fermionFormatEv,"axG",@progbits,_ZNK12Dirac_Wilson17get_fermionFormatEv,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson17get_fermionFormatEv
	.type	_ZNK12Dirac_Wilson17get_fermionFormatEv, @function
_ZNK12Dirac_Wilson17get_fermionFormatEv:
.LFB2718:
	.cfi_startproc
	movq	36(%rsi), %rdx
	movq	%rdi, %rax
	movq	%rdx, (%rdi)
	movq	44(%rsi), %rdx
	movq	%rdx, 8(%rdi)
	movl	52(%rsi), %edx
	movl	%edx, 16(%rdi)
	ret
	.cfi_endproc
.LFE2718:
	.size	_ZNK12Dirac_Wilson17get_fermionFormatEv, .-_ZNK12Dirac_Wilson17get_fermionFormatEv
	.section	.text._ZN12Dirac_Wilson21update_internal_stateEv,"axG",@progbits,_ZN12Dirac_Wilson21update_internal_stateEv,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN12Dirac_Wilson21update_internal_stateEv
	.type	_ZN12Dirac_Wilson21update_internal_stateEv, @function
_ZN12Dirac_Wilson21update_internal_stateEv:
.LFB2719:
	.cfi_startproc
	ret
	.cfi_endproc
.LFE2719:
	.size	_ZN12Dirac_Wilson21update_internal_stateEv, .-_ZN12Dirac_Wilson21update_internal_stateEv
	.section	.text._ZN12Dirac_WilsonD0Ev,"axG",@progbits,_ZN12Dirac_WilsonD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN12Dirac_WilsonD0Ev
	.type	_ZN12Dirac_WilsonD0Ev, @function
_ZN12Dirac_WilsonD0Ev:
.LFB2705:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	jmp	_ZdlPv
	.cfi_endproc
.LFE2705:
	.size	_ZN12Dirac_WilsonD0Ev, .-_ZN12Dirac_WilsonD0Ev
	.section	.text._ZN15DiracWilsonLikeD0Ev,"axG",@progbits,_ZN15DiracWilsonLikeD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN15DiracWilsonLikeD0Ev
	.type	_ZN15DiracWilsonLikeD0Ev, @function
_ZN15DiracWilsonLikeD0Ev:
.LFB2652:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	jmp	_ZdlPv
	.cfi_endproc
.LFE2652:
	.size	_ZN15DiracWilsonLikeD0Ev, .-_ZN15DiracWilsonLikeD0Ev
	.section	.text._ZN5DiracD0Ev,"axG",@progbits,_ZN5DiracD5Ev,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZN5DiracD0Ev
	.type	_ZN5DiracD0Ev, @function
_ZN5DiracD0Ev:
.LFB2648:
	.cfi_startproc
	movq	$_ZTV5Dirac+16, (%rdi)
	jmp	_ZdlPv
	.cfi_endproc
.LFE2648:
	.size	_ZN5DiracD0Ev, .-_ZN5DiracD0Ev
	.text
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson8mult_dagERK5Field
	.type	_ZNK12Dirac_Wilson8mult_dagERK5Field, @function
_ZNK12Dirac_Wilson8mult_dagERK5Field:
.LFB2825:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2825
	movq	%rbx, -32(%rsp)
	movq	%rbp, -24(%rsp)
	movq	%rsi, %rbx
	.cfi_offset 6, -32
	.cfi_offset 3, -40
	movq	%r12, -16(%rsp)
	movq	%r13, -8(%rsp)
	subq	$72, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset 13, -16
	.cfi_offset 12, -24
	movq	(%rsi), %rax
	movq	%rdi, %r12
	movq	%rsp, %rdi
	movq	40(%rax), %r13
	movq	120(%rax), %rbp
.LEHB0:
	call	*%rbp
.LEHE0:
	leaq	16(%rsp), %rdi
	movq	%rsp, %rdx
	movq	%rbx, %rsi
.LEHB1:
	call	*%r13
.LEHE1:
	leaq	16(%rsp), %rdx
	movq	%rbx, %rsi
	movq	%r12, %rdi
.LEHB2:
	call	*%rbp
.LEHE2:
	movq	24(%rsp), %rdi
	call	_ZdlPv
	movq	8(%rsp), %rdi
	call	_ZdlPv
	movq	%r12, %rax
	movq	40(%rsp), %rbx
	movq	48(%rsp), %rbp
	movq	56(%rsp), %r12
	movq	64(%rsp), %r13
	addq	$72, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L37:
	.cfi_restore_state
	movq	%rax, %rbx
.L34:
	movq	8(%rsp), %rdi
	call	_ZdlPv
	movq	%rbx, %rdi
.LEHB3:
	call	_Unwind_Resume
.LEHE3:
.L38:
	movq	24(%rsp), %rdi
	movq	%rax, %rbx
	call	_ZdlPv
	jmp	.L34
	.cfi_endproc
.LFE2825:
	.globl	__gxx_personality_v0
	.section	.gcc_except_table,"a",@progbits
.LLSDA2825:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2825-.LLSDACSB2825
.LLSDACSB2825:
	.uleb128 .LEHB0-.LFB2825
	.uleb128 .LEHE0-.LEHB0
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB1-.LFB2825
	.uleb128 .LEHE1-.LEHB1
	.uleb128 .L37-.LFB2825
	.uleb128 0
	.uleb128 .LEHB2-.LFB2825
	.uleb128 .LEHE2-.LEHB2
	.uleb128 .L38-.LFB2825
	.uleb128 0
	.uleb128 .LEHB3-.LFB2825
	.uleb128 .LEHE3-.LEHB3
	.uleb128 0
	.uleb128 0
.LLSDACSE2825:
	.text
	.size	_ZNK12Dirac_Wilson8mult_dagERK5Field, .-_ZNK12Dirac_Wilson8mult_dagERK5Field
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson9get_gsiteEv
	.type	_ZNK12Dirac_Wilson9get_gsiteEv, @function
_ZNK12Dirac_Wilson9get_gsiteEv:
.LFB2829:
	.cfi_startproc
	movq	%rbx, -24(%rsp)
	movq	%rbp, -16(%rsp)
	movq	%rdi, %rbx
	.cfi_offset 6, -24
	.cfi_offset 3, -32
	movq	%r12, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -16
	call	_ZN9SiteIndex8instanceEv
	movq	_ZN9SiteIndex6gsite_E+8(%rip), %rax
	subq	_ZN9SiteIndex6gsite_E(%rip), %rax
	xorl	%r12d, %r12d
	xorl	%ebp, %ebp
	movq	$0, (%rbx)
	movq	$0, 8(%rbx)
	movq	$0, 16(%rbx)
	sarq	$2, %rax
	testq	%rax, %rax
	je	.L41
	movabsq	$4611686018427387903, %rdx
	cmpq	%rdx, %rax
	ja	.L46
	leaq	0(,%rax,4), %r12
	movq	%r12, %rdi
	call	_Znwm
	movq	%rax, %rbp
.L41:
	movq	%rbp, (%rbx)
	movq	%rbp, 8(%rbx)
	addq	%rbp, %r12
	movq	_ZN9SiteIndex6gsite_E(%rip), %rsi
	movq	_ZN9SiteIndex6gsite_E+8(%rip), %rax
	movq	%r12, 16(%rbx)
	xorl	%r12d, %r12d
	subq	%rsi, %rax
	sarq	$2, %rax
	testq	%rax, %rax
	je	.L43
	leaq	0(,%rax,4), %r12
	movq	%rbp, %rdi
	movq	%r12, %rdx
	call	memmove
.L43:
	addq	%r12, %rbp
	movq	%rbx, %rax
	movq	16(%rsp), %r12
	movq	%rbp, 8(%rbx)
	movq	(%rsp), %rbx
	movq	8(%rsp), %rbp
	addq	$24, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 8
	ret
.L46:
	.cfi_restore_state
	call	_ZSt17__throw_bad_allocv
	.cfi_endproc
.LFE2829:
	.size	_ZNK12Dirac_Wilson9get_gsiteEv, .-_ZNK12Dirac_Wilson9get_gsiteEv
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_:
.LFB2818:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2818
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	movq	%rdx, %r15
	.cfi_offset 15, -24
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	subq	$344, %rsp
	movq	%rsi, -328(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -332(%rbp)
	movq	96(%rdi), %rax
	movq	(%rax), %rdx
	movq	80(%rdx), %rcx
	movq	%rcx, -352(%rbp)
	movq	72(%rdx), %rcx
	subq	%rcx, -352(%rbp)
	sarq	$2, -352(%rbp)
	movl	-352(%rbp), %edx
	movl	%edx, -336(%rbp)
	movq	136(%rdi), %rdx
	movq	(%rdx), %rdx
	movq	72(%rdx), %rcx
	movq	80(%rdx), %rdx
	movq	%rdx, -368(%rbp)
	movl	-332(%rbp), %edx
	imull	-336(%rbp), %edx
	movq	%rcx, -360(%rbp)
	movl	%edx, -372(%rbp)
	movslq	%edx, %rdx
	leaq	46(,%rdx,8), %rdx
	movq	%rdx, -384(%rbp)
	andq	$-16, %rdx
	subq	%rdx, %rsp
	movl	-336(%rbp), %edx
	leaq	31(%rsp), %r12
	andq	$-32, %r12
	testl	%edx, %edx
	jle	.L48
	movl	-352(%rbp), %edx
	movq	$0, -216(%rbp)
	xorl	%r14d, %r14d
	subl	$1, %edx
	leaq	4(,%rdx,4), %rdx
	movq	%rdx, -344(%rbp)
	jmp	.L70
	.p2align 4,,10
	.p2align 3
.L104:
	movq	96(%rbx), %rax
.L70:
	movq	(%rax), %rax
	movq	-216(%rbp), %rcx
	movq	%rbx, %rdi
	movl	52(%rbx), %edx
	addq	184(%rbx), %rdi
	movq	72(%rax), %rax
	movl	(%rax,%rcx), %r13d
	movq	8(%r15), %rax
	imull	%r13d, %edx
	movslq	%edx, %rcx
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	1(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -224(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm9
	leal	6(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm9, -232(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm11
	leal	7(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm11, -240(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	2(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -256(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm13
	leal	3(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm13, -264(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm15
	leal	8(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm15, -272(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	9(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -280(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm9
	leal	4(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm9, -288(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm11
	leal	5(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm11, -296(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	10(%rdx), %ecx
	addl	$11, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	(%rax,%rdx,8), %xmm15
	vmovsd	(%rax,%rcx,8), %xmm13
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -304(%rbp)
	vmovsd	%xmm13, -312(%rbp)
	testb	$1, %al
	vmovsd	%xmm15, -320(%rbp)
	je	.L50
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L50:
	movl	%r13d, %esi
.LEHB4:
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L51
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L51:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	addl	$1, %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L52
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L52:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	addl	$2, %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L53
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L53:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L54
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L54:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	addl	$4, %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L55
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L55:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L57
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L57:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L58
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L58:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	1(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L59
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L59:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	2(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L60
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L60:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	3(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L61
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L61:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	4(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L62
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L62:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movq	%rbx, %rdi
	movl	60(%rbx), %esi
	addq	184(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	5(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L64
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L64:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L65
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L65:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	1(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L66
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L66:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	2(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L67
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L67:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	3(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L68
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L68:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	4(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L69
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L69:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	vmovsd	-224(%rbp), %xmm9
	vmovsd	-232(%rbp), %xmm12
	leal	(%rdx,%rdx,2), %edx
	vmovsd	-240(%rbp), %xmm13
	vaddsd	%xmm9, %xmm9, %xmm11
	vmovsd	-256(%rbp), %xmm15
	addl	%eax, %edx
	movq	8(%rcx), %rax
	vaddsd	%xmm12, %xmm12, %xmm10
	imull	68(%rbx), %edx
	vaddsd	%xmm13, %xmm13, %xmm9
	vmovsd	-272(%rbp), %xmm12
	vmovsd	-280(%rbp), %xmm13
	vaddsd	%xmm15, %xmm15, %xmm8
	vaddsd	%xmm12, %xmm12, %xmm6
	vmovsd	-288(%rbp), %xmm15
	vaddsd	%xmm13, %xmm13, %xmm5
	vmovsd	-304(%rbp), %xmm12
	vmovsd	-312(%rbp), %xmm13
	vaddsd	%xmm15, %xmm15, %xmm4
	leal	5(%rdx,%rsi,4), %edx
	vaddsd	%xmm12, %xmm12, %xmm2
	leal	1(%r14), %ecx
	vaddsd	%xmm13, %xmm13, %xmm1
	vmovsd	-208(%rbp), %xmm12
	movslq	%edx, %rdx
	vmovsd	-200(%rbp), %xmm13
	vmovsd	(%rax,%rdx,8), %xmm0
	leal	6(%r14), %edx
	vmovsd	-320(%rbp), %xmm15
	leal	7(%r14), %eax
	vmovsd	%xmm0, -248(%rbp)
	vmulsd	%xmm13, %xmm10, %xmm14
	movslq	%r14d, %rsi
	vmovsd	%xmm0, -72(%rbp)
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vmovsd	-264(%rbp), %xmm0
	cltq
	movq	$0, (%r12,%rsi,8)
	vaddsd	%xmm0, %xmm0, %xmm7
	vmovsd	-296(%rbp), %xmm0
	movq	$0, (%r12,%rcx,8)
	movq	$0, (%r12,%rdx,8)
	vaddsd	%xmm0, %xmm0, %xmm3
	movq	$0, (%r12,%rax,8)
	vaddsd	%xmm15, %xmm15, %xmm0
	vmulsd	%xmm12, %xmm11, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm11, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm10, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm8, %xmm14
	vmulsd	%xmm13, %xmm9, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-160(%rbp), %xmm12
	vmovsd	-152(%rbp), %xmm13
	vmulsd	%xmm12, %xmm7, %xmm15
	vmulsd	%xmm13, %xmm6, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm7, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm6, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm5, %xmm15
	vmulsd	%xmm12, %xmm4, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm4, %xmm14
	vmulsd	%xmm13, %xmm5, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-104(%rbp), %xmm13
	vmulsd	%xmm13, %xmm2, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-112(%rbp), %xmm12
	vmulsd	%xmm12, %xmm3, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm3, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm2, %xmm14
	leal	2(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm1, %xmm15
	vmulsd	%xmm12, %xmm0, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm0, %xmm14
	leal	3(%r14), %ecx
	vmulsd	%xmm13, %xmm1, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	8(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-184(%rbp), %xmm13
	vmulsd	%xmm13, %xmm10, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	leal	9(%r14), %eax
	movq	$0, (%r12,%rsi,8)
	vmovsd	-192(%rbp), %xmm12
	movq	$0, (%r12,%rcx,8)
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm12, %xmm11, %xmm15
	movq	$0, (%r12,%rax,8)
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm11, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm10, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm8, %xmm14
	vmulsd	%xmm13, %xmm9, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-144(%rbp), %xmm12
	vmovsd	-136(%rbp), %xmm13
	vmulsd	%xmm12, %xmm7, %xmm15
	vmulsd	%xmm13, %xmm6, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm7, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm6, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm5, %xmm15
	vmulsd	%xmm12, %xmm4, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm4, %xmm14
	vmulsd	%xmm13, %xmm5, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-88(%rbp), %xmm13
	vmulsd	%xmm13, %xmm2, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-96(%rbp), %xmm12
	vmulsd	%xmm12, %xmm3, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm13, %xmm3, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm12, %xmm2, %xmm14
	leal	4(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm1, %xmm15
	vmulsd	%xmm12, %xmm0, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm0, %xmm14
	leal	5(%r14), %ecx
	vmulsd	%xmm13, %xmm1, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	10(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-168(%rbp), %xmm13
	vmulsd	%xmm13, %xmm10, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	leal	11(%r14), %eax
	movq	$0, (%r12,%rsi,8)
	vmovsd	-176(%rbp), %xmm12
	movq	$0, (%r12,%rcx,8)
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm12, %xmm11, %xmm15
	movq	$0, (%r12,%rax,8)
	vmulsd	%xmm13, %xmm11, %xmm11
	vmulsd	%xmm12, %xmm10, %xmm10
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm10, %xmm11, %xmm10
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vmulsd	%xmm12, %xmm9, %xmm11
	vmulsd	%xmm13, %xmm9, %xmm9
	vmovsd	%xmm10, (%r12,%rcx,8)
	vmulsd	%xmm13, %xmm8, %xmm10
	vmulsd	%xmm12, %xmm8, %xmm8
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	(%r12,%rdx,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm9
	vsubsd	%xmm8, %xmm9, %xmm8
	vmovsd	%xmm8, (%r12,%rax,8)
	addq	$4, -216(%rbp)
	addl	-332(%rbp), %r14d
	vmovsd	-128(%rbp), %xmm8
	vmovsd	-120(%rbp), %xmm9
	vmulsd	%xmm8, %xmm7, %xmm11
	vmulsd	%xmm9, %xmm6, %xmm10
	vmulsd	%xmm9, %xmm7, %xmm7
	vmulsd	%xmm8, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	(%r12,%rsi,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm7
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm8, %xmm5, %xmm7
	vmulsd	%xmm9, %xmm5, %xmm5
	vmovsd	%xmm6, (%r12,%rcx,8)
	vmulsd	%xmm9, %xmm4, %xmm6
	vmulsd	%xmm8, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	(%r12,%rdx,8), %xmm6, %xmm6
	vmovsd	%xmm6, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm5
	vsubsd	%xmm4, %xmm5, %xmm4
	vmulsd	-248(%rbp), %xmm2, %xmm5
	vmovsd	%xmm4, (%r12,%rax,8)
	vmovsd	-80(%rbp), %xmm4
	vmulsd	%xmm4, %xmm3, %xmm6
	vmulsd	-248(%rbp), %xmm3, %xmm3
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vsubsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rsi,8), %xmm5, %xmm5
	vmovsd	%xmm5, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm3
	vsubsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulsd	-248(%rbp), %xmm1, %xmm1
	vmovsd	%xmm2, (%r12,%rcx,8)
	vmulsd	-248(%rbp), %xmm0, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rdx,8), %xmm2, %xmm2
	vmovsd	%xmm2, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%r12,%rax,8)
	movq	-344(%rbp), %rax
	cmpq	%rax, -216(%rbp)
	jne	.L104
.L48:
	movq	-384(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	call	_ZN12Communicator8instanceEv
	movl	-372(%rbp), %ecx
	movl	$3, %r8d
	movq	%r12, %rdx
	movq	%r13, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_bkEPdS0_ii
	movl	-336(%rbp), %eax
	testl	%eax, %eax
	jle	.L71
	movq	104(%rbx), %rax
	movq	-328(%rbp), %rdx
	xorl	%esi, %esi
	movl	52(%rbx), %r8d
	movl	-332(%rbp), %r11d
	movq	(%rax), %rax
	movq	8(%rdx), %rcx
	movq	72(%rax), %r9
	movl	-352(%rbp), %eax
	subl	$1, %eax
	leaq	4(,%rax,4), %r10
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L72:
	movl	(%r9,%rsi), %edx
	movslq	%eax, %r12
	addq	$4, %rsi
	imull	%r8d, %edx
	movslq	%edx, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	1(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	1(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	6(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	6(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	7(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	7(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	2(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	2(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	3(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	3(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	8(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	8(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	9(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	9(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	4(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	4(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	5(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	5(%rdx), %edi
	movslq	%edi, %rdi
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	leal	10(%rax), %r12d
	movslq	%r12d, %r12
	vmovsd	%xmm0, (%rdi)
	leal	10(%rdx), %edi
	addl	$11, %edx
	movslq	%edx, %rdx
	movslq	%edi, %rdi
	leaq	(%rcx,%rdx,8), %rdx
	leaq	(%rcx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r12,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	11(%rax), %edi
	addl	%r11d, %eax
	vmovsd	(%rdx), %xmm0
	cmpq	%r10, %rsi
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdx)
	jne	.L72
.L71:
	movq	-368(%rbp), %rax
	subq	-360(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L47
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %rax
	movq	%rax, -320(%rbp)
	.p2align 4,,10
	.p2align 3
.L97:
	movq	136(%rbx), %rax
	movq	%rbx, %rdi
	addq	216(%rbx), %rdi
	movq	(%rax), %rax
	movq	72(%rax), %rax
	movl	(%rax,%r13), %r14d
	movq	208(%rbx), %rax
	testb	$1, %al
	je	.L75
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L75:
	movl	$3, %edx
	movl	%r14d, %esi
	call	*%rax
	movl	52(%rbx), %edx
	movl	%eax, %r12d
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	imull	%eax, %edx
	movq	8(%r15), %rax
	movslq	%edx, %rcx
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	1(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -216(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm9
	leal	6(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm9, -224(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm11
	leal	7(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm11, -232(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	2(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -240(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm13
	leal	3(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm13, -256(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm15
	leal	8(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm15, -264(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	9(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -272(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm9
	leal	4(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm9, -280(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm11
	leal	5(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm11, -288(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	10(%rdx), %ecx
	addl	$11, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	(%rax,%rdx,8), %xmm15
	vmovsd	(%rax,%rcx,8), %xmm13
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -296(%rbp)
	vmovsd	%xmm13, -304(%rbp)
	testb	$1, %al
	vmovsd	%xmm15, -312(%rbp)
	je	.L77
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L77:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L78
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L78:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L79
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L79:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L80
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L80:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L81
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L81:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L82
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L82:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L84
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L84:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L85
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L85:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L86
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L86:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L87
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L87:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L88
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L88:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L89
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L89:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movq	%rbx, %rdi
	movl	60(%rbx), %esi
	addq	184(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L91
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L91:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L92
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L92:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L93
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L93:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L94
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L94:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L95
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L95:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L96
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L96:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
.LEHE4:
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	addq	$4, %r13
	movl	60(%rbx), %esi
	vmovsd	-216(%rbp), %xmm9
	vmovsd	-224(%rbp), %xmm12
	leal	(%rdx,%rdx,2), %edx
	vmovsd	-232(%rbp), %xmm13
	vaddsd	%xmm9, %xmm9, %xmm11
	vmovsd	-240(%rbp), %xmm15
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	vaddsd	%xmm12, %xmm12, %xmm10
	imull	68(%rbx), %eax
	vaddsd	%xmm13, %xmm13, %xmm9
	vmovsd	-264(%rbp), %xmm12
	vmovsd	-272(%rbp), %xmm13
	vaddsd	%xmm15, %xmm15, %xmm8
	vaddsd	%xmm12, %xmm12, %xmm6
	vmovsd	-280(%rbp), %xmm15
	vaddsd	%xmm13, %xmm13, %xmm5
	vmovsd	-296(%rbp), %xmm12
	vmovsd	-304(%rbp), %xmm13
	vaddsd	%xmm15, %xmm15, %xmm4
	leal	5(%rax,%rsi,4), %eax
	vaddsd	%xmm12, %xmm12, %xmm2
	vaddsd	%xmm13, %xmm13, %xmm1
	vmovsd	-208(%rbp), %xmm12
	cltq
	vmovsd	-200(%rbp), %xmm13
	vmovsd	(%rdx,%rax,8), %xmm0
	vmovsd	-312(%rbp), %xmm15
	vmovsd	%xmm0, -248(%rbp)
	vmulsd	%xmm13, %xmm10, %xmm14
	vmovsd	%xmm0, -72(%rbp)
	vmovsd	-256(%rbp), %xmm0
	vaddsd	%xmm0, %xmm0, %xmm7
	vmovsd	-288(%rbp), %xmm0
	vaddsd	%xmm0, %xmm0, %xmm3
	vaddsd	%xmm15, %xmm15, %xmm0
	vmulsd	%xmm12, %xmm11, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm13, %xmm11, %xmm15
	vmovsd	%xmm14, -240(%rbp)
	vmulsd	%xmm12, %xmm10, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	%xmm13, %xmm8, %xmm14
	vmulsd	%xmm13, %xmm9, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vsubsd	%xmm12, %xmm13, %xmm13
	vmovsd	-160(%rbp), %xmm12
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm7, %xmm12, %xmm15
	vmovsd	%xmm14, -224(%rbp)
	vmovsd	%xmm13, -232(%rbp)
	vmovsd	-152(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	-240(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -240(%rbp)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm13
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -224(%rbp)
	movq	-328(%rbp), %rdx
	vmovsd	-232(%rbp), %xmm12
	imull	52(%rbx), %r14d
	vsubsd	%xmm13, %xmm12, %xmm13
	vmovsd	-112(%rbp), %xmm12
	movq	8(%rdx), %rax
	vmulsd	%xmm3, %xmm12, %xmm15
	vmovsd	%xmm13, -232(%rbp)
	vmovsd	-104(%rbp), %xmm13
	movslq	%r14d, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm2, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	-240(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	1(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	6(%r14), %edx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-232(%rbp), %xmm13
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-184(%rbp), %xmm13
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	7(%r14), %edx
	vmulsd	%xmm10, %xmm13, %xmm14
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm12, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	2(%r14), %edx
	vmovsd	-192(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm12, %xmm15
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vmovsd	%xmm14, -224(%rbp)
	vmulsd	%xmm10, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, -232(%rbp)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vsubsd	%xmm12, %xmm13, %xmm13
	vmovsd	-144(%rbp), %xmm12
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm7, %xmm12, %xmm15
	vmovsd	%xmm14, -216(%rbp)
	vmovsd	%xmm13, -240(%rbp)
	vmovsd	-136(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -224(%rbp)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-232(%rbp), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, -232(%rbp)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm13
	vmovsd	-240(%rbp), %xmm12
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vsubsd	%xmm13, %xmm12, %xmm13
	vmovsd	%xmm14, -256(%rbp)
	vmovsd	%xmm13, -216(%rbp)
	vmovsd	-96(%rbp), %xmm12
	vmovsd	-88(%rbp), %xmm13
	vmulsd	%xmm3, %xmm12, %xmm15
	vmulsd	%xmm2, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	3(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-232(%rbp), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	8(%r14), %edx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-216(%rbp), %xmm13
	vaddsd	-256(%rbp), %xmm14, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-168(%rbp), %xmm13
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	9(%r14), %edx
	vmulsd	%xmm13, %xmm10, %xmm14
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm12, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	4(%r14), %edx
	vmovsd	-176(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm12, %xmm11, %xmm15
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm13, %xmm11, %xmm11
	vmulsd	%xmm12, %xmm10, %xmm10
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vsubsd	%xmm10, %xmm11, %xmm10
	vmulsd	%xmm13, %xmm8, %xmm11
	vmulsd	%xmm12, %xmm8, %xmm8
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm10, %xmm15, %xmm10
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm13, %xmm9, %xmm9
	vmovsd	-128(%rbp), %xmm12
	vaddsd	%xmm11, %xmm15, %xmm15
	vxorpd	%xmm11, %xmm11, %xmm11
	vsubsd	%xmm8, %xmm9, %xmm8
	vmulsd	%xmm12, %xmm7, %xmm13
	vaddsd	%xmm11, %xmm15, %xmm15
	vsubsd	%xmm8, %xmm11, %xmm8
	vmovsd	-120(%rbp), %xmm11
	vmulsd	%xmm11, %xmm6, %xmm9
	vmulsd	%xmm11, %xmm7, %xmm7
	vmulsd	%xmm12, %xmm6, %xmm6
	vaddsd	%xmm9, %xmm13, %xmm9
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm12, %xmm5, %xmm7
	vmulsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm9, %xmm14, %xmm9
	vsubsd	%xmm6, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm4, %xmm6
	vmulsd	%xmm12, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vmulsd	-248(%rbp), %xmm2, %xmm5
	vaddsd	%xmm6, %xmm15, %xmm15
	vsubsd	%xmm4, %xmm8, %xmm8
	vmovsd	-80(%rbp), %xmm4
	vmulsd	%xmm4, %xmm3, %xmm14
	vmulsd	-248(%rbp), %xmm3, %xmm3
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm14, %xmm14
	vsubsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm4, %xmm1, %xmm3
	vaddsd	%xmm14, %xmm9, %xmm9
	vmulsd	-248(%rbp), %xmm1, %xmm1
	vsubsd	%xmm2, %xmm10, %xmm10
	vmulsd	-248(%rbp), %xmm0, %xmm2
	vaddsd	(%rdx), %xmm9, %xmm9
	vmulsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm2
	vmovsd	%xmm9, (%rdx)
	leal	5(%r14), %edx
	movslq	%edx, %rdx
	vsubsd	%xmm0, %xmm8, %xmm8
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm2, %xmm15, %xmm15
	vaddsd	(%rdx), %xmm10, %xmm10
	vmovsd	%xmm10, (%rdx)
	leal	10(%r14), %edx
	addl	$11, %r14d
	movslq	%r14d, %r14
	cmpq	-320(%rbp), %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r14,8), %rax
	vaddsd	(%rdx), %xmm15, %xmm15
	vmovsd	%xmm15, (%rdx)
	vaddsd	(%rax), %xmm8, %xmm8
	vmovsd	%xmm8, (%rax)
	jne	.L97
.L47:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L105
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L100:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB5:
	call	_Unwind_Resume
.LEHE5:
.L105:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2818:
	.section	.gcc_except_table
.LLSDA2818:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2818-.LLSDACSB2818
.LLSDACSB2818:
	.uleb128 .LEHB4-.LFB2818
	.uleb128 .LEHE4-.LEHB4
	.uleb128 .L100-.LFB2818
	.uleb128 0
	.uleb128 .LEHB5-.LFB2818
	.uleb128 .LEHE5-.LEHB5
	.uleb128 0
	.uleb128 0
.LLSDACSE2818:
	.text
	.size	_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_:
.LFB2817:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2817
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	movq	%rdx, %r15
	.cfi_offset 15, -24
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	subq	$440, %rsp
	movq	%rsi, -424(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -428(%rbp)
	movq	96(%rdi), %rax
	movq	(%rax), %rdx
	movq	56(%rdx), %rcx
	movq	%rcx, -448(%rbp)
	movq	48(%rdx), %rcx
	subq	%rcx, -448(%rbp)
	sarq	$2, -448(%rbp)
	movl	-448(%rbp), %edx
	movl	%edx, -432(%rbp)
	movq	136(%rdi), %rdx
	movl	-432(%rbp), %esi
	movq	(%rdx), %rdx
	movq	48(%rdx), %rcx
	movq	56(%rdx), %rdx
	movq	%rdx, -464(%rbp)
	movl	-428(%rbp), %edx
	imull	-432(%rbp), %edx
	movq	%rcx, -456(%rbp)
	movl	%edx, -468(%rbp)
	movslq	%edx, %rdx
	leaq	46(,%rdx,8), %rdx
	movq	%rdx, -480(%rbp)
	andq	$-16, %rdx
	subq	%rdx, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
	testl	%esi, %esi
	jle	.L107
	movl	-448(%rbp), %edx
	movq	$0, -224(%rbp)
	xorl	%r14d, %r14d
	subl	$1, %edx
	leaq	4(,%rdx,4), %rdx
	movq	%rdx, -440(%rbp)
	jmp	.L129
	.p2align 4,,10
	.p2align 3
.L163:
	movq	96(%rbx), %rax
.L129:
	movq	(%rax), %rax
	movq	-224(%rbp), %rcx
	movq	%rbx, %rdi
	movl	52(%rbx), %edx
	movq	48(%rax), %rax
	movl	(%rax,%rcx), %r13d
	movq	8(%r15), %rax
	imull	%r13d, %edx
	movslq	%edx, %rcx
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	13(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -232(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	1(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -240(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	12(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -248(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	6(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -256(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	19(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -264(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	7(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -272(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	18(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -280(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	2(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -288(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	15(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -296(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	3(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -304(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	14(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -312(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	8(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -320(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	21(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -328(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	9(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -336(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	20(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -344(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	4(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -352(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	17(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -360(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	5(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -368(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	16(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -376(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	10(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -384(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -392(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	11(%rdx), %ecx
	addl	$22, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	(%rax,%rdx,8), %xmm12
	vmovsd	(%rax,%rcx,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -400(%rbp)
	vmovsd	%xmm2, -408(%rbp)
	testb	$1, %al
	vmovsd	%xmm12, -416(%rbp)
	je	.L109
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L109:
	movl	%r13d, %esi
.LEHB6:
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L110
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L110:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L111
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L111:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L112
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L112:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L113
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L113:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L114
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L114:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L116
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L116:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L117
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L117:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L118
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L118:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L119
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L119:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L120
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L120:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L121
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L121:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	movl	60(%rbx), %ecx
	addq	184(%rbx), %rdi
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L123
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L123:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L124
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L124:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L125
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L125:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L126
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L126:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L127
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L127:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L128
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L128:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movl	60(%rbx), %ecx
	movq	8(%rbx), %rdx
	vmovsd	-264(%rbp), %xmm12
	vmovsd	-232(%rbp), %xmm1
	leal	(%rax,%rsi,2), %eax
	vsubsd	-272(%rbp), %xmm12, %xmm9
	movslq	%r14d, %rsi
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	vmovsd	-248(%rbp), %xmm2
	vmovsd	-328(%rbp), %xmm12
	vaddsd	-240(%rbp), %xmm1, %xmm11
	vsubsd	-256(%rbp), %xmm2, %xmm10
	vmovsd	-296(%rbp), %xmm1
	vsubsd	-336(%rbp), %xmm12, %xmm5
	vmovsd	-312(%rbp), %xmm2
	vmovsd	-376(%rbp), %xmm12
	vaddsd	-304(%rbp), %xmm1, %xmm7
	leal	5(%rax,%rcx,4), %eax
	leal	1(%r14), %ecx
	vsubsd	-320(%rbp), %xmm2, %xmm6
	vmovsd	-360(%rbp), %xmm1
	vsubsd	-384(%rbp), %xmm12, %xmm2
	cltq
	vmovsd	-408(%rbp), %xmm12
	vmovsd	(%rdx,%rax,8), %xmm0
	movslq	%ecx, %rcx
	vaddsd	-368(%rbp), %xmm1, %xmm3
	movq	$0, (%r12,%rsi,8)
	leal	6(%r14), %edx
	vmovsd	%xmm0, -216(%rbp)
	leal	7(%r14), %eax
	vmovsd	%xmm0, -72(%rbp)
	movslq	%edx, %rdx
	vmovsd	-280(%rbp), %xmm0
	cltq
	vaddsd	-288(%rbp), %xmm0, %xmm8
	vmovsd	-344(%rbp), %xmm0
	vaddsd	-352(%rbp), %xmm0, %xmm4
	vmovsd	-392(%rbp), %xmm0
	vsubsd	-400(%rbp), %xmm0, %xmm1
	vaddsd	-416(%rbp), %xmm12, %xmm0
	movq	$0, (%r12,%rcx,8)
	vmovsd	-208(%rbp), %xmm12
	movq	$0, (%r12,%rdx,8)
	vmovsd	-200(%rbp), %xmm13
	movq	$0, (%r12,%rax,8)
	vmulsd	%xmm11, %xmm12, %xmm15
	vmulsd	%xmm10, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm10, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-152(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-160(%rbp), %xmm12
	vmulsd	%xmm7, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-104(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-112(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	2(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	3(%r14), %ecx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	8(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	vmovsd	-192(%rbp), %xmm12
	leal	9(%r14), %eax
	vmovsd	-184(%rbp), %xmm13
	vmulsd	%xmm11, %xmm12, %xmm15
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm10, %xmm13, %xmm14
	movq	$0, (%r12,%rax,8)
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm10, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-136(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-144(%rbp), %xmm12
	vmulsd	%xmm7, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-88(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-96(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	4(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	5(%r14), %ecx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	10(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	vmovsd	-176(%rbp), %xmm12
	leal	11(%r14), %eax
	vmovsd	-168(%rbp), %xmm13
	vmulsd	%xmm11, %xmm12, %xmm15
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm10, %xmm13, %xmm14
	movq	$0, (%r12,%rax,8)
	vmulsd	%xmm11, %xmm13, %xmm11
	vmulsd	%xmm10, %xmm12, %xmm10
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm10, %xmm11, %xmm10
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vmulsd	%xmm9, %xmm12, %xmm11
	vmulsd	%xmm9, %xmm13, %xmm9
	vmovsd	%xmm10, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm10
	vmulsd	%xmm8, %xmm12, %xmm8
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	(%r12,%rdx,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm9
	vsubsd	%xmm8, %xmm9, %xmm8
	vmovsd	-120(%rbp), %xmm9
	vmulsd	%xmm6, %xmm9, %xmm10
	vmovsd	%xmm8, (%r12,%rax,8)
	vmovsd	-128(%rbp), %xmm8
	vmulsd	%xmm7, %xmm8, %xmm11
	vmulsd	%xmm6, %xmm8, %xmm6
	vmulsd	%xmm7, %xmm9, %xmm7
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	(%r12,%rsi,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm7
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm5, %xmm8, %xmm7
	vmulsd	%xmm5, %xmm9, %xmm5
	vmovsd	%xmm6, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm9, %xmm6
	vmulsd	%xmm4, %xmm8, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	(%r12,%rdx,8), %xmm6, %xmm6
	vmovsd	%xmm6, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm5
	vsubsd	%xmm4, %xmm5, %xmm4
	vmulsd	-216(%rbp), %xmm2, %xmm5
	vmovsd	%xmm4, (%r12,%rax,8)
	vmovsd	-80(%rbp), %xmm4
	vmulsd	%xmm4, %xmm3, %xmm6
	vmulsd	-216(%rbp), %xmm3, %xmm3
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vsubsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rsi,8), %xmm5, %xmm5
	vmovsd	%xmm5, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm3
	vsubsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulsd	-216(%rbp), %xmm1, %xmm1
	vmovsd	%xmm2, (%r12,%rcx,8)
	vmulsd	-216(%rbp), %xmm0, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rdx,8), %xmm2, %xmm2
	vmovsd	%xmm2, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%r12,%rax,8)
	addq	$4, -224(%rbp)
	addl	-428(%rbp), %r14d
	movq	-440(%rbp), %rax
	cmpq	%rax, -224(%rbp)
	jne	.L163
.L107:
	movq	-480(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	call	_ZN12Communicator8instanceEv
	movl	-468(%rbp), %ecx
	movl	$2, %r8d
	movq	%r12, %rdx
	movq	%r13, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_bkEPdS0_ii
	movl	-432(%rbp), %ecx
	testl	%ecx, %ecx
	jle	.L130
	movq	104(%rbx), %rax
	movq	-424(%rbp), %rcx
	xorl	%esi, %esi
	movq	%r15, -224(%rbp)
	movl	-428(%rbp), %r15d
	movl	52(%rbx), %r14d
	movq	%rbx, -216(%rbp)
	movq	(%rax), %rax
	movq	8(%rcx), %rdx
	xorl	%ecx, %ecx
	movq	48(%rax), %r12
	movl	-448(%rbp), %eax
	subl	$1, %eax
	leaq	4(,%rax,4), %rax
	movq	%rax, %rbx
	.p2align 4,,10
	.p2align 3
.L131:
	movl	(%r12,%rsi), %eax
	movslq	%ecx, %r9
	leal	1(%rcx), %r11d
	addq	$4, %rsi
	movslq	%r11d, %r11
	imull	%r14d, %eax
	movslq	%eax, %rdi
	leal	6(%rax), %r8d
	leal	7(%rax), %r10d
	leaq	(%rdx,%rdi,8), %rdi
	movslq	%r8d, %r8
	movslq	%r10d, %r10
	vmovsd	(%rdi), %xmm0
	leaq	(%rdx,%r8,8), %r8
	leaq	(%rdx,%r10,8), %r10
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	1(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	6(%rcx), %edi
	vmovsd	(%r8), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	7(%rcx), %r8d
	vmovsd	(%r10), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	12(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	3(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	13(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	18(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	9(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	19(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	2(%rcx), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	2(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	%xmm0, (%r8)
	leal	8(%rax), %r8d
	vmovsd	(%rdi), %xmm0
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	3(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	8(%rcx), %edi
	vmovsd	(%r8), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	9(%rcx), %r8d
	vmovsd	(%r10), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	14(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	5(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	15(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	20(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	11(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	4(%rcx), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	4(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	%xmm0, (%r8)
	leal	10(%rax), %r8d
	vmovsd	(%rdi), %xmm0
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	5(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	(%rdi), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	10(%rcx), %edi
	vmovsd	(%r8), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	11(%rcx), %r8d
	addl	%r15d, %ecx
	vmovsd	(%r10), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	16(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	17(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	22(%rax), %r9d
	addl	$23, %eax
	cltq
	cmpq	%rbx, %rsi
	movslq	%r9d, %r9
	leaq	(%rdx,%rax,8), %rax
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	vmovsd	(%r9), %xmm0
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	vmovsd	(%rax), %xmm0
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rax)
	jne	.L131
	movq	-216(%rbp), %rbx
	movq	-224(%rbp), %r15
.L130:
	movq	-464(%rbp), %rax
	subq	-456(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L106
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %rax
	movq	%rax, -312(%rbp)
	.p2align 4,,10
	.p2align 3
.L156:
	movq	136(%rbx), %rax
	movq	%rbx, %rdi
	addq	216(%rbx), %rdi
	movq	(%rax), %rax
	movq	48(%rax), %rax
	movl	(%rax,%r13), %r14d
	movq	208(%rbx), %rax
	testb	$1, %al
	je	.L134
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L134:
	movl	$2, %edx
	movl	%r14d, %esi
	call	*%rax
	movl	52(%rbx), %edx
	movl	%eax, %r12d
	movq	%rbx, %rdi
	imull	%eax, %edx
	movq	8(%r15), %rax
	movslq	%edx, %rsi
	leal	13(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm5
	leal	1(%rdx), %esi
	movslq	%ecx, %rcx
	vaddsd	(%rax,%rcx,8), %xmm5, %xmm5
	movslq	%esi, %rsi
	leal	12(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm2
	leal	6(%rdx), %esi
	movslq	%ecx, %rcx
	vsubsd	(%rax,%rcx,8), %xmm2, %xmm2
	movslq	%esi, %rsi
	leal	19(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm3
	leal	7(%rdx), %esi
	movslq	%ecx, %rcx
	vsubsd	(%rax,%rcx,8), %xmm3, %xmm3
	movslq	%esi, %rsi
	leal	18(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm1
	leal	2(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm5, -216(%rbp)
	vaddsd	(%rax,%rcx,8), %xmm1, %xmm1
	movslq	%esi, %rsi
	leal	15(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	3(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -224(%rbp)
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	14(%rdx), %ecx
	vmovsd	%xmm3, -232(%rbp)
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -240(%rbp)
	vmovsd	%xmm0, -248(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	8(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	21(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -256(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	9(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	20(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -264(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	4(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	17(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -272(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	5(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	16(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -280(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	10(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -288(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	leal	11(%rdx), %ecx
	addl	$22, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -296(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm0
	vaddsd	(%rax,%rdx,8), %xmm0, %xmm0
	movq	176(%rbx), %rax
	testb	$1, %al
	vmovsd	%xmm0, -304(%rbp)
	je	.L136
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L136:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L137
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L137:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L138
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L138:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L139
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L139:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L140
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L140:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L141
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L141:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L143
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L143:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L144
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L144:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L145
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L145:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L146
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L146:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L147
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L147:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L148
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L148:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	movl	60(%rbx), %ecx
	addq	184(%rbx), %rdi
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L150
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L150:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L151
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L151:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L152
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L152:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L153
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L153:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L154
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L154:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L155
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L155:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
.LEHE6:
	vmovsd	-208(%rbp), %xmm1
	movl	56(%rbx), %esi
	vmovsd	-200(%rbp), %xmm9
	movq	8(%rbx), %rdx
	vmulsd	-216(%rbp), %xmm1, %xmm12
	movl	60(%rbx), %ecx
	addq	$4, %r13
	vmulsd	-224(%rbp), %xmm9, %xmm2
	leal	(%rax,%rsi,2), %eax
	vmulsd	-216(%rbp), %xmm9, %xmm11
	movq	8(%rdx), %rdx
	vmulsd	-232(%rbp), %xmm1, %xmm10
	imull	68(%rbx), %eax
	imull	52(%rbx), %r14d
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	leal	5(%rax,%rcx,4), %eax
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	-424(%rbp), %rdx
	vmovsd	%xmm0, -72(%rbp)
	movq	8(%rdx), %rax
	movslq	%r14d, %rdx
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	vmovsd	-160(%rbp), %xmm1
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-152(%rbp), %xmm2
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-248(%rbp), %xmm2, %xmm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-112(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-104(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vmulsd	-288(%rbp), %xmm1, %xmm4
	vaddsd	%xmm3, %xmm8, %xmm3
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm4, %xmm8, %xmm4
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm4, %xmm7, %xmm4
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	1(%r14), %edx
	vmovsd	-184(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-224(%rbp), %xmm9, %xmm2
	vmulsd	-216(%rbp), %xmm9, %xmm11
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	6(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	7(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	12(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	-192(%rbp), %xmm1
	vmulsd	-216(%rbp), %xmm1, %xmm12
	vmulsd	-232(%rbp), %xmm1, %xmm10
	vmovsd	%xmm4, (%rdx)
	leal	13(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	vmovsd	%xmm3, (%rdx)
	leal	18(%r14), %edx
	movslq	%edx, %rdx
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm5
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vmovsd	%xmm5, (%rdx)
	leal	19(%r14), %edx
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	movslq	%edx, %rdx
	vmovsd	-144(%rbp), %xmm1
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm2, %xmm10, %xmm10
	vmovsd	(%rdx), %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-136(%rbp), %xmm2
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vsubsd	%xmm7, %xmm6, %xmm7
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vmovsd	%xmm7, (%rdx)
	vmulsd	-248(%rbp), %xmm2, %xmm7
	leal	2(%r14), %edx
	movslq	%edx, %rdx
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vxorpd	%xmm12, %xmm12, %xmm12
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-96(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-88(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vmulsd	-288(%rbp), %xmm1, %xmm4
	vaddsd	%xmm3, %xmm8, %xmm3
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm4, %xmm8, %xmm4
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm4, %xmm7, %xmm4
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	3(%r14), %edx
	vmovsd	-80(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	8(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	9(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	14(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	-176(%rbp), %xmm1
	vmovsd	%xmm4, (%rdx)
	leal	15(%r14), %edx
	vmulsd	-232(%rbp), %xmm1, %xmm4
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	leal	20(%r14), %edx
	vmovsd	-168(%rbp), %xmm3
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-224(%rbp), %xmm3, %xmm2
	vaddsd	(%rdx), %xmm5, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	21(%r14), %edx
	vmulsd	-216(%rbp), %xmm3, %xmm5
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm6
	vsubsd	%xmm7, %xmm6, %xmm7
	vmulsd	-216(%rbp), %xmm1, %xmm6
	vmovsd	%xmm7, (%rdx)
	leal	4(%r14), %edx
	vaddsd	%xmm2, %xmm6, %xmm6
	vmulsd	-224(%rbp), %xmm1, %xmm2
	movslq	%edx, %rdx
	vmulsd	-240(%rbp), %xmm1, %xmm1
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm12, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm5, %xmm5
	vmulsd	-240(%rbp), %xmm3, %xmm2
	vmulsd	-232(%rbp), %xmm3, %xmm3
	vsubsd	%xmm5, %xmm12, %xmm5
	vaddsd	%xmm2, %xmm4, %xmm4
	vmovsd	-120(%rbp), %xmm2
	vsubsd	%xmm1, %xmm3, %xmm3
	vmovsd	-128(%rbp), %xmm1
	vmulsd	-256(%rbp), %xmm2, %xmm7
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm12, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm12, %xmm3
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	-256(%rbp), %xmm1, %xmm8
	vaddsd	%xmm7, %xmm6, %xmm7
	vmulsd	-248(%rbp), %xmm2, %xmm6
	vsubsd	%xmm8, %xmm6, %xmm6
	vmulsd	-264(%rbp), %xmm1, %xmm8
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm5, %xmm5
	vmulsd	-272(%rbp), %xmm2, %xmm6
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vaddsd	%xmm6, %xmm8, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-280(%rbp), %xmm0, %xmm2
	vaddsd	%xmm6, %xmm4, %xmm4
	vmulsd	-280(%rbp), %xmm9, %xmm6
	vsubsd	%xmm1, %xmm3, %xmm3
	vmulsd	-288(%rbp), %xmm0, %xmm1
	vaddsd	%xmm1, %xmm6, %xmm6
	vmulsd	-288(%rbp), %xmm9, %xmm1
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-296(%rbp), %xmm9, %xmm2
	vmulsd	-304(%rbp), %xmm9, %xmm9
	vsubsd	%xmm1, %xmm5, %xmm5
	vmulsd	-304(%rbp), %xmm0, %xmm1
	vmulsd	-296(%rbp), %xmm0, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	5(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm5, %xmm0, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	17(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	22(%r14), %edx
	addl	$23, %r14d
	movslq	%r14d, %r14
	cmpq	-312(%rbp), %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r14,8), %rax
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	vmovsd	(%rax), %xmm0
	vsubsd	%xmm4, %xmm0, %xmm4
	vmovsd	%xmm4, (%rax)
	jne	.L156
.L106:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L164
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L159:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB7:
	call	_Unwind_Resume
.LEHE7:
.L164:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2817:
	.section	.gcc_except_table
.LLSDA2817:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2817-.LLSDACSB2817
.LLSDACSB2817:
	.uleb128 .LEHB6-.LFB2817
	.uleb128 .LEHE6-.LEHB6
	.uleb128 .L159-.LFB2817
	.uleb128 0
	.uleb128 .LEHB7-.LFB2817
	.uleb128 .LEHE7-.LEHB7
	.uleb128 0
	.uleb128 0
.LLSDACSE2817:
	.text
	.size	_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_:
.LFB2816:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2816
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	movq	%rdx, %r15
	.cfi_offset 15, -24
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	subq	$440, %rsp
	movq	%rsi, -424(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -428(%rbp)
	movq	96(%rdi), %rax
	movq	(%rax), %rdx
	movq	32(%rdx), %rcx
	movq	%rcx, -448(%rbp)
	movq	24(%rdx), %rcx
	subq	%rcx, -448(%rbp)
	sarq	$2, -448(%rbp)
	movl	-448(%rbp), %edx
	movl	%edx, -432(%rbp)
	movq	136(%rdi), %rdx
	movl	-432(%rbp), %r8d
	movq	(%rdx), %rdx
	movq	24(%rdx), %rcx
	movq	32(%rdx), %rdx
	movq	%rdx, -464(%rbp)
	movl	-428(%rbp), %edx
	imull	-432(%rbp), %edx
	movq	%rcx, -456(%rbp)
	movl	%edx, -468(%rbp)
	movslq	%edx, %rdx
	leaq	46(,%rdx,8), %rdx
	movq	%rdx, -480(%rbp)
	andq	$-16, %rdx
	subq	%rdx, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
	testl	%r8d, %r8d
	jle	.L166
	movl	-448(%rbp), %edx
	movq	$0, -224(%rbp)
	xorl	%r14d, %r14d
	subl	$1, %edx
	leaq	4(,%rdx,4), %rdx
	movq	%rdx, -440(%rbp)
	jmp	.L188
	.p2align 4,,10
	.p2align 3
.L222:
	movq	96(%rbx), %rax
.L188:
	movq	(%rax), %rax
	movq	-224(%rbp), %rcx
	movq	%rbx, %rdi
	movl	52(%rbx), %edx
	movq	24(%rax), %rax
	movl	(%rax,%rcx), %r13d
	movq	8(%r15), %rax
	imull	%r13d, %edx
	movslq	%edx, %rcx
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	18(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -232(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	1(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -240(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	19(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -248(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	6(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -256(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	12(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -264(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	7(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -272(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	13(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -280(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	2(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -288(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	20(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -296(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	3(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -304(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	21(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -312(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	8(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -320(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	14(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -328(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	9(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -336(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	15(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -344(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	4(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -352(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	22(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -360(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	5(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -368(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -376(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	10(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -384(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	16(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -392(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	11(%rdx), %ecx
	addl	$17, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	(%rax,%rdx,8), %xmm12
	vmovsd	(%rax,%rcx,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -400(%rbp)
	vmovsd	%xmm2, -408(%rbp)
	testb	$1, %al
	vmovsd	%xmm12, -416(%rbp)
	je	.L168
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L168:
	movl	%r13d, %esi
.LEHB8:
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L169
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L169:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L170
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L170:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L171
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L171:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L172
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L172:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L173
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L173:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	addq	184(%rbx), %rdi
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L175
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L175:
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L176
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L176:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L177
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L177:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L178
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L178:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L179
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L179:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L180
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L180:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	addq	184(%rbx), %rdi
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L182
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L182:
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L183
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L183:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L184
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L184:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L185
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L185:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L186
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L186:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L187
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L187:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movl	60(%rbx), %ecx
	movslq	%r14d, %rsi
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	vmovsd	-264(%rbp), %xmm12
	vmovsd	-232(%rbp), %xmm1
	vaddsd	-272(%rbp), %xmm12, %xmm9
	vmovsd	-248(%rbp), %xmm2
	movq	8(%rdx), %rdx
	vmovsd	-328(%rbp), %xmm12
	vsubsd	-240(%rbp), %xmm1, %xmm11
	vmovsd	-296(%rbp), %xmm1
	leal	5(%rax,%rcx,4), %eax
	vsubsd	-256(%rbp), %xmm2, %xmm10
	leal	1(%r14), %ecx
	vaddsd	-336(%rbp), %xmm12, %xmm5
	vmovsd	-312(%rbp), %xmm2
	cltq
	vmovsd	-376(%rbp), %xmm12
	vmovsd	(%rdx,%rax,8), %xmm0
	vsubsd	-304(%rbp), %xmm1, %xmm7
	vsubsd	-320(%rbp), %xmm2, %xmm6
	vmovsd	-360(%rbp), %xmm1
	vmovsd	%xmm0, -216(%rbp)
	vsubsd	-384(%rbp), %xmm12, %xmm2
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -72(%rbp)
	vsubsd	-368(%rbp), %xmm1, %xmm3
	movq	$0, (%r12,%rsi,8)
	vmovsd	-280(%rbp), %xmm0
	leal	6(%r14), %edx
	vmovsd	-408(%rbp), %xmm12
	leal	7(%r14), %eax
	vaddsd	-288(%rbp), %xmm0, %xmm8
	vmovsd	-344(%rbp), %xmm0
	movslq	%edx, %rdx
	cltq
	vaddsd	-352(%rbp), %xmm0, %xmm4
	vmovsd	-392(%rbp), %xmm0
	vaddsd	-400(%rbp), %xmm0, %xmm1
	vaddsd	-416(%rbp), %xmm12, %xmm0
	movq	$0, (%r12,%rcx,8)
	vmovsd	-208(%rbp), %xmm12
	movq	$0, (%r12,%rdx,8)
	vmovsd	-200(%rbp), %xmm13
	movq	$0, (%r12,%rax,8)
	vmulsd	%xmm11, %xmm12, %xmm15
	vmulsd	%xmm10, %xmm13, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm10, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-152(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-160(%rbp), %xmm12
	vmulsd	%xmm7, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-104(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-112(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	2(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	3(%r14), %ecx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	8(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	vmovsd	-192(%rbp), %xmm12
	leal	9(%r14), %eax
	vmovsd	-184(%rbp), %xmm13
	vmulsd	%xmm11, %xmm12, %xmm15
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm10, %xmm13, %xmm14
	movq	$0, (%r12,%rax,8)
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm10, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-136(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-144(%rbp), %xmm12
	vmulsd	%xmm7, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm6, %xmm12, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm13
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	-88(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vmovsd	%xmm12, (%r12,%rax,8)
	vmovsd	-96(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	4(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	(%r12,%rcx,8), %xmm15
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vmovsd	%xmm14, (%r12,%rcx,8)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	5(%r14), %ecx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm12, %xmm13, %xmm12
	vaddsd	(%r12,%rdx,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rdx,8)
	leal	10(%r14), %edx
	vmovsd	(%r12,%rax,8), %xmm13
	movslq	%edx, %rdx
	vsubsd	%xmm12, %xmm13, %xmm12
	vmovsd	%xmm12, (%r12,%rax,8)
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	vmovsd	-176(%rbp), %xmm12
	leal	11(%r14), %eax
	vmovsd	-168(%rbp), %xmm13
	vmulsd	%xmm11, %xmm12, %xmm15
	cltq
	movq	$0, (%r12,%rdx,8)
	vmulsd	%xmm10, %xmm13, %xmm14
	movq	$0, (%r12,%rax,8)
	vmulsd	%xmm11, %xmm13, %xmm11
	vmulsd	%xmm10, %xmm12, %xmm10
	vaddsd	%xmm14, %xmm15, %xmm14
	vsubsd	%xmm10, %xmm11, %xmm10
	vaddsd	(%r12,%rsi,8), %xmm14, %xmm14
	vmovsd	%xmm14, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vmulsd	%xmm9, %xmm12, %xmm11
	vmulsd	%xmm9, %xmm13, %xmm9
	vmovsd	%xmm10, (%r12,%rcx,8)
	vmulsd	%xmm8, %xmm13, %xmm10
	vmulsd	%xmm8, %xmm12, %xmm8
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	(%r12,%rdx,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm9
	vsubsd	%xmm8, %xmm9, %xmm8
	vmovsd	-120(%rbp), %xmm9
	vmulsd	%xmm6, %xmm9, %xmm10
	vmovsd	%xmm8, (%r12,%rax,8)
	vmovsd	-128(%rbp), %xmm8
	vmulsd	%xmm7, %xmm8, %xmm11
	vmulsd	%xmm6, %xmm8, %xmm6
	vmulsd	%xmm7, %xmm9, %xmm7
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	(%r12,%rsi,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm7
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm5, %xmm8, %xmm7
	vmulsd	%xmm5, %xmm9, %xmm5
	vmovsd	%xmm6, (%r12,%rcx,8)
	vmulsd	%xmm4, %xmm9, %xmm6
	vmulsd	%xmm4, %xmm8, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	(%r12,%rdx,8), %xmm6, %xmm6
	vmovsd	%xmm6, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm5
	vsubsd	%xmm4, %xmm5, %xmm4
	vmulsd	-216(%rbp), %xmm2, %xmm5
	vmovsd	%xmm4, (%r12,%rax,8)
	vmovsd	-80(%rbp), %xmm4
	vmulsd	%xmm4, %xmm3, %xmm6
	vmulsd	-216(%rbp), %xmm3, %xmm3
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vsubsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rsi,8), %xmm5, %xmm5
	vmovsd	%xmm5, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm3
	vsubsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulsd	-216(%rbp), %xmm1, %xmm1
	vmovsd	%xmm2, (%r12,%rcx,8)
	vmulsd	-216(%rbp), %xmm0, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rdx,8), %xmm2, %xmm2
	vmovsd	%xmm2, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%r12,%rax,8)
	addq	$4, -224(%rbp)
	addl	-428(%rbp), %r14d
	movq	-440(%rbp), %rax
	cmpq	%rax, -224(%rbp)
	jne	.L222
.L166:
	movq	-480(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	call	_ZN12Communicator8instanceEv
	movl	-468(%rbp), %ecx
	movl	$1, %r8d
	movq	%r12, %rdx
	movq	%r13, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_bkEPdS0_ii
	movl	-432(%rbp), %edi
	testl	%edi, %edi
	jle	.L189
	movq	104(%rbx), %rax
	movq	-424(%rbp), %rcx
	xorl	%esi, %esi
	movq	%r15, -224(%rbp)
	movl	-428(%rbp), %r15d
	movl	52(%rbx), %r14d
	movq	%rbx, -216(%rbp)
	movq	(%rax), %rax
	movq	8(%rcx), %rdx
	xorl	%ecx, %ecx
	movq	24(%rax), %r12
	movl	-448(%rbp), %eax
	subl	$1, %eax
	leaq	4(,%rax,4), %rax
	movq	%rax, %rbx
	.p2align 4,,10
	.p2align 3
.L190:
	movl	(%r12,%rsi), %eax
	movslq	%ecx, %r8
	leal	6(%rcx), %r11d
	addq	$4, %rsi
	movslq	%r11d, %r11
	imull	%r14d, %eax
	movslq	%eax, %rdi
	leal	1(%rax), %r9d
	leal	7(%rax), %r10d
	leaq	(%rdx,%rdi,8), %rdi
	movslq	%r9d, %r9
	movslq	%r10d, %r10
	vmovsd	(%rdi), %xmm0
	leaq	(%rdx,%r9,8), %r9
	leaq	(%rdx,%r10,8), %r10
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	1(%rcx), %edi
	vmovsd	(%r9), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	6(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	(%r9), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	7(%rcx), %r9d
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	12(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	8(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	13(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	18(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	9(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	19(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	3(%rax), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	2(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	%xmm0, (%r8)
	leal	2(%rcx), %r8d
	vmovsd	(%rdi), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	3(%rcx), %edi
	vmovsd	(%r9), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	8(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	(%r9), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	9(%rcx), %r9d
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	14(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	10(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	15(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	20(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	11(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	5(%rax), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	4(%rax), %edi
	movslq	%edi, %rdi
	leaq	(%rdx,%rdi,8), %rdi
	vmovsd	%xmm0, (%r8)
	leal	4(%rcx), %r8d
	vmovsd	(%rdi), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdi)
	leal	5(%rcx), %edi
	vmovsd	(%r9), %xmm0
	movslq	%edi, %rdi
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	10(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	(%r9), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	11(%rcx), %r9d
	addl	%r15d, %ecx
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	16(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	17(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	22(%rax), %r9d
	addl	$23, %eax
	cltq
	cmpq	%rbx, %rsi
	movslq	%r9d, %r9
	leaq	(%rdx,%rax,8), %rax
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	vmovsd	(%r9), %xmm0
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	vmovsd	(%rax), %xmm0
	vsubsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rax)
	jne	.L190
	movq	-216(%rbp), %rbx
	movq	-224(%rbp), %r15
.L189:
	movq	-464(%rbp), %rax
	subq	-456(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L165
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %rax
	movq	%rax, -312(%rbp)
	.p2align 4,,10
	.p2align 3
.L215:
	movq	136(%rbx), %rax
	movq	%rbx, %rdi
	addq	216(%rbx), %rdi
	movq	(%rax), %rax
	movq	24(%rax), %rax
	movl	(%rax,%r13), %r14d
	movq	208(%rbx), %rax
	testb	$1, %al
	je	.L193
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L193:
	movl	$1, %edx
	movl	%r14d, %esi
	call	*%rax
	movl	52(%rbx), %edx
	movl	%eax, %r12d
	movq	%rbx, %rdi
	imull	%eax, %edx
	movq	8(%r15), %rax
	movslq	%edx, %rsi
	leal	18(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm5
	leal	1(%rdx), %esi
	movslq	%ecx, %rcx
	vsubsd	(%rax,%rcx,8), %xmm5, %xmm5
	movslq	%esi, %rsi
	leal	19(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm2
	leal	6(%rdx), %esi
	movslq	%ecx, %rcx
	vsubsd	(%rax,%rcx,8), %xmm2, %xmm2
	movslq	%esi, %rsi
	leal	12(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm3
	leal	7(%rdx), %esi
	movslq	%ecx, %rcx
	vaddsd	(%rax,%rcx,8), %xmm3, %xmm3
	movslq	%esi, %rsi
	leal	13(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm1
	leal	2(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm5, -216(%rbp)
	vaddsd	(%rax,%rcx,8), %xmm1, %xmm1
	movslq	%esi, %rsi
	leal	20(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	3(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -224(%rbp)
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	21(%rdx), %ecx
	vmovsd	%xmm3, -232(%rbp)
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -240(%rbp)
	vmovsd	%xmm0, -248(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	8(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	14(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -256(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	9(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	15(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -264(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	4(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	22(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -272(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	5(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -280(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	10(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	16(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -288(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	leal	11(%rdx), %ecx
	addl	$17, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -296(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm0
	vaddsd	(%rax,%rdx,8), %xmm0, %xmm0
	movq	176(%rbx), %rax
	testb	$1, %al
	vmovsd	%xmm0, -304(%rbp)
	je	.L195
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L195:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L196
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L196:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L197
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L197:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L198
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L198:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L199
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L199:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L200
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L200:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	addq	184(%rbx), %rdi
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L202
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L202:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L203
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L203:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L204
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L204:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L205
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L205:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L206
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L206:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L207
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L207:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	addq	184(%rbx), %rdi
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L209
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L209:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L210
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L210:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L211
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L211:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L212
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L212:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L213
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L213:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L214
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L214:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
.LEHE8:
	vmovsd	-208(%rbp), %xmm1
	addl	56(%rbx), %eax
	vmovsd	-200(%rbp), %xmm9
	movq	8(%rbx), %rdx
	vmulsd	-216(%rbp), %xmm1, %xmm12
	movl	60(%rbx), %ecx
	addq	$4, %r13
	vmulsd	-224(%rbp), %xmm9, %xmm2
	vmulsd	-216(%rbp), %xmm9, %xmm11
	movq	8(%rdx), %rdx
	vmulsd	-232(%rbp), %xmm1, %xmm10
	imull	68(%rbx), %eax
	imull	52(%rbx), %r14d
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	leal	5(%rax,%rcx,4), %eax
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	-424(%rbp), %rdx
	vmovsd	%xmm0, -72(%rbp)
	movq	8(%rdx), %rax
	movslq	%r14d, %rdx
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	vmovsd	-160(%rbp), %xmm1
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-152(%rbp), %xmm2
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-248(%rbp), %xmm2, %xmm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-112(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-104(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	-288(%rbp), %xmm1, %xmm3
	vaddsd	%xmm4, %xmm8, %xmm4
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm3, %xmm8, %xmm3
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm3, %xmm7, %xmm3
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm4, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	1(%r14), %edx
	vmovsd	-184(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-224(%rbp), %xmm9, %xmm2
	vmulsd	-216(%rbp), %xmm9, %xmm11
	vaddsd	(%rdx), %xmm3, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	6(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	7(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	12(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	13(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	18(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	19(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm3, %xmm1, %xmm3
	vmovsd	-192(%rbp), %xmm1
	vmulsd	-216(%rbp), %xmm1, %xmm12
	vmulsd	-232(%rbp), %xmm1, %xmm10
	vmovsd	%xmm3, (%rdx)
	leal	2(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	vmovsd	-144(%rbp), %xmm1
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-136(%rbp), %xmm2
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-248(%rbp), %xmm2, %xmm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vxorpd	%xmm12, %xmm12, %xmm12
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-96(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-88(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	-288(%rbp), %xmm1, %xmm3
	vaddsd	%xmm4, %xmm8, %xmm4
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm3, %xmm8, %xmm3
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm3, %xmm7, %xmm3
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm4, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	3(%r14), %edx
	vmovsd	-80(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	8(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	9(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	14(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	15(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	20(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	21(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm3, %xmm1, %xmm3
	vmovsd	-176(%rbp), %xmm1
	vmulsd	-216(%rbp), %xmm1, %xmm6
	vmulsd	-232(%rbp), %xmm1, %xmm4
	vmovsd	%xmm3, (%rdx)
	leal	4(%r14), %edx
	vmovsd	-168(%rbp), %xmm3
	movslq	%edx, %rdx
	vmulsd	-224(%rbp), %xmm3, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-216(%rbp), %xmm3, %xmm5
	vaddsd	%xmm2, %xmm6, %xmm6
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	vaddsd	%xmm12, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm5, %xmm5
	vmulsd	-240(%rbp), %xmm3, %xmm2
	vmulsd	-232(%rbp), %xmm3, %xmm3
	vsubsd	%xmm5, %xmm12, %xmm5
	vaddsd	%xmm2, %xmm4, %xmm4
	vmovsd	-120(%rbp), %xmm2
	vsubsd	%xmm1, %xmm3, %xmm3
	vmovsd	-128(%rbp), %xmm1
	vmulsd	-256(%rbp), %xmm2, %xmm7
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm12, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm12, %xmm3
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	-256(%rbp), %xmm1, %xmm8
	vaddsd	%xmm7, %xmm6, %xmm7
	vmulsd	-248(%rbp), %xmm2, %xmm6
	vsubsd	%xmm8, %xmm6, %xmm6
	vmulsd	-264(%rbp), %xmm1, %xmm8
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm5, %xmm5
	vmulsd	-272(%rbp), %xmm2, %xmm6
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vaddsd	%xmm6, %xmm8, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-280(%rbp), %xmm0, %xmm2
	vaddsd	%xmm6, %xmm4, %xmm4
	vmulsd	-280(%rbp), %xmm9, %xmm6
	vsubsd	%xmm1, %xmm3, %xmm3
	vmulsd	-288(%rbp), %xmm0, %xmm1
	vaddsd	%xmm1, %xmm6, %xmm6
	vmulsd	-288(%rbp), %xmm9, %xmm1
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-296(%rbp), %xmm9, %xmm2
	vmulsd	-304(%rbp), %xmm9, %xmm9
	vsubsd	%xmm1, %xmm5, %xmm5
	vmulsd	-304(%rbp), %xmm0, %xmm1
	vmulsd	-296(%rbp), %xmm0, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	5(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	17(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	leal	22(%r14), %edx
	addl	$23, %r14d
	movslq	%r14d, %r14
	cmpq	-312(%rbp), %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r14,8), %rax
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm7, %xmm0, %xmm7
	vmovsd	%xmm7, (%rdx)
	vmovsd	(%rax), %xmm0
	vsubsd	%xmm5, %xmm0, %xmm5
	vmovsd	%xmm5, (%rax)
	jne	.L215
.L165:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L223
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L218:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB9:
	call	_Unwind_Resume
.LEHE9:
.L223:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2816:
	.section	.gcc_except_table
.LLSDA2816:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2816-.LLSDACSB2816
.LLSDACSB2816:
	.uleb128 .LEHB8-.LFB2816
	.uleb128 .LEHE8-.LEHB8
	.uleb128 .L218-.LFB2816
	.uleb128 0
	.uleb128 .LEHB9-.LFB2816
	.uleb128 .LEHE9-.LEHB9
	.uleb128 0
	.uleb128 0
.LLSDACSE2816:
	.text
	.size	_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_:
.LFB2815:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2815
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	movq	%rdx, %r15
	.cfi_offset 15, -24
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	subq	$568, %rsp
	movq	%rsi, -552(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -556(%rbp)
	movq	96(%rdi), %rax
	movq	(%rax), %rdx
	movq	8(%rdx), %rcx
	movq	%rcx, -576(%rbp)
	movq	(%rdx), %rcx
	subq	%rcx, -576(%rbp)
	sarq	$2, -576(%rbp)
	movl	-576(%rbp), %edx
	movl	%edx, -560(%rbp)
	movq	136(%rdi), %rdx
	movl	-560(%rbp), %r10d
	movq	(%rdx), %rdx
	movq	(%rdx), %rcx
	movq	8(%rdx), %rdx
	movq	%rdx, -592(%rbp)
	movl	-556(%rbp), %edx
	imull	-560(%rbp), %edx
	movq	%rcx, -584(%rbp)
	movl	%edx, -596(%rbp)
	movslq	%edx, %rdx
	leaq	46(,%rdx,8), %rdx
	movq	%rdx, -608(%rbp)
	andq	$-16, %rdx
	subq	%rdx, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
	testl	%r10d, %r10d
	jle	.L225
	movl	-576(%rbp), %edx
	movq	$0, -352(%rbp)
	xorl	%r14d, %r14d
	subl	$1, %edx
	leaq	4(,%rdx,4), %rdx
	movq	%rdx, -568(%rbp)
	jmp	.L245
	.p2align 4,,10
	.p2align 3
.L277:
	movq	96(%rbx), %rax
.L245:
	movq	(%rax), %rax
	movq	-352(%rbp), %rcx
	movq	%rbx, %rdi
	movl	52(%rbx), %edx
	movq	(%rax), %rax
	movl	(%rax,%rcx), %r13d
	movq	8(%r15), %rax
	imull	%r13d, %edx
	movslq	%edx, %rcx
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	19(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -360(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	1(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -368(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	18(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -376(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	6(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -384(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	13(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -392(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	7(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -400(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	12(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -408(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	2(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -416(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	21(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -424(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	3(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -432(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	20(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -440(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	8(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -448(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	15(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -456(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	9(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -464(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	14(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -472(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	4(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -480(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -488(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	5(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -496(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm2
	leal	22(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -504(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm12
	leal	10(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm12, -512(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm0
	leal	17(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -520(%rbp)
	vmovsd	(%rax,%rcx,8), %xmm1
	leal	11(%rdx), %ecx
	addl	$16, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	(%rax,%rdx,8), %xmm12
	vmovsd	(%rax,%rcx,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -528(%rbp)
	vmovsd	%xmm2, -536(%rbp)
	testb	$1, %al
	vmovsd	%xmm12, -544(%rbp)
	je	.L227
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L227:
	movl	%r13d, %esi
.LEHB10:
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	176(%rbx), %rax
	vmovsd	%xmm0, -216(%rbp)
	testb	$1, %al
	je	.L228
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L228:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm1
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -224(%rbp)
	testb	$1, %al
	je	.L229
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L229:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm2, -232(%rbp)
	testb	$1, %al
	je	.L230
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L230:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm12
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -240(%rbp)
	testb	$1, %al
	je	.L231
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L231:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	176(%rbx), %rax
	vmovsd	%xmm0, -248(%rbp)
	testb	$1, %al
	je	.L232
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L232:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm1
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -256(%rbp)
	testb	$1, %al
	je	.L233
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L233:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm2, -264(%rbp)
	testb	$1, %al
	je	.L234
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L234:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm12
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -272(%rbp)
	testb	$1, %al
	je	.L235
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L235:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	176(%rbx), %rax
	vmovsd	%xmm0, -280(%rbp)
	testb	$1, %al
	je	.L236
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L236:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm1
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -288(%rbp)
	testb	$1, %al
	je	.L237
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L237:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm2, -296(%rbp)
	testb	$1, %al
	je	.L238
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L238:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm12
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -304(%rbp)
	testb	$1, %al
	je	.L239
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L239:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	176(%rbx), %rax
	vmovsd	%xmm0, -312(%rbp)
	testb	$1, %al
	je	.L240
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L240:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm1
	movq	176(%rbx), %rax
	vmovsd	%xmm1, -320(%rbp)
	testb	$1, %al
	je	.L241
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L241:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm2
	movq	176(%rbx), %rax
	vmovsd	%xmm2, -328(%rbp)
	testb	$1, %al
	je	.L242
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L242:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm12
	movq	176(%rbx), %rax
	vmovsd	%xmm12, -336(%rbp)
	testb	$1, %al
	je	.L243
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L243:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	176(%rbx), %rax
	vmovsd	%xmm0, -344(%rbp)
	testb	$1, %al
	je	.L244
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L244:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	vmovsd	-360(%rbp), %xmm1
	movl	60(%rbx), %ecx
	vmovsd	-376(%rbp), %xmm2
	movq	8(%rbx), %rdx
	vaddsd	-368(%rbp), %xmm1, %xmm11
	vmovsd	-392(%rbp), %xmm12
	vsubsd	-384(%rbp), %xmm2, %xmm10
	vmovsd	-408(%rbp), %xmm0
	imull	68(%rbx), %eax
	vaddsd	-400(%rbp), %xmm12, %xmm9
	vmovsd	-456(%rbp), %xmm12
	vsubsd	-416(%rbp), %xmm0, %xmm8
	vmovsd	-424(%rbp), %xmm1
	vaddsd	-464(%rbp), %xmm12, %xmm5
	vmovsd	-440(%rbp), %xmm2
	vmulsd	-216(%rbp), %xmm11, %xmm14
	movq	8(%rdx), %rdx
	movslq	%r14d, %rsi
	vmulsd	-224(%rbp), %xmm10, %xmm13
	vaddsd	-432(%rbp), %xmm1, %xmm7
	vmovsd	-472(%rbp), %xmm0
	leal	5(%rax,%rcx,4), %eax
	vsubsd	-448(%rbp), %xmm2, %xmm6
	vmovsd	-504(%rbp), %xmm12
	leal	1(%r14), %ecx
	vsubsd	-480(%rbp), %xmm0, %xmm4
	vmovsd	-488(%rbp), %xmm1
	vsubsd	-512(%rbp), %xmm12, %xmm2
	vmovsd	-520(%rbp), %xmm0
	vmovsd	-536(%rbp), %xmm12
	cltq
	vaddsd	-496(%rbp), %xmm1, %xmm3
	movslq	%ecx, %rcx
	vaddsd	-528(%rbp), %xmm0, %xmm1
	vaddsd	%xmm13, %xmm14, %xmm13
	vsubsd	-544(%rbp), %xmm12, %xmm0
	vmovsd	(%rdx,%rax,8), %xmm12
	leal	6(%r14), %edx
	leal	7(%r14), %eax
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	movslq	%edx, %rdx
	cltq
	movq	$0, (%r12,%rdx,8)
	movq	$0, (%r12,%rax,8)
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-224(%rbp), %xmm11, %xmm14
	vmulsd	-216(%rbp), %xmm10, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-216(%rbp), %xmm9, %xmm14
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-224(%rbp), %xmm8, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-224(%rbp), %xmm9, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-216(%rbp), %xmm8, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-264(%rbp), %xmm7, %xmm14
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-272(%rbp), %xmm6, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-272(%rbp), %xmm7, %xmm14
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-264(%rbp), %xmm6, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-264(%rbp), %xmm5, %xmm14
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-272(%rbp), %xmm4, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-272(%rbp), %xmm5, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-264(%rbp), %xmm4, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-312(%rbp), %xmm3, %xmm14
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-320(%rbp), %xmm2, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-320(%rbp), %xmm3, %xmm14
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-312(%rbp), %xmm2, %xmm13
	leal	2(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-312(%rbp), %xmm1, %xmm14
	leal	3(%r14), %ecx
	vmulsd	-320(%rbp), %xmm0, %xmm13
	movslq	%ecx, %rcx
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-320(%rbp), %xmm1, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-312(%rbp), %xmm0, %xmm13
	leal	8(%r14), %edx
	movslq	%edx, %rdx
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-232(%rbp), %xmm11, %xmm14
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-240(%rbp), %xmm10, %xmm13
	leal	9(%r14), %eax
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	cltq
	movq	$0, (%r12,%rdx,8)
	movq	$0, (%r12,%rax,8)
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm11, %xmm14
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-232(%rbp), %xmm10, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-232(%rbp), %xmm9, %xmm14
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-240(%rbp), %xmm8, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm9, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-232(%rbp), %xmm8, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-280(%rbp), %xmm7, %xmm14
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-288(%rbp), %xmm6, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-288(%rbp), %xmm7, %xmm14
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-280(%rbp), %xmm6, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-280(%rbp), %xmm5, %xmm14
	vmulsd	-288(%rbp), %xmm4, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-288(%rbp), %xmm5, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-280(%rbp), %xmm4, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-328(%rbp), %xmm3, %xmm14
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-336(%rbp), %xmm2, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-336(%rbp), %xmm3, %xmm14
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmulsd	-328(%rbp), %xmm2, %xmm13
	leal	4(%r14), %esi
	movslq	%esi, %rsi
	vsubsd	%xmm13, %xmm14, %xmm13
	vmovsd	(%r12,%rcx,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-328(%rbp), %xmm1, %xmm14
	vmovsd	%xmm13, (%r12,%rcx,8)
	vmulsd	-336(%rbp), %xmm0, %xmm13
	leal	5(%r14), %ecx
	movslq	%ecx, %rcx
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-328(%rbp), %xmm0, %xmm14
	vaddsd	(%r12,%rdx,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rdx,8)
	vmulsd	-336(%rbp), %xmm1, %xmm13
	leal	10(%r14), %edx
	movslq	%edx, %rdx
	vsubsd	%xmm14, %xmm13, %xmm13
	vmovsd	(%r12,%rax,8), %xmm14
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	-248(%rbp), %xmm11, %xmm14
	vmulsd	-256(%rbp), %xmm11, %xmm11
	vmovsd	%xmm13, (%r12,%rax,8)
	vmulsd	-256(%rbp), %xmm10, %xmm13
	leal	11(%r14), %eax
	movq	$0, (%r12,%rsi,8)
	movq	$0, (%r12,%rcx,8)
	cltq
	movq	$0, (%r12,%rdx,8)
	movq	$0, (%r12,%rax,8)
	vmulsd	-248(%rbp), %xmm10, %xmm10
	vaddsd	%xmm13, %xmm14, %xmm13
	vsubsd	%xmm10, %xmm11, %xmm10
	vaddsd	(%r12,%rsi,8), %xmm13, %xmm13
	vmovsd	%xmm13, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vmovsd	%xmm10, (%r12,%rcx,8)
	vmulsd	-248(%rbp), %xmm9, %xmm11
	vmulsd	-256(%rbp), %xmm8, %xmm10
	vmulsd	-256(%rbp), %xmm9, %xmm9
	vmulsd	-248(%rbp), %xmm8, %xmm8
	vaddsd	%xmm10, %xmm11, %xmm10
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	(%r12,%rdx,8), %xmm10, %xmm10
	vmovsd	%xmm10, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm9
	vsubsd	%xmm8, %xmm9, %xmm8
	vmulsd	-296(%rbp), %xmm7, %xmm9
	vmulsd	-304(%rbp), %xmm7, %xmm7
	vmovsd	%xmm8, (%r12,%rax,8)
	vmulsd	-304(%rbp), %xmm6, %xmm8
	vmulsd	-296(%rbp), %xmm6, %xmm6
	vaddsd	%xmm8, %xmm9, %xmm8
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	(%r12,%rsi,8), %xmm8, %xmm8
	vmovsd	%xmm8, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm7
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	-296(%rbp), %xmm5, %xmm7
	vmulsd	-304(%rbp), %xmm5, %xmm5
	vmovsd	%xmm6, (%r12,%rcx,8)
	vmulsd	-304(%rbp), %xmm4, %xmm6
	vmulsd	-296(%rbp), %xmm4, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	(%r12,%rdx,8), %xmm6, %xmm6
	vmovsd	%xmm6, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm5
	vsubsd	%xmm4, %xmm5, %xmm4
	vmulsd	-344(%rbp), %xmm3, %xmm5
	vmulsd	%xmm12, %xmm3, %xmm3
	vmovsd	%xmm4, (%r12,%rax,8)
	vmulsd	%xmm12, %xmm2, %xmm4
	vmulsd	-344(%rbp), %xmm2, %xmm2
	vaddsd	%xmm4, %xmm5, %xmm4
	vsubsd	%xmm2, %xmm3, %xmm2
	vaddsd	(%r12,%rsi,8), %xmm4, %xmm4
	vmovsd	%xmm4, (%r12,%rsi,8)
	vmovsd	(%r12,%rcx,8), %xmm3
	vsubsd	%xmm2, %xmm3, %xmm2
	vmulsd	-344(%rbp), %xmm1, %xmm3
	vmulsd	%xmm12, %xmm1, %xmm1
	vmovsd	%xmm2, (%r12,%rcx,8)
	vmulsd	%xmm12, %xmm0, %xmm2
	vmulsd	-344(%rbp), %xmm0, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm2
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	(%r12,%rdx,8), %xmm2, %xmm2
	vmovsd	%xmm2, (%r12,%rdx,8)
	vmovsd	(%r12,%rax,8), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%r12,%rax,8)
	addq	$4, -352(%rbp)
	addl	-556(%rbp), %r14d
	movq	-568(%rbp), %rax
	cmpq	%rax, -352(%rbp)
	jne	.L277
.L225:
	movq	-608(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	call	_ZN12Communicator8instanceEv
	movl	-596(%rbp), %ecx
	xorl	%r8d, %r8d
	movq	%r12, %rdx
	movq	%r13, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_bkEPdS0_ii
	movl	-560(%rbp), %r9d
	testl	%r9d, %r9d
	jle	.L246
	movq	104(%rbx), %rax
	movq	-552(%rbp), %rcx
	xorl	%esi, %esi
	movq	%r15, -224(%rbp)
	movl	-556(%rbp), %r15d
	movl	52(%rbx), %r14d
	movq	%rbx, -216(%rbp)
	movq	(%rax), %rax
	movq	8(%rcx), %rdx
	xorl	%ecx, %ecx
	movq	(%rax), %r12
	movl	-576(%rbp), %eax
	subl	$1, %eax
	leaq	4(,%rax,4), %rax
	movq	%rax, %rbx
	.p2align 4,,10
	.p2align 3
.L247:
	movl	(%r12,%rsi), %eax
	leal	7(%rcx), %r11d
	addq	$4, %rsi
	movslq	%r11d, %r11
	imull	%r14d, %eax
	movslq	%eax, %rdi
	leal	1(%rax), %r9d
	leal	6(%rax), %r10d
	leaq	(%rdx,%rdi,8), %r8
	movslq	%ecx, %rdi
	movslq	%r9d, %r9
	movslq	%r10d, %r10
	vmovsd	(%r8), %xmm0
	leaq	(%rdx,%r9,8), %r9
	leaq	(%rdx,%r10,8), %r10
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	1(%rcx), %r8d
	vmovsd	(%r9), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	6(%rcx), %r9d
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	7(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	12(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	9(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	13(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	18(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	8(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	19(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	3(%rax), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	2(%rcx), %edi
	movslq	%edi, %rdi
	vmovsd	%xmm0, (%r8)
	leal	2(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	(%r8), %xmm0
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	3(%rcx), %r8d
	vmovsd	(%r9), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	8(%rcx), %r9d
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	9(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	14(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	leal	11(%rcx), %r11d
	movslq	%r11d, %r11
	vmovsd	%xmm0, (%r10)
	leal	15(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	20(%rax), %r9d
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	leal	10(%rax), %r10d
	vmovsd	(%r9), %xmm0
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	%xmm0, (%r9)
	leal	5(%rax), %r9d
	vmovsd	(%r8), %xmm0
	movslq	%r9d, %r9
	leaq	(%rdx,%r9,8), %r9
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	leal	4(%rcx), %edi
	movslq	%edi, %rdi
	vmovsd	%xmm0, (%r8)
	leal	4(%rax), %r8d
	movslq	%r8d, %r8
	leaq	(%rdx,%r8,8), %r8
	vmovsd	(%r8), %xmm0
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r8)
	leal	5(%rcx), %r8d
	vmovsd	(%r9), %xmm0
	movslq	%r8d, %r8
	vaddsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	leal	10(%rcx), %r9d
	addl	%r15d, %ecx
	vmovsd	(%r10), %xmm0
	movslq	%r9d, %r9
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	11(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	16(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vsubsd	0(%r13,%r11,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r10)
	leal	17(%rax), %r10d
	movslq	%r10d, %r10
	leaq	(%rdx,%r10,8), %r10
	vmovsd	(%r10), %xmm0
	vaddsd	0(%r13,%r9,8), %xmm0, %xmm0
	leal	22(%rax), %r9d
	addl	$23, %eax
	cltq
	cmpq	%rbx, %rsi
	movslq	%r9d, %r9
	leaq	(%rdx,%rax,8), %rax
	leaq	(%rdx,%r9,8), %r9
	vmovsd	%xmm0, (%r10)
	vmovsd	(%r9), %xmm0
	vsubsd	0(%r13,%r8,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%r9)
	vmovsd	(%rax), %xmm0
	vaddsd	0(%r13,%rdi,8), %xmm0, %xmm0
	vmovsd	%xmm0, (%rax)
	jne	.L247
	movq	-216(%rbp), %rbx
	movq	-224(%rbp), %r15
.L246:
	movq	-592(%rbp), %rax
	subq	-584(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L224
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %rax
	movq	%rax, -312(%rbp)
	.p2align 4,,10
	.p2align 3
.L270:
	movq	136(%rbx), %rax
	movq	%rbx, %rdi
	addq	216(%rbx), %rdi
	movq	(%rax), %rax
	movq	(%rax), %rax
	movl	(%rax,%r13), %r14d
	movq	208(%rbx), %rax
	testb	$1, %al
	je	.L250
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L250:
	xorl	%edx, %edx
	movl	%r14d, %esi
	call	*%rax
	movl	52(%rbx), %edx
	movl	%eax, %r12d
	movq	%rbx, %rdi
	imull	%eax, %edx
	movq	8(%r15), %rax
	movslq	%edx, %rsi
	leal	19(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm5
	leal	1(%rdx), %esi
	movslq	%ecx, %rcx
	vaddsd	(%rax,%rcx,8), %xmm5, %xmm5
	movslq	%esi, %rsi
	leal	18(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm2
	leal	6(%rdx), %esi
	movslq	%ecx, %rcx
	vsubsd	(%rax,%rcx,8), %xmm2, %xmm2
	movslq	%esi, %rsi
	leal	13(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm3
	leal	7(%rdx), %esi
	movslq	%ecx, %rcx
	vaddsd	(%rax,%rcx,8), %xmm3, %xmm3
	movslq	%esi, %rsi
	leal	12(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm1
	leal	2(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm5, -216(%rbp)
	vsubsd	(%rax,%rcx,8), %xmm1, %xmm1
	movslq	%esi, %rsi
	leal	21(%rdx), %ecx
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	3(%rdx), %esi
	movslq	%ecx, %rcx
	vmovsd	%xmm2, -224(%rbp)
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	20(%rdx), %ecx
	vmovsd	%xmm3, -232(%rbp)
	movslq	%ecx, %rcx
	vmovsd	%xmm1, -240(%rbp)
	vmovsd	%xmm0, -248(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	8(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	15(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -256(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	9(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	14(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -264(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	4(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	23(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -272(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	5(%rdx), %esi
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	22(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -280(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	leal	10(%rdx), %esi
	vsubsd	(%rax,%rcx,8), %xmm0, %xmm0
	movslq	%esi, %rsi
	leal	17(%rdx), %ecx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -288(%rbp)
	vmovsd	(%rax,%rsi,8), %xmm0
	vaddsd	(%rax,%rcx,8), %xmm0, %xmm0
	leal	11(%rdx), %ecx
	addl	$16, %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vmovsd	%xmm0, -296(%rbp)
	addq	184(%rbx), %rdi
	vmovsd	(%rax,%rcx,8), %xmm0
	vsubsd	(%rax,%rdx,8), %xmm0, %xmm0
	movq	176(%rbx), %rax
	testb	$1, %al
	vmovsd	%xmm0, -304(%rbp)
	je	.L252
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L252:
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L253
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L253:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L254
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L254:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L255
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L255:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L256
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L256:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L257
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L257:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L258
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L258:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L259
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L259:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L260
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L260:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L261
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L261:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L262
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L262:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L263
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L263:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L264
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L264:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L265
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L265:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L266
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L266:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L267
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L267:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L268
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L268:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	176(%rbx), %rax
	testb	$1, %al
	je	.L269
	movq	184(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L269:
	movq	%rbx, %rdi
	addq	184(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
.LEHE10:
	vmovsd	-208(%rbp), %xmm1
	movq	8(%rbx), %rdx
	vmovsd	-200(%rbp), %xmm9
	movl	60(%rbx), %ecx
	vmulsd	-216(%rbp), %xmm1, %xmm12
	addq	$4, %r13
	vmulsd	-224(%rbp), %xmm9, %xmm2
	movq	8(%rdx), %rdx
	vmulsd	-216(%rbp), %xmm9, %xmm11
	vmulsd	-232(%rbp), %xmm1, %xmm10
	imull	68(%rbx), %eax
	imull	52(%rbx), %r14d
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	leal	5(%rax,%rcx,4), %eax
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	cltq
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	-552(%rbp), %rdx
	vmovsd	%xmm0, -72(%rbp)
	movq	8(%rdx), %rax
	movslq	%r14d, %rdx
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	vmovsd	-160(%rbp), %xmm1
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-152(%rbp), %xmm2
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-248(%rbp), %xmm2, %xmm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-112(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-104(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vmulsd	-288(%rbp), %xmm1, %xmm4
	vaddsd	%xmm3, %xmm8, %xmm3
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm4, %xmm8, %xmm4
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm4, %xmm7, %xmm4
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	1(%r14), %edx
	vmovsd	-184(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-224(%rbp), %xmm9, %xmm2
	vmulsd	-216(%rbp), %xmm9, %xmm11
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	6(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	7(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	12(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm5, %xmm1, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	13(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	18(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	-192(%rbp), %xmm1
	vmulsd	-216(%rbp), %xmm1, %xmm12
	vmulsd	-232(%rbp), %xmm1, %xmm10
	vmovsd	%xmm4, (%rdx)
	leal	19(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm2, %xmm12, %xmm12
	vxorpd	%xmm2, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm3
	vaddsd	%xmm2, %xmm12, %xmm12
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	vmovsd	%xmm3, (%rdx)
	leal	2(%r14), %edx
	movslq	%edx, %rdx
	vsubsd	%xmm2, %xmm11, %xmm11
	vxorpd	%xmm2, %xmm2, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm11, %xmm2, %xmm11
	vmulsd	-240(%rbp), %xmm9, %xmm2
	vmulsd	-232(%rbp), %xmm9, %xmm9
	vaddsd	%xmm2, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm9, %xmm9
	vmovsd	-144(%rbp), %xmm1
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	-264(%rbp), %xmm1, %xmm6
	vsubsd	%xmm9, %xmm2, %xmm9
	vmovsd	-136(%rbp), %xmm2
	vmulsd	-256(%rbp), %xmm2, %xmm3
	vmulsd	-248(%rbp), %xmm2, %xmm7
	vaddsd	%xmm3, %xmm8, %xmm8
	vmulsd	-256(%rbp), %xmm1, %xmm3
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vaddsd	%xmm8, %xmm12, %xmm8
	vxorpd	%xmm12, %xmm12, %xmm12
	vsubsd	%xmm3, %xmm7, %xmm7
	vmulsd	-272(%rbp), %xmm2, %xmm3
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vsubsd	%xmm7, %xmm11, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vmovsd	-96(%rbp), %xmm1
	vmulsd	-280(%rbp), %xmm1, %xmm4
	vaddsd	%xmm6, %xmm10, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm5
	vmovsd	-88(%rbp), %xmm2
	vmulsd	-288(%rbp), %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm3
	vmulsd	-288(%rbp), %xmm1, %xmm4
	vaddsd	%xmm3, %xmm8, %xmm3
	vmulsd	-280(%rbp), %xmm2, %xmm8
	vsubsd	%xmm4, %xmm8, %xmm4
	vmulsd	-296(%rbp), %xmm1, %xmm8
	vmulsd	-304(%rbp), %xmm1, %xmm1
	vsubsd	%xmm4, %xmm7, %xmm4
	vmulsd	-304(%rbp), %xmm2, %xmm7
	vmulsd	-296(%rbp), %xmm2, %xmm2
	vaddsd	%xmm7, %xmm8, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm7, %xmm6, %xmm7
	vsubsd	%xmm2, %xmm5, %xmm5
	vmovsd	%xmm1, (%rdx)
	leal	3(%r14), %edx
	vmovsd	-80(%rbp), %xmm9
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	8(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	9(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	14(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm5, %xmm1, %xmm5
	vmovsd	%xmm5, (%rdx)
	leal	15(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	20(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	-176(%rbp), %xmm1
	vmulsd	-216(%rbp), %xmm1, %xmm6
	vmovsd	%xmm4, (%rdx)
	leal	21(%r14), %edx
	vmulsd	-232(%rbp), %xmm1, %xmm4
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	leal	4(%r14), %edx
	vmovsd	-168(%rbp), %xmm3
	movslq	%edx, %rdx
	vmulsd	-224(%rbp), %xmm3, %xmm2
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	-216(%rbp), %xmm3, %xmm5
	vaddsd	%xmm2, %xmm6, %xmm6
	vmulsd	-224(%rbp), %xmm1, %xmm2
	vmulsd	-240(%rbp), %xmm1, %xmm1
	vaddsd	%xmm12, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm5, %xmm5
	vmulsd	-240(%rbp), %xmm3, %xmm2
	vmulsd	-232(%rbp), %xmm3, %xmm3
	vsubsd	%xmm5, %xmm12, %xmm5
	vaddsd	%xmm2, %xmm4, %xmm4
	vmovsd	-120(%rbp), %xmm2
	vsubsd	%xmm1, %xmm3, %xmm3
	vmovsd	-128(%rbp), %xmm1
	vmulsd	-256(%rbp), %xmm2, %xmm7
	vmulsd	-248(%rbp), %xmm1, %xmm8
	vaddsd	%xmm12, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm12, %xmm3
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	-256(%rbp), %xmm1, %xmm8
	vaddsd	%xmm7, %xmm6, %xmm7
	vmulsd	-248(%rbp), %xmm2, %xmm6
	vsubsd	%xmm8, %xmm6, %xmm6
	vmulsd	-264(%rbp), %xmm1, %xmm8
	vmulsd	-272(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm5, %xmm5
	vmulsd	-272(%rbp), %xmm2, %xmm6
	vmulsd	-264(%rbp), %xmm2, %xmm2
	vaddsd	%xmm6, %xmm8, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-280(%rbp), %xmm0, %xmm2
	vaddsd	%xmm6, %xmm4, %xmm4
	vmulsd	-280(%rbp), %xmm9, %xmm6
	vsubsd	%xmm1, %xmm3, %xmm3
	vmulsd	-288(%rbp), %xmm0, %xmm1
	vaddsd	%xmm1, %xmm6, %xmm6
	vmulsd	-288(%rbp), %xmm9, %xmm1
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulsd	-296(%rbp), %xmm9, %xmm2
	vmulsd	-304(%rbp), %xmm9, %xmm9
	vsubsd	%xmm1, %xmm5, %xmm5
	vmulsd	-304(%rbp), %xmm0, %xmm1
	vmulsd	-296(%rbp), %xmm0, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	5(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm5, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm3, %xmm0, %xmm3
	vmovsd	%xmm3, (%rdx)
	leal	17(%r14), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	22(%r14), %edx
	addl	$23, %r14d
	movslq	%r14d, %r14
	cmpq	-312(%rbp), %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r14,8), %rax
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm5, %xmm0, %xmm5
	vmovsd	%xmm5, (%rdx)
	vaddsd	(%rax), %xmm7, %xmm7
	vmovsd	%xmm7, (%rax)
	jne	.L270
.L224:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L278
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L273:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB11:
	call	_Unwind_Resume
.LEHE11:
.L278:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2815:
	.section	.gcc_except_table
.LLSDA2815:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2815-.LLSDACSB2815
.LLSDACSB2815:
	.uleb128 .LEHB10-.LFB2815
	.uleb128 .LEHE10-.LEHB10
	.uleb128 .L273-.LFB2815
	.uleb128 0
	.uleb128 .LEHB11-.LFB2815
	.uleb128 .LEHE11-.LEHB11
	.uleb128 0
	.uleb128 0
.LLSDACSE2815:
	.text
	.size	_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_:
.LFB2814:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2814
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	subq	$264, %rsp
	movq	%rsi, -256(%rbp)
	movq	%rdx, -264(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -268(%rbp)
	movq	120(%rdi), %rax
	movl	-268(%rbp), %esi
	movq	(%rax), %rax
	movq	72(%rax), %r9
	movq	80(%rax), %r14
	movq	144(%rdi), %rax
	subq	%r9, %r14
	movq	(%rax), %rax
	sarq	$2, %r14
	imull	%r14d, %esi
	movl	%r14d, %r15d
	movq	72(%rax), %rdx
	movq	80(%rax), %rax
	movl	%esi, -216(%rbp)
	movq	%rax, -296(%rbp)
	movslq	%esi, %rax
	movq	%rdx, -288(%rbp)
	leaq	46(,%rax,8), %r12
	movq	%r12, %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	testl	%r14d, %r14d
	jle	.L280
	movq	-264(%rbp), %rdx
	leal	-1(%r14), %eax
	movl	52(%rdi), %r11d
	movl	-268(%rbp), %r15d
	xorl	%esi, %esi
	movl	%r14d, -224(%rbp)
	leaq	4(,%rax,4), %r10
	xorl	%eax, %eax
	movq	8(%rdx), %rcx
	.p2align 4,,10
	.p2align 3
.L281:
	movl	(%r9,%rsi), %edx
	movslq	%eax, %rdi
	addq	$4, %rsi
	imull	%r11d, %edx
	leal	12(%rdx), %r8d
	movslq	%r8d, %r8
	vmovsd	(%rcx,%r8,8), %xmm0
	leal	13(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	1(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	18(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	6(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	19(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	7(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	14(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	2(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	15(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	3(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	20(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	8(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	21(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	9(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	16(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	4(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	17(%rdx), %r8d
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	5(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	leal	22(%rdx), %r8d
	addl	$23, %edx
	vaddsd	%xmm0, %xmm0, %xmm0
	movslq	%edx, %rdx
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	10(%rax), %edi
	vmovsd	(%rcx,%r8,8), %xmm0
	movslq	%edi, %rdi
	vaddsd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	11(%rax), %edi
	addl	%r15d, %eax
	cmpq	%r10, %rsi
	vmovsd	(%rcx,%rdx,8), %xmm0
	movslq	%edi, %rdi
	vaddsd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	jne	.L281
	movl	-224(%rbp), %r15d
.L280:
	andq	$-16, %r12
	subq	%r12, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
.LEHB12:
	call	_ZN12Communicator8instanceEv
	movl	-216(%rbp), %ecx
	movl	$2, %r8d
	movq	%r13, %rdx
	movq	%r12, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_fwEPdS0_ii
	testl	%r15d, %r15d
	jle	.L282
	subl	$1, %r14d
	movq	$0, -216(%rbp)
	leaq	4(,%r14,4), %r14
	movq	%r14, -280(%rbp)
	xorl	%r14d, %r14d
	.p2align 4,,10
	.p2align 3
.L304:
	movq	112(%rbx), %rax
	movq	-216(%rbp), %r15
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	72(%rax), %rax
	movl	(%rax,%r15), %r13d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L284
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L284:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L285
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L285:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L286
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L286:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L287
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L287:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L288
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L288:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L289
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L289:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L291
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L291:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L292
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L292:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	1(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L293
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L293:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	2(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L294
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L294:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	3(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L295
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L295:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	4(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L296
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L296:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movq	%rbx, %rdi
	movl	60(%rbx), %esi
	addq	168(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	5(%rdx,%rsi,2), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L298
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L298:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L299
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L299:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	1(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L300
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L300:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	2(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L301
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L301:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	3(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L302
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L302:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%eax, %edx
	movq	8(%rcx), %rax
	imull	68(%rbx), %edx
	leal	4(%rdx,%rsi,4), %edx
	movslq	%edx, %rdx
	movq	(%rax,%rdx,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L303
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L303:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	leal	1(%r14), %r10d
	movslq	%r14d, %r11
	leal	6(%r14), %r9d
	vmovsd	-200(%rbp), %xmm2
	leal	7(%r14), %r8d
	movslq	%r10d, %r10
	vmovsd	-208(%rbp), %xmm1
	vmovsd	(%r12,%r11,8), %xmm4
	movl	56(%rbx), %edx
	vmovsd	(%r12,%r10,8), %xmm3
	movslq	%r9d, %r9
	vmulsd	%xmm2, %xmm4, %xmm10
	movslq	%r8d, %r8
	movl	60(%rbx), %esi
	vmulsd	%xmm3, %xmm2, %xmm5
	leal	(%rdx,%rdx,2), %edx
	leal	2(%r14), %edi
	vmulsd	%xmm3, %xmm1, %xmm3
	movq	8(%rbx), %rcx
	movq	-256(%rbp), %r15
	vmulsd	%xmm4, %xmm1, %xmm6
	addl	%eax, %edx
	movslq	%edi, %rdi
	vmovsd	(%r12,%r9,8), %xmm4
	imull	68(%rbx), %edx
	vaddsd	%xmm3, %xmm10, %xmm10
	vmovsd	(%r12,%r8,8), %xmm3
	vmulsd	%xmm4, %xmm1, %xmm7
	vsubsd	%xmm5, %xmm6, %xmm5
	movq	8(%rcx), %rax
	vmulsd	%xmm3, %xmm2, %xmm6
	leal	8(%r14), %ecx
	vmulsd	%xmm3, %xmm1, %xmm1
	vxorpd	%xmm9, %xmm9, %xmm9
	movslq	%ecx, %rcx
	leal	5(%rdx,%rsi,4), %edx
	leal	3(%r14), %esi
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm7
	movslq	%esi, %rsi
	vmovsd	-184(%rbp), %xmm2
	vmovsd	(%r12,%rsi,8), %xmm3
	vaddsd	%xmm9, %xmm5, %xmm5
	vmovsd	(%r12,%rdi,8), %xmm4
	vaddsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-192(%rbp), %xmm1
	vaddsd	%xmm9, %xmm6, %xmm6
	vmulsd	%xmm3, %xmm2, %xmm8
	movslq	%edx, %rdx
	vmulsd	%xmm3, %xmm1, %xmm3
	vmovsd	(%rax,%rdx,8), %xmm0
	leal	9(%r14), %edx
	leal	4(%r14), %eax
	vaddsd	%xmm9, %xmm7, %xmm7
	vmulsd	%xmm4, %xmm1, %xmm9
	vmulsd	%xmm2, %xmm4, %xmm4
	movslq	%edx, %rdx
	cltq
	movq	%rax, -224(%rbp)
	vmovsd	%xmm0, -72(%rbp)
	imull	52(%rbx), %r13d
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	%xmm3, %xmm4, %xmm3
	vmovsd	(%r12,%rcx,8), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm9
	vaddsd	%xmm8, %xmm5, %xmm8
	vaddsd	%xmm3, %xmm10, %xmm10
	vmovsd	(%r12,%rdx,8), %xmm3
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-168(%rbp), %xmm2
	vaddsd	%xmm9, %xmm6, %xmm9
	vmovsd	(%r12,%rax,8), %xmm6
	leal	5(%r14), %eax
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-176(%rbp), %xmm1
	cltq
	vmovsd	(%r12,%rax,8), %xmm5
	movq	%rax, -232(%rbp)
	leal	10(%r14), %eax
	vmulsd	%xmm6, %xmm1, %xmm12
	vmulsd	%xmm5, %xmm2, %xmm11
	cltq
	vmovsd	(%r12,%rax,8), %xmm4
	movq	%rax, -240(%rbp)
	leal	11(%r14), %eax
	vmulsd	%xmm2, %xmm6, %xmm6
	cltq
	vmulsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	(%r12,%rax,8), %xmm3
	movq	%rax, -248(%rbp)
	movq	8(%r15), %rax
	leal	12(%r13), %r15d
	movslq	%r15d, %r15
	vaddsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm6
	vaddsd	%xmm11, %xmm8, %xmm8
	leaq	(%rax,%r15,8), %r15
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm10, %xmm5
	vaddsd	(%r15), %xmm8, %xmm8
	vmovsd	%xmm8, (%r15)
	leal	13(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	vmulsd	%xmm3, %xmm2, %xmm5
	leal	18(%r13), %r15d
	vmulsd	%xmm4, %xmm2, %xmm2
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vsubsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-152(%rbp), %xmm2
	vaddsd	%xmm5, %xmm9, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm7, %xmm1
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	19(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm1
	vmovsd	%xmm1, (%r15)
	movq	-224(%rbp), %r15
	vmovsd	-160(%rbp), %xmm1
	vmovsd	(%r12,%r11,8), %xmm4
	vmovsd	(%r12,%r10,8), %xmm3
	vmulsd	%xmm2, %xmm4, %xmm10
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm4, %xmm1, %xmm6
	vmovsd	(%r12,%r9,8), %xmm4
	vaddsd	%xmm3, %xmm10, %xmm10
	vmovsd	(%r12,%r8,8), %xmm3
	vmulsd	%xmm4, %xmm1, %xmm7
	vsubsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm3, %xmm2, %xmm6
	vmulsd	%xmm3, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm3
	vaddsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm5, %xmm5
	vsubsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm7
	vmovsd	(%r12,%rdi,8), %xmm4
	vmovsd	-136(%rbp), %xmm2
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-144(%rbp), %xmm1
	vaddsd	%xmm9, %xmm6, %xmm6
	vmulsd	%xmm3, %xmm2, %xmm8
	vmulsd	%xmm3, %xmm1, %xmm3
	vaddsd	%xmm9, %xmm7, %xmm7
	vmulsd	%xmm4, %xmm1, %xmm9
	vmulsd	%xmm2, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm9, %xmm8
	vaddsd	%xmm3, %xmm4, %xmm3
	vmovsd	(%r12,%rcx,8), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm9
	vaddsd	%xmm8, %xmm5, %xmm8
	vaddsd	%xmm3, %xmm10, %xmm10
	vmovsd	(%r12,%rdx,8), %xmm3
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-120(%rbp), %xmm2
	vaddsd	%xmm9, %xmm6, %xmm9
	vmovsd	(%r12,%r15,8), %xmm6
	movq	-232(%rbp), %r15
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-128(%rbp), %xmm1
	vmovsd	(%r12,%r15,8), %xmm5
	movq	-240(%rbp), %r15
	vmulsd	%xmm6, %xmm1, %xmm12
	vmulsd	%xmm5, %xmm2, %xmm11
	vmovsd	(%r12,%r15,8), %xmm4
	movq	-248(%rbp), %r15
	vmulsd	%xmm2, %xmm6, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	(%r12,%r15,8), %xmm3
	leal	14(%r13), %r15d
	movslq	%r15d, %r15
	vaddsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm6
	vaddsd	%xmm11, %xmm8, %xmm8
	leaq	(%rax,%r15,8), %r15
	vmulsd	%xmm3, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm5, %xmm10, %xmm5
	vaddsd	(%r15), %xmm8, %xmm8
	vmovsd	%xmm8, (%r15)
	leal	15(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	vmulsd	%xmm3, %xmm2, %xmm5
	leal	20(%r13), %r15d
	vmulsd	%xmm4, %xmm2, %xmm2
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vsubsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm5, %xmm9, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm7, %xmm1
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	21(%r13), %r15d
	addq	$4, -216(%rbp)
	vmovsd	-104(%rbp), %xmm2
	addl	-268(%rbp), %r14d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm1
	vmovsd	%xmm1, (%r15)
	movq	-240(%rbp), %r15
	vmovsd	-112(%rbp), %xmm1
	vmovsd	(%r12,%r11,8), %xmm4
	vmovsd	(%r12,%r10,8), %xmm3
	vmulsd	%xmm4, %xmm1, %xmm10
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm3
	vsubsd	%xmm5, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm10, %xmm10
	vmulsd	%xmm2, %xmm4, %xmm9
	vmovsd	(%r12,%r9,8), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm6
	vaddsd	%xmm3, %xmm9, %xmm9
	vmovsd	(%r12,%r8,8), %xmm3
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm3
	vaddsd	%xmm11, %xmm9, %xmm9
	movq	-232(%rbp), %rsi
	vsubsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm4, %xmm2, %xmm6
	vmovsd	(%r12,%rdi,8), %xmm4
	vmovsd	-88(%rbp), %xmm2
	vaddsd	%xmm1, %xmm6, %xmm6
	vmovsd	-96(%rbp), %xmm1
	vmulsd	%xmm3, %xmm2, %xmm7
	vaddsd	%xmm11, %xmm5, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm8
	vmulsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm11, %xmm6, %xmm6
	vsubsd	%xmm7, %xmm8, %xmm8
	vaddsd	%xmm3, %xmm4, %xmm3
	vmovsd	(%r12,%rcx,8), %xmm4
	vaddsd	%xmm8, %xmm10, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm10
	vaddsd	%xmm3, %xmm9, %xmm9
	vmovsd	(%r12,%rdx,8), %xmm3
	movq	-224(%rbp), %rdx
	vmulsd	%xmm3, %xmm2, %xmm7
	vmulsd	%xmm3, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovsd	(%r12,%rsi,8), %xmm4
	vsubsd	%xmm7, %xmm10, %xmm7
	vmovsd	(%r12,%r15,8), %xmm3
	vmulsd	%xmm4, %xmm0, %xmm10
	movq	-280(%rbp), %r15
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm7, %xmm5, %xmm7
	vmovsd	(%r12,%rdx,8), %xmm5
	movq	-248(%rbp), %rdx
	vaddsd	%xmm1, %xmm6, %xmm6
	vmovsd	-80(%rbp), %xmm1
	vmulsd	%xmm5, %xmm1, %xmm11
	vmovsd	(%r12,%rdx,8), %xmm2
	leal	16(%r13), %edx
	vmulsd	%xmm0, %xmm5, %xmm5
	movslq	%edx, %rdx
	vmulsd	%xmm4, %xmm1, %xmm4
	vsubsd	%xmm10, %xmm11, %xmm10
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm4, %xmm5, %xmm4
	vmulsd	%xmm3, %xmm1, %xmm5
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm9, %xmm4
	vaddsd	(%rdx), %xmm8, %xmm8
	vmovsd	%xmm8, (%rdx)
	leal	17(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm4
	vmovsd	%xmm4, (%rdx)
	vmulsd	%xmm2, %xmm0, %xmm4
	leal	22(%r13), %edx
	vmulsd	%xmm3, %xmm0, %xmm0
	addl	$23, %r13d
	cmpq	%r15, -216(%rbp)
	movslq	%edx, %rdx
	movslq	%r13d, %r13
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r13,8), %rax
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm7, %xmm4
	vaddsd	%xmm0, %xmm6, %xmm0
	vaddsd	(%rdx), %xmm4, %xmm4
	vmovsd	%xmm4, (%rdx)
	vaddsd	(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, (%rax)
	jne	.L304
.L282:
	movq	-296(%rbp), %rax
	subq	-288(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L279
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %r14
	.p2align 4,,10
	.p2align 3
.L328:
	movq	144(%rbx), %rax
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	72(%rax), %rax
	movl	(%rax,%r13), %r12d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L307
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L307:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L308
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L308:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L309
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L309:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L310
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L310:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L311
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L311:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L312
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L312:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L314
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L314:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L315
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L315:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L316
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L316:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L317
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L317:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L318
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L318:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L319
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L319:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movq	%rbx, %rdi
	movl	60(%rbx), %esi
	addq	168(%rbx), %rdi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rsi,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L321
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L321:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L322
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L322:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L323
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L323:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L324
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L324:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L325
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L325:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L326
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L326:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	8(%rbx), %rcx
	movl	60(%rbx), %esi
	leal	(%rdx,%rdx,2), %edx
	addl	%edx, %eax
	movq	8(%rcx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rsi,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -72(%rbp)
	movq	192(%rbx), %rax
	testb	$1, %al
	je	.L327
	movq	200(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L327:
	movq	%rbx, %rdi
	addq	200(%rbx), %rdi
	movl	$3, %edx
	movl	%r12d, %esi
	call	*%rax
.LEHE12:
	movl	52(%rbx), %ecx
	movq	-264(%rbp), %rsi
	addq	$4, %r13
	vmovsd	-208(%rbp), %xmm12
	vmovsd	-200(%rbp), %xmm13
	imull	%ecx, %eax
	movq	8(%rsi), %rdx
	imull	%ecx, %r12d
	leal	12(%rax), %esi
	movslq	%esi, %rsi
	vmovsd	(%rdx,%rsi,8), %xmm11
	leal	13(%rax), %esi
	movslq	%esi, %rsi
	vaddsd	%xmm11, %xmm11, %xmm11
	vmovsd	(%rdx,%rsi,8), %xmm10
	leal	18(%rax), %esi
	vaddsd	%xmm10, %xmm10, %xmm10
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm11, %xmm15
	vmovsd	(%rdx,%rsi,8), %xmm9
	leal	19(%rax), %esi
	vmulsd	%xmm13, %xmm10, %xmm14
	movslq	%esi, %rsi
	vaddsd	%xmm9, %xmm9, %xmm9
	vmovsd	(%rdx,%rsi,8), %xmm8
	leal	14(%rax), %esi
	vaddsd	%xmm8, %xmm8, %xmm8
	movslq	%esi, %rsi
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	(%rdx,%rsi,8), %xmm7
	leal	15(%rax), %esi
	movslq	%esi, %rsi
	vaddsd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm13, %xmm11, %xmm15
	vmovsd	(%rdx,%rsi,8), %xmm6
	leal	20(%rax), %esi
	vaddsd	%xmm6, %xmm6, %xmm6
	movslq	%esi, %rsi
	vmovsd	%xmm14, -240(%rbp)
	vmulsd	%xmm12, %xmm10, %xmm14
	vmovsd	(%rdx,%rsi,8), %xmm5
	leal	21(%rax), %esi
	movslq	%esi, %rsi
	vaddsd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	(%rdx,%rsi,8), %xmm4
	leal	16(%rax), %esi
	vaddsd	%xmm4, %xmm4, %xmm4
	movslq	%esi, %rsi
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	(%rdx,%rsi,8), %xmm3
	leal	17(%rax), %esi
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	%xmm13, %xmm8, %xmm14
	movslq	%esi, %rsi
	vmulsd	%xmm13, %xmm9, %xmm13
	vaddsd	%xmm3, %xmm3, %xmm3
	vmovsd	(%rdx,%rsi,8), %xmm2
	leal	22(%rax), %esi
	addl	$23, %eax
	cltq
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm12, %xmm13, %xmm13
	vmovsd	-192(%rbp), %xmm12
	vaddsd	%xmm2, %xmm2, %xmm2
	movslq	%esi, %rsi
	vmovsd	(%rdx,%rsi,8), %xmm1
	movq	-256(%rbp), %rsi
	vaddsd	%xmm15, %xmm14, %xmm14
	vmovsd	(%rdx,%rax,8), %xmm0
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm12, %xmm15
	leal	12(%r12), %edx
	movq	8(%rsi), %rax
	vaddsd	%xmm1, %xmm1, %xmm1
	movslq	%edx, %rdx
	vaddsd	%xmm0, %xmm0, %xmm0
	vmovsd	%xmm14, -224(%rbp)
	vmovsd	%xmm13, -232(%rbp)
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	-184(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	-240(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -240(%rbp)
	vmulsd	%xmm6, %xmm12, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -248(%rbp)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vsubsd	%xmm14, %xmm15, %xmm14
	vaddsd	%xmm12, %xmm13, %xmm13
	vmovsd	-176(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm13, %xmm13
	vmovsd	%xmm14, -216(%rbp)
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	-168(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	-240(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	13(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vaddsd	-248(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	18(%r12), %edx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm14, %xmm15, %xmm14
	vaddsd	%xmm12, %xmm13, %xmm12
	vmovsd	-152(%rbp), %xmm13
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vaddsd	-224(%rbp), %xmm12, %xmm12
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	19(%r12), %edx
	vmulsd	%xmm10, %xmm13, %xmm14
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm12, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	14(%r12), %edx
	vmovsd	-160(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm12, %xmm15
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm13, %xmm15
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	%xmm10, %xmm12, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm9, %xmm12, %xmm15
	vmulsd	%xmm8, %xmm12, %xmm12
	vmovsd	%xmm14, -224(%rbp)
	vmulsd	%xmm8, %xmm13, %xmm14
	vmulsd	%xmm9, %xmm13, %xmm13
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm12, %xmm13, %xmm13
	vmovsd	-144(%rbp), %xmm12
	vaddsd	%xmm15, %xmm14, %xmm14
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm12, %xmm15
	vmovsd	%xmm14, -248(%rbp)
	vmovsd	%xmm13, -232(%rbp)
	vmovsd	-136(%rbp), %xmm13
	vmulsd	%xmm6, %xmm13, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm7, %xmm13, %xmm15
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	%xmm6, %xmm12, %xmm14
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm5, %xmm12, %xmm15
	vmulsd	%xmm4, %xmm12, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -240(%rbp)
	vmulsd	%xmm4, %xmm13, %xmm14
	vmulsd	%xmm5, %xmm13, %xmm13
	vsubsd	%xmm14, %xmm15, %xmm14
	vaddsd	%xmm12, %xmm13, %xmm13
	vmovsd	-128(%rbp), %xmm12
	vmulsd	%xmm3, %xmm12, %xmm15
	vaddsd	-248(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm13, %xmm13
	vmovsd	%xmm14, -224(%rbp)
	vmovsd	%xmm13, -232(%rbp)
	vmovsd	-120(%rbp), %xmm13
	vmulsd	%xmm2, %xmm13, %xmm14
	vsubsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm3, %xmm13, %xmm15
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm2, %xmm12, %xmm14
	leal	15(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm1, %xmm12, %xmm15
	vmulsd	%xmm0, %xmm12, %xmm12
	vaddsd	-240(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm0, %xmm13, %xmm14
	leal	20(%r12), %edx
	vmulsd	%xmm1, %xmm13, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm14, %xmm15, %xmm14
	vaddsd	%xmm12, %xmm13, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vaddsd	-232(%rbp), %xmm12, %xmm12
	leal	21(%r12), %edx
	vmovsd	-104(%rbp), %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm13, %xmm10, %xmm14
	vaddsd	(%rdx), %xmm12, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	16(%r12), %edx
	vmovsd	-112(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm12, %xmm11, %xmm15
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm13, %xmm11, %xmm11
	vmulsd	%xmm12, %xmm10, %xmm10
	vsubsd	%xmm14, %xmm15, %xmm14
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm10, %xmm11, %xmm10
	vmulsd	%xmm13, %xmm8, %xmm11
	vmulsd	%xmm12, %xmm8, %xmm8
	vaddsd	%xmm15, %xmm14, %xmm14
	vaddsd	%xmm15, %xmm10, %xmm10
	vmulsd	%xmm12, %xmm9, %xmm15
	vmulsd	%xmm13, %xmm9, %xmm9
	vmovsd	-96(%rbp), %xmm12
	vsubsd	%xmm11, %xmm15, %xmm15
	vxorpd	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm8, %xmm9, %xmm8
	vmulsd	%xmm12, %xmm7, %xmm13
	vaddsd	%xmm11, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm8, %xmm8
	vmovsd	-88(%rbp), %xmm11
	vmulsd	%xmm11, %xmm6, %xmm9
	vmulsd	%xmm11, %xmm7, %xmm7
	vmulsd	%xmm12, %xmm6, %xmm6
	vsubsd	%xmm9, %xmm13, %xmm9
	vaddsd	%xmm6, %xmm7, %xmm6
	vmulsd	%xmm12, %xmm5, %xmm7
	vmulsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm9, %xmm14, %xmm14
	vaddsd	%xmm6, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm4, %xmm6
	vmulsd	%xmm12, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	%xmm4, %xmm5, %xmm4
	vmovsd	-72(%rbp), %xmm5
	vaddsd	%xmm6, %xmm15, %xmm15
	vmulsd	%xmm5, %xmm2, %xmm6
	vaddsd	%xmm4, %xmm8, %xmm8
	vmovsd	-80(%rbp), %xmm4
	vmulsd	%xmm4, %xmm3, %xmm7
	vmulsd	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm7, %xmm6
	vaddsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm14, %xmm14
	vaddsd	%xmm2, %xmm10, %xmm10
	vmulsd	%xmm5, %xmm0, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vaddsd	(%rdx), %xmm14, %xmm14
	vsubsd	%xmm2, %xmm3, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm14, (%rdx)
	leal	17(%r12), %edx
	vaddsd	%xmm2, %xmm15, %xmm15
	vaddsd	%xmm0, %xmm8, %xmm8
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm10, %xmm10
	vmovsd	%xmm10, (%rdx)
	leal	22(%r12), %edx
	addl	$23, %r12d
	movslq	%r12d, %r12
	cmpq	%r14, %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r12,8), %rax
	vaddsd	(%rdx), %xmm15, %xmm15
	vmovsd	%xmm15, (%rdx)
	vaddsd	(%rax), %xmm8, %xmm8
	vmovsd	%xmm8, (%rax)
	jne	.L328
.L279:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L335
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L331:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB13:
	call	_Unwind_Resume
.LEHE13:
.L335:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2814:
	.section	.gcc_except_table
.LLSDA2814:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2814-.LLSDACSB2814
.LLSDACSB2814:
	.uleb128 .LEHB12-.LFB2814
	.uleb128 .LEHE12-.LEHB12
	.uleb128 .L331-.LFB2814
	.uleb128 0
	.uleb128 .LEHB13-.LFB2814
	.uleb128 .LEHE13-.LEHB13
	.uleb128 0
	.uleb128 0
.LLSDACSE2814:
	.text
	.size	_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_:
.LFB2813:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2813
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	subq	$264, %rsp
	movq	%rsi, -256(%rbp)
	movq	%rdx, -264(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -268(%rbp)
	movq	120(%rdi), %rax
	movl	-268(%rbp), %esi
	movq	(%rax), %rax
	movq	48(%rax), %r10
	movq	56(%rax), %r14
	movq	144(%rdi), %rax
	subq	%r10, %r14
	movq	(%rax), %rax
	sarq	$2, %r14
	imull	%r14d, %esi
	movl	%r14d, %r15d
	movq	48(%rax), %rdx
	movq	56(%rax), %rax
	movl	%esi, -240(%rbp)
	movq	%rax, -296(%rbp)
	movslq	%esi, %rax
	movq	%rdx, -288(%rbp)
	leaq	46(,%rax,8), %rax
	movq	%rax, -216(%rbp)
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	testl	%r14d, %r14d
	jle	.L337
	movq	-264(%rbp), %rax
	movl	52(%rdi), %r11d
	xorl	%esi, %esi
	movl	-268(%rbp), %r15d
	xorl	%ecx, %ecx
	movl	%r14d, -224(%rbp)
	movq	8(%rax), %rdx
	leal	-1(%r14), %eax
	leaq	4(,%rax,4), %r12
	.p2align 4,,10
	.p2align 3
.L338:
	movl	(%r10,%rsi), %eax
	movslq	%ecx, %rdi
	addq	$4, %rsi
	imull	%r11d, %eax
	movslq	%eax, %r9
	leal	13(%rax), %r8d
	vmovsd	(%rdx,%r9,8), %xmm0
	leal	1(%rax), %r9d
	movslq	%r8d, %r8
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	movslq	%r9d, %r9
	leal	12(%rax), %r8d
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	1(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	6(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	19(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	6(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	7(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	18(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	7(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	2(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	15(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	2(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	3(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	14(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	3(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	8(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	8(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	9(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	20(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	9(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	4(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	17(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	4(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	5(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	16(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	5(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	10(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	23(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	10(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	11(%rax), %r8d
	addl	$22, %eax
	cltq
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	11(%rcx), %edi
	addl	%r15d, %ecx
	vmovsd	(%rdx,%r8,8), %xmm0
	cmpq	%r12, %rsi
	movslq	%edi, %rdi
	vsubsd	(%rdx,%rax,8), %xmm0, %xmm0
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	jne	.L338
	movl	-224(%rbp), %r15d
.L337:
	movq	-216(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
.LEHB14:
	call	_ZN12Communicator8instanceEv
	movl	-240(%rbp), %ecx
	movl	$2, %r8d
	movq	%r13, %rdx
	movq	%r12, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_fwEPdS0_ii
	testl	%r15d, %r15d
	jle	.L339
	subl	$1, %r14d
	movq	$0, -240(%rbp)
	leaq	4(,%r14,4), %r14
	movq	%r14, -280(%rbp)
	xorl	%r14d, %r14d
	.p2align 4,,10
	.p2align 3
.L361:
	movq	112(%rbx), %rax
	movq	-240(%rbp), %r15
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	48(%rax), %rax
	movl	(%rax,%r15), %r13d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L341
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L341:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L342
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L342:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L343
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L343:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L344
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L344:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L345
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L345:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L346
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L346:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L348
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L348:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L349
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L349:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L350
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L350:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L351
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L351:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L352
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L352:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L353
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L353:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	movl	60(%rbx), %ecx
	addq	168(%rbx), %rdi
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L355
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L355:
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L356
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L356:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L357
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L357:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L358
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L358:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L359
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L359:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L360
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L360:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	leal	5(%r14), %r11d
	leal	1(%r14), %r10d
	movl	56(%rbx), %esi
	vmovsd	-200(%rbp), %xmm2
	leal	6(%r14), %r9d
	movl	%r11d, -224(%rbp)
	leal	10(%r14), %r11d
	movslq	%r10d, %r10
	vmovsd	-208(%rbp), %xmm1
	leal	7(%r14), %r8d
	movl	%r11d, -232(%rbp)
	leal	11(%r14), %r11d
	movslq	%r9d, %r9
	vmovsd	(%r12,%r10,8), %xmm3
	movslq	%r8d, %r8
	movl	%r11d, -248(%rbp)
	movslq	%r14d, %r11
	leal	(%rax,%rsi,2), %eax
	vmovsd	(%r12,%r11,8), %xmm5
	movl	60(%rbx), %ecx
	vmulsd	%xmm3, %xmm2, %xmm4
	movq	8(%rbx), %rdx
	movq	-256(%rbp), %rsi
	vmulsd	%xmm5, %xmm1, %xmm6
	leal	2(%r14), %edi
	leal	4(%r14), %r15d
	vmulsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm2, %xmm5, %xmm5
	movq	8(%rdx), %rdx
	movslq	%edi, %rdi
	vxorpd	%xmm8, %xmm8, %xmm8
	movslq	%r15d, %r15
	vsubsd	%xmm4, %xmm6, %xmm4
	vmovsd	(%r12,%r9,8), %xmm7
	vmovsd	(%r12,%r8,8), %xmm6
	movq	%r15, -216(%rbp)
	vaddsd	%xmm3, %xmm5, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	imull	68(%rbx), %eax
	vaddsd	%xmm8, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm3
	imull	52(%rbx), %r13d
	vaddsd	%xmm8, %xmm5, %xmm5
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	leal	5(%rax,%rcx,4), %eax
	leal	8(%r14), %ecx
	vsubsd	%xmm3, %xmm8, %xmm3
	vmulsd	%xmm7, %xmm2, %xmm8
	cltq
	vmovsd	-184(%rbp), %xmm2
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	8(%rsi), %rax
	leal	3(%r14), %esi
	vmovsd	(%r12,%rdi,8), %xmm7
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-192(%rbp), %xmm1
	movslq	%esi, %rsi
	vaddsd	%xmm10, %xmm3, %xmm3
	leal	9(%r14), %edx
	vmovsd	(%r12,%rsi,8), %xmm6
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vmovsd	%xmm0, -72(%rbp)
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm4, %xmm10
	vaddsd	%xmm6, %xmm5, %xmm5
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-224(%rbp), %r15
	vsubsd	%xmm4, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-168(%rbp), %xmm2
	vmovsd	(%r12,%r15,8), %xmm6
	movq	%r15, -224(%rbp)
	vaddsd	%xmm9, %xmm3, %xmm9
	movslq	-232(%rbp), %r15
	vmulsd	%xmm6, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-176(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm4
	movq	%r15, -232(%rbp)
	vmulsd	%xmm2, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm1, %xmm6
	vsubsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-248(%rbp), %r15
	vaddsd	%xmm4, %xmm10, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm5, %xmm5
	vmovsd	(%r12,%r15,8), %xmm6
	movq	%r15, -248(%rbp)
	movslq	%r13d, %r15
	leaq	(%rax,%r15,8), %r15
	vmulsd	%xmm6, %xmm2, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm10, %xmm3
	vxorpd	%xmm10, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm4, %xmm2
	vaddsd	%xmm3, %xmm9, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	1(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	6(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	7(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	12(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	13(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm4, %xmm2, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	18(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm1, %xmm2, %xmm1
	vmovsd	-152(%rbp), %xmm2
	vmovsd	%xmm1, (%r15)
	leal	19(%r13), %r15d
	vmovsd	-160(%rbp), %xmm1
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm3
	vmovsd	%xmm3, (%r15)
	movq	-216(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm5
	vmovsd	(%r12,%r10,8), %xmm3
	vmulsd	%xmm5, %xmm1, %xmm6
	vmulsd	%xmm3, %xmm2, %xmm4
	vmulsd	%xmm2, %xmm5, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm3
	vmovsd	(%r12,%r9,8), %xmm7
	vsubsd	%xmm4, %xmm6, %xmm4
	vmovsd	(%r12,%r8,8), %xmm6
	vaddsd	%xmm3, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm3
	vaddsd	%xmm8, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm5, %xmm5
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm6
	vsubsd	%xmm3, %xmm8, %xmm3
	vmulsd	%xmm7, %xmm2, %xmm8
	vmovsd	-136(%rbp), %xmm2
	vmovsd	(%r12,%rdi,8), %xmm7
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-144(%rbp), %xmm1
	vaddsd	%xmm10, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm6
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm4, %xmm10
	vaddsd	%xmm6, %xmm5, %xmm5
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-224(%rbp), %r15
	vsubsd	%xmm4, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	(%r12,%r15,8), %xmm6
	vmovsd	-120(%rbp), %xmm2
	movq	-232(%rbp), %r15
	vaddsd	%xmm9, %xmm3, %xmm9
	vmulsd	%xmm6, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-128(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-248(%rbp), %r15
	vaddsd	%xmm4, %xmm10, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm5, %xmm5
	vmovsd	(%r12,%r15,8), %xmm6
	leal	2(%r13), %r15d
	vmulsd	%xmm6, %xmm2, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	movslq	%r15d, %r15
	vmulsd	%xmm7, %xmm2, %xmm2
	leaq	(%rax,%r15,8), %r15
	vsubsd	%xmm3, %xmm10, %xmm3
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm4, %xmm2
	vaddsd	%xmm3, %xmm9, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	3(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	8(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	9(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	14(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	15(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm4, %xmm2, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	20(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm1, %xmm2, %xmm1
	vmovsd	-104(%rbp), %xmm2
	vmovsd	%xmm1, (%r15)
	leal	21(%r13), %r15d
	vmovsd	-112(%rbp), %xmm1
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm3
	vmovsd	%xmm3, (%r15)
	movq	-248(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm5
	vmovsd	(%r12,%r10,8), %xmm4
	movq	-232(%rbp), %r11
	vmulsd	%xmm5, %xmm1, %xmm10
	vmulsd	%xmm4, %xmm2, %xmm3
	vmulsd	%xmm2, %xmm5, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm4
	vmovsd	(%r12,%r9,8), %xmm6
	vsubsd	%xmm3, %xmm10, %xmm10
	vmulsd	%xmm6, %xmm1, %xmm9
	vaddsd	%xmm4, %xmm5, %xmm4
	vmovsd	(%r12,%r8,8), %xmm5
	vmulsd	%xmm6, %xmm2, %xmm7
	vmulsd	%xmm5, %xmm2, %xmm3
	vaddsd	%xmm8, %xmm10, %xmm10
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovsd	-88(%rbp), %xmm2
	vaddsd	%xmm8, %xmm4, %xmm4
	vmovsd	(%r12,%rsi,8), %xmm5
	movq	-224(%rbp), %rsi
	vsubsd	%xmm3, %xmm9, %xmm9
	vmovsd	(%r12,%rdi,8), %xmm6
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-96(%rbp), %xmm1
	vmulsd	%xmm5, %xmm2, %xmm3
	vmulsd	%xmm5, %xmm1, %xmm5
	vaddsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm1, %xmm8
	vmulsd	%xmm2, %xmm6, %xmm6
	vsubsd	%xmm3, %xmm8, %xmm3
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%rcx,8), %xmm6
	vaddsd	%xmm3, %xmm10, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm10
	vaddsd	%xmm5, %xmm4, %xmm4
	vmovsd	(%r12,%rdx,8), %xmm5
	movq	-216(%rbp), %rdx
	vmulsd	%xmm5, %xmm2, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm2, %xmm2
	vmovsd	(%r12,%rsi,8), %xmm5
	vsubsd	%xmm8, %xmm10, %xmm8
	vmovsd	(%r12,%rdx,8), %xmm6
	leal	4(%r13), %edx
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm5, %xmm0, %xmm2
	movslq	%edx, %rdx
	vaddsd	%xmm8, %xmm9, %xmm8
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm1
	vmulsd	%xmm6, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm9, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%r11,8), %xmm6
	vaddsd	%xmm2, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm4, %xmm4
	vmovsd	(%r12,%r15,8), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm2
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm9, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm2, %xmm8, %xmm2
	vaddsd	%xmm0, %xmm7, %xmm0
	vmovsd	%xmm1, (%rdx)
	leal	5(%r13), %edx
	addq	$4, -240(%rbp)
	addl	-268(%rbp), %r14d
	movq	-280(%rbp), %r15
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	10(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm2, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	11(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm0, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	16(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	17(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm3, %xmm1, %xmm3
	vmovsd	%xmm3, (%rdx)
	leal	22(%r13), %edx
	addl	$23, %r13d
	movslq	%r13d, %r13
	cmpq	%r15, -240(%rbp)
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r13,8), %rax
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%rdx)
	vaddsd	(%rax), %xmm2, %xmm2
	vmovsd	%xmm2, (%rax)
	jne	.L361
.L339:
	movq	-296(%rbp), %rax
	subq	-288(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L336
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %r14
	.p2align 4,,10
	.p2align 3
.L385:
	movq	144(%rbx), %rax
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	48(%rax), %rax
	movl	(%rax,%r13), %r12d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L364
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L364:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L365
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L365:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L366
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L366:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L367
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L367:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L368
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L368:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %ecx
	movq	8(%rbx), %rdx
	leal	(%rax,%rcx,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L369
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L369:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %edx
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	leal	(%rax,%rdx,2), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L371
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L371:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L372
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L372:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L373
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L373:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L374
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L374:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L375
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L375:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L376
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L376:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	movl	60(%rbx), %ecx
	addq	168(%rbx), %rdi
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L378
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L378:
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L379
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L379:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L380
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L380:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L381
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L381:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L382
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L382:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L383
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L383:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	movl	56(%rbx), %esi
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	leal	(%rax,%rsi,2), %eax
	movq	8(%rdx), %rdx
	imull	68(%rbx), %eax
	leal	5(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -72(%rbp)
	movq	192(%rbx), %rax
	testb	$1, %al
	je	.L384
	movq	200(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L384:
	movq	%rbx, %rdi
	addq	200(%rbx), %rdi
	movl	$2, %edx
	movl	%r12d, %esi
	call	*%rax
.LEHE14:
	movl	52(%rbx), %ecx
	movq	-264(%rbp), %rsi
	addq	$4, %r13
	vmovsd	-200(%rbp), %xmm12
	imull	%ecx, %eax
	movq	8(%rsi), %rdx
	imull	%ecx, %r12d
	movslq	%eax, %rdi
	leal	13(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm10
	leal	1(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm10, %xmm10
	movslq	%edi, %rdi
	leal	12(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm9
	leal	6(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm9, %xmm9
	movslq	%edi, %rdi
	leal	19(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm8
	leal	7(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm8, %xmm8
	movslq	%edi, %rdi
	leal	18(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm7
	leal	2(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm10, %xmm15
	vsubsd	(%rdx,%rsi,8), %xmm7, %xmm7
	movslq	%edi, %rdi
	leal	15(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm6
	leal	3(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm9, %xmm13
	vsubsd	(%rdx,%rsi,8), %xmm6, %xmm6
	movslq	%edi, %rdi
	leal	14(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm5
	leal	8(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm5, %xmm5
	movslq	%edi, %rdi
	leal	21(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm4
	leal	9(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm4, %xmm4
	movslq	%edi, %rdi
	leal	20(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm3
	leal	4(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm3, %xmm3
	movslq	%edi, %rdi
	leal	17(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm2
	leal	5(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm2, %xmm2
	movslq	%edi, %rdi
	leal	16(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm1
	leal	10(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm1, %xmm1
	movslq	%edi, %rdi
	leal	23(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm0
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm0, %xmm0
	leal	11(%rax), %esi
	addl	$22, %eax
	cltq
	movslq	%esi, %rsi
	vmovsd	(%rdx,%rsi,8), %xmm11
	movq	-256(%rbp), %rsi
	vsubsd	(%rdx,%rax,8), %xmm11, %xmm11
	movslq	%r12d, %rdx
	movq	8(%rsi), %rax
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	%xmm11, -240(%rbp)
	vmovsd	-208(%rbp), %xmm11
	vmulsd	%xmm11, %xmm10, %xmm14
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm15, -216(%rbp)
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-192(%rbp), %xmm11
	vmovsd	-184(%rbp), %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm6, %xmm15
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmovsd	%xmm13, -248(%rbp)
	vmulsd	%xmm11, %xmm5, %xmm13
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-176(%rbp), %xmm11
	vaddsd	-216(%rbp), %xmm15, %xmm15
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-224(%rbp), %xmm13, %xmm13
	vaddsd	-232(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-168(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm14
	vaddsd	-248(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm11, %xmm1, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm15, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vmovsd	%xmm12, (%rdx)
	leal	1(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	6(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	7(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	12(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	13(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm15, %xmm12, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	18(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	19(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	2(%r12), %edx
	vmovsd	-160(%rbp), %xmm11
	vmovsd	-152(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm10, %xmm14
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm12, %xmm9, %xmm13
	vmulsd	%xmm12, %xmm10, %xmm15
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm15, -216(%rbp)
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-144(%rbp), %xmm11
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-136(%rbp), %xmm12
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm6, %xmm15
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmovsd	%xmm13, -248(%rbp)
	vmulsd	%xmm11, %xmm5, %xmm13
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-128(%rbp), %xmm11
	vaddsd	-216(%rbp), %xmm15, %xmm15
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-224(%rbp), %xmm13, %xmm13
	vaddsd	-232(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-120(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm14
	vaddsd	-248(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm11, %xmm1, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm15, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vmovsd	%xmm12, (%rdx)
	leal	3(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	8(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	9(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	14(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	15(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm15, %xmm12, %xmm12
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	%xmm12, (%rdx)
	leal	20(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	-104(%rbp), %xmm12
	vmulsd	%xmm12, %xmm9, %xmm13
	vmovsd	%xmm11, (%rdx)
	leal	21(%r12), %edx
	vmovsd	-112(%rbp), %xmm11
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm11, %xmm9, %xmm9
	vaddsd	(%rdx), %xmm14, %xmm14
	vmovsd	%xmm14, (%rdx)
	vmulsd	%xmm11, %xmm10, %xmm14
	leal	4(%r12), %edx
	vmulsd	%xmm12, %xmm10, %xmm10
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vsubsd	%xmm13, %xmm14, %xmm13
	vxorpd	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm10, %xmm9
	vmulsd	%xmm12, %xmm7, %xmm10
	vmulsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm9, %xmm9
	vmulsd	%xmm11, %xmm8, %xmm14
	vmulsd	%xmm12, %xmm8, %xmm8
	vmovsd	-96(%rbp), %xmm11
	vsubsd	%xmm10, %xmm14, %xmm14
	vmovsd	-88(%rbp), %xmm10
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	%xmm11, %xmm6, %xmm12
	vmulsd	%xmm10, %xmm5, %xmm8
	vmulsd	%xmm10, %xmm6, %xmm6
	vmulsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm15, %xmm14, %xmm14
	vaddsd	%xmm15, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm15
	vsubsd	%xmm8, %xmm12, %xmm8
	vaddsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm11, %xmm4, %xmm6
	vmulsd	%xmm10, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm13, %xmm13
	vaddsd	%xmm5, %xmm9, %xmm9
	vmulsd	%xmm10, %xmm3, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm5, %xmm14, %xmm14
	vmulsd	%xmm15, %xmm2, %xmm5
	vaddsd	%xmm3, %xmm7, %xmm7
	vmovsd	-72(%rbp), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm15, %xmm0, %xmm2
	vmulsd	-240(%rbp), %xmm15, %xmm15
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm1, %xmm9, %xmm9
	vmulsd	-240(%rbp), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vaddsd	(%rdx), %xmm13, %xmm0
	vaddsd	%xmm1, %xmm14, %xmm14
	vmovsd	%xmm0, (%rdx)
	leal	5(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm9, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm9, %xmm9
	vmovsd	%xmm9, (%rdx)
	leal	17(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm13, %xmm0, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	22(%r12), %edx
	addl	$23, %r12d
	movslq	%r12d, %r12
	cmpq	%r14, %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r12,8), %rax
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm7, %xmm0, %xmm7
	vmovsd	%xmm7, (%rdx)
	vaddsd	(%rax), %xmm14, %xmm14
	vmovsd	%xmm14, (%rax)
	jne	.L385
.L336:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L392
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L388:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB15:
	call	_Unwind_Resume
.LEHE15:
.L392:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2813:
	.section	.gcc_except_table
.LLSDA2813:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2813-.LLSDACSB2813
.LLSDACSB2813:
	.uleb128 .LEHB14-.LFB2813
	.uleb128 .LEHE14-.LEHB14
	.uleb128 .L388-.LFB2813
	.uleb128 0
	.uleb128 .LEHB15-.LFB2813
	.uleb128 .LEHE15-.LEHB15
	.uleb128 0
	.uleb128 0
.LLSDACSE2813:
	.text
	.size	_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_:
.LFB2812:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2812
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	subq	$264, %rsp
	movq	%rsi, -256(%rbp)
	movq	%rdx, -264(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -268(%rbp)
	movq	120(%rdi), %rax
	movl	-268(%rbp), %esi
	movq	(%rax), %rax
	movq	24(%rax), %r10
	movq	32(%rax), %r14
	movq	144(%rdi), %rax
	subq	%r10, %r14
	movq	(%rax), %rax
	sarq	$2, %r14
	imull	%r14d, %esi
	movl	%r14d, %r15d
	movq	24(%rax), %rdx
	movq	32(%rax), %rax
	movl	%esi, -240(%rbp)
	movq	%rax, -296(%rbp)
	movslq	%esi, %rax
	movq	%rdx, -288(%rbp)
	leaq	46(,%rax,8), %rax
	movq	%rax, -216(%rbp)
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	testl	%r14d, %r14d
	jle	.L394
	movq	-264(%rbp), %rax
	movl	52(%rdi), %r11d
	xorl	%esi, %esi
	movl	-268(%rbp), %r15d
	xorl	%ecx, %ecx
	movl	%r14d, -224(%rbp)
	movq	8(%rax), %rdx
	leal	-1(%r14), %eax
	leaq	4(,%rax,4), %r12
	.p2align 4,,10
	.p2align 3
.L395:
	movl	(%r10,%rsi), %eax
	movslq	%ecx, %rdi
	addq	$4, %rsi
	imull	%r11d, %eax
	movslq	%eax, %r9
	leal	18(%rax), %r8d
	vmovsd	(%rdx,%r9,8), %xmm0
	leal	1(%rax), %r9d
	movslq	%r8d, %r8
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	movslq	%r9d, %r9
	leal	19(%rax), %r8d
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	1(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	6(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	12(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	6(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	7(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	13(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	7(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	2(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	20(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	2(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	3(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	3(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	8(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	14(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	8(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	9(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	15(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	9(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	4(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	22(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	4(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	5(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	23(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	5(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	10(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	16(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	10(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	11(%rax), %r8d
	addl	$17, %eax
	cltq
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	11(%rcx), %edi
	addl	%r15d, %ecx
	vmovsd	(%rdx,%r8,8), %xmm0
	cmpq	%r12, %rsi
	movslq	%edi, %rdi
	vsubsd	(%rdx,%rax,8), %xmm0, %xmm0
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	jne	.L395
	movl	-224(%rbp), %r15d
.L394:
	movq	-216(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
.LEHB16:
	call	_ZN12Communicator8instanceEv
	movl	-240(%rbp), %ecx
	movl	$1, %r8d
	movq	%r13, %rdx
	movq	%r12, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_fwEPdS0_ii
	testl	%r15d, %r15d
	jle	.L396
	subl	$1, %r14d
	movq	$0, -240(%rbp)
	leaq	4(,%r14,4), %r14
	movq	%r14, -280(%rbp)
	xorl	%r14d, %r14d
	.p2align 4,,10
	.p2align 3
.L418:
	movq	112(%rbx), %rax
	movq	-240(%rbp), %r15
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	24(%rax), %rax
	movl	(%rax,%r15), %r13d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L398
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L398:
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L399
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L399:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L400
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L400:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L401
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L401:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L402
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L402:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L403
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L403:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	addq	168(%rbx), %rdi
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L405
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L405:
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L406
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L406:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L407
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L407:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L408
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L408:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L409
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L409:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L410
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L410:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	addq	168(%rbx), %rdi
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L412
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L412:
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L413
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L413:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L414
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L414:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L415
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L415:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L416
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L416:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L417
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L417:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	leal	5(%r14), %r11d
	leal	1(%r14), %r10d
	leal	6(%r14), %r9d
	vmovsd	-200(%rbp), %xmm2
	leal	7(%r14), %r8d
	movl	%r11d, -224(%rbp)
	leal	10(%r14), %r11d
	movslq	%r10d, %r10
	vmovsd	-208(%rbp), %xmm1
	addl	56(%rbx), %eax
	movl	%r11d, -232(%rbp)
	leal	11(%r14), %r11d
	movslq	%r9d, %r9
	vmovsd	(%r12,%r10,8), %xmm3
	movslq	%r8d, %r8
	movl	%r11d, -248(%rbp)
	movslq	%r14d, %r11
	movl	60(%rbx), %ecx
	vmovsd	(%r12,%r11,8), %xmm5
	movq	8(%rbx), %rdx
	vmulsd	%xmm3, %xmm2, %xmm4
	movq	-256(%rbp), %rsi
	leal	2(%r14), %edi
	vmulsd	%xmm5, %xmm1, %xmm6
	leal	4(%r14), %r15d
	vmulsd	%xmm2, %xmm5, %xmm5
	movq	8(%rdx), %rdx
	movslq	%edi, %rdi
	vmulsd	%xmm3, %xmm1, %xmm3
	movslq	%r15d, %r15
	vxorpd	%xmm8, %xmm8, %xmm8
	movq	%r15, -216(%rbp)
	vsubsd	%xmm4, %xmm6, %xmm4
	vmovsd	(%r12,%r9,8), %xmm7
	vmovsd	(%r12,%r8,8), %xmm6
	vaddsd	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm10, %xmm10, %xmm10
	imull	68(%rbx), %eax
	vaddsd	%xmm8, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm5
	imull	52(%rbx), %r13d
	vaddsd	%xmm8, %xmm3, %xmm3
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	leal	5(%rax,%rcx,4), %eax
	leal	8(%r14), %ecx
	vsubsd	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm7, %xmm2, %xmm8
	cltq
	vmovsd	-184(%rbp), %xmm2
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	8(%rsi), %rax
	leal	3(%r14), %esi
	vmovsd	(%r12,%rdi,8), %xmm7
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-192(%rbp), %xmm1
	movslq	%esi, %rsi
	vaddsd	%xmm10, %xmm5, %xmm5
	leal	9(%r14), %edx
	vmovsd	(%r12,%rsi,8), %xmm6
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vmovsd	%xmm0, -72(%rbp)
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm4, %xmm10
	vaddsd	%xmm6, %xmm3, %xmm3
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-224(%rbp), %r15
	vsubsd	%xmm4, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-168(%rbp), %xmm2
	vmovsd	(%r12,%r15,8), %xmm6
	movq	%r15, -224(%rbp)
	vaddsd	%xmm9, %xmm5, %xmm9
	movslq	-232(%rbp), %r15
	vmulsd	%xmm6, %xmm2, %xmm4
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-176(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm5
	movq	%r15, -232(%rbp)
	vmulsd	%xmm2, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm1, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-248(%rbp), %r15
	vaddsd	%xmm4, %xmm10, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm3, %xmm3
	vmovsd	(%r12,%r15,8), %xmm6
	movq	%r15, -248(%rbp)
	movslq	%r13d, %r15
	leaq	(%rax,%r15,8), %r15
	vmulsd	%xmm6, %xmm2, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm10, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm4, %xmm2
	vaddsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	1(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	6(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	7(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	12(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm5, %xmm2, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	13(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm1, %xmm2, %xmm1
	vmovsd	-152(%rbp), %xmm2
	vmovsd	%xmm1, (%r15)
	leal	18(%r13), %r15d
	vmovsd	-160(%rbp), %xmm1
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	19(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm3
	vmovsd	%xmm3, (%r15)
	movq	-216(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm5
	vmovsd	(%r12,%r10,8), %xmm3
	vmulsd	%xmm5, %xmm1, %xmm6
	vmulsd	%xmm3, %xmm2, %xmm4
	vmulsd	%xmm2, %xmm5, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm3
	vmovsd	(%r12,%r9,8), %xmm7
	vsubsd	%xmm4, %xmm6, %xmm4
	vmovsd	(%r12,%r8,8), %xmm6
	vaddsd	%xmm3, %xmm5, %xmm3
	vmulsd	%xmm6, %xmm2, %xmm5
	vaddsd	%xmm8, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm3, %xmm3
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm6
	vsubsd	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm7, %xmm2, %xmm8
	vmovsd	-136(%rbp), %xmm2
	vmovsd	(%r12,%rdi,8), %xmm7
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-144(%rbp), %xmm1
	vaddsd	%xmm10, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm6
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm4, %xmm10
	vaddsd	%xmm6, %xmm3, %xmm3
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-224(%rbp), %r15
	vsubsd	%xmm4, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	(%r12,%r15,8), %xmm6
	vmovsd	-120(%rbp), %xmm2
	movq	-232(%rbp), %r15
	vaddsd	%xmm9, %xmm5, %xmm9
	vmulsd	%xmm6, %xmm2, %xmm4
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-128(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-248(%rbp), %r15
	vaddsd	%xmm4, %xmm10, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm3, %xmm3
	vmovsd	(%r12,%r15,8), %xmm6
	leal	2(%r13), %r15d
	vmulsd	%xmm6, %xmm2, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm1
	movslq	%r15d, %r15
	vmulsd	%xmm7, %xmm2, %xmm2
	leaq	(%rax,%r15,8), %r15
	vsubsd	%xmm5, %xmm10, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm4, %xmm2
	vaddsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	3(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	8(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	9(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	14(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm5, %xmm2, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	15(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm2
	vsubsd	%xmm1, %xmm2, %xmm1
	vmovsd	-104(%rbp), %xmm2
	vmovsd	%xmm1, (%r15)
	leal	20(%r13), %r15d
	vmovsd	-112(%rbp), %xmm1
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	21(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm3, %xmm3
	vmovsd	%xmm3, (%r15)
	movq	-248(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm5
	vmovsd	(%r12,%r10,8), %xmm4
	movq	-232(%rbp), %r11
	vmulsd	%xmm5, %xmm1, %xmm10
	vmulsd	%xmm4, %xmm2, %xmm3
	vmulsd	%xmm2, %xmm5, %xmm9
	vmulsd	%xmm4, %xmm1, %xmm4
	vmovsd	(%r12,%r9,8), %xmm6
	vmovsd	(%r12,%r8,8), %xmm5
	vsubsd	%xmm3, %xmm10, %xmm10
	vmulsd	%xmm6, %xmm2, %xmm7
	vmulsd	%xmm5, %xmm2, %xmm3
	vaddsd	%xmm4, %xmm9, %xmm9
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm10, %xmm10
	vmovsd	-88(%rbp), %xmm2
	vmovsd	(%r12,%rsi,8), %xmm5
	vaddsd	%xmm8, %xmm9, %xmm9
	movq	-224(%rbp), %rsi
	vsubsd	%xmm3, %xmm4, %xmm4
	vmovsd	(%r12,%rdi,8), %xmm6
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-96(%rbp), %xmm1
	vmulsd	%xmm5, %xmm2, %xmm3
	vmulsd	%xmm5, %xmm1, %xmm5
	vaddsd	%xmm8, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm1, %xmm8
	vmulsd	%xmm2, %xmm6, %xmm6
	vsubsd	%xmm3, %xmm8, %xmm3
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%rcx,8), %xmm6
	vaddsd	%xmm3, %xmm10, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm10
	vaddsd	%xmm5, %xmm9, %xmm9
	vmovsd	(%r12,%rdx,8), %xmm5
	movq	-216(%rbp), %rdx
	vmulsd	%xmm5, %xmm2, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm2, %xmm2
	vmovsd	(%r12,%rsi,8), %xmm5
	vsubsd	%xmm8, %xmm10, %xmm8
	vmovsd	(%r12,%rdx,8), %xmm6
	leal	4(%r13), %edx
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm5, %xmm0, %xmm2
	movslq	%edx, %rdx
	vaddsd	%xmm8, %xmm4, %xmm8
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm1
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm4, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%r11,8), %xmm6
	vaddsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm9, %xmm2
	vmovsd	(%r12,%r15,8), %xmm5
	vmulsd	%xmm6, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm9, %xmm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	(%rdx), %xmm3, %xmm1
	vaddsd	%xmm4, %xmm8, %xmm4
	vaddsd	%xmm0, %xmm7, %xmm0
	vmovsd	%xmm1, (%rdx)
	leal	5(%r13), %edx
	addq	$4, -240(%rbp)
	addl	-268(%rbp), %r14d
	movq	-280(%rbp), %r15
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm2, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	10(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	11(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm0, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	16(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm4, %xmm1, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	17(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm1
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	22(%r13), %edx
	addl	$23, %r13d
	movslq	%r13d, %r13
	cmpq	%r15, -240(%rbp)
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r13,8), %rax
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	vaddsd	(%rax), %xmm2, %xmm2
	vmovsd	%xmm2, (%rax)
	jne	.L418
.L396:
	movq	-296(%rbp), %rax
	subq	-288(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L393
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %r14
	.p2align 4,,10
	.p2align 3
.L442:
	movq	144(%rbx), %rax
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	24(%rax), %rax
	movl	(%rax,%r13), %r12d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L421
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L421:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L422
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L422:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L423
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L423:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L424
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L424:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L425
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L425:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L426
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L426:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	addq	168(%rbx), %rdi
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L428
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L428:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L429
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L429:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L430
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L430:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L431
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L431:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L432
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L432:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L433
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L433:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	%rbx, %rdi
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	addq	168(%rbx), %rdi
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L435
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L435:
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L436
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L436:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L437
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L437:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L438
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L438:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L439
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L439:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L440
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L440:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	addl	56(%rbx), %eax
	movq	8(%rbx), %rdx
	imull	68(%rbx), %eax
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -72(%rbp)
	movq	192(%rbx), %rax
	testb	$1, %al
	je	.L441
	movq	200(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L441:
	movq	%rbx, %rdi
	addq	200(%rbx), %rdi
	movl	$1, %edx
	movl	%r12d, %esi
	call	*%rax
.LEHE16:
	movl	52(%rbx), %ecx
	movq	-264(%rbp), %rsi
	addq	$4, %r13
	vmovsd	-200(%rbp), %xmm12
	imull	%ecx, %eax
	movq	8(%rsi), %rdx
	imull	%ecx, %r12d
	movslq	%eax, %rdi
	leal	18(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm10
	leal	1(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm10, %xmm10
	movslq	%edi, %rdi
	leal	19(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm9
	leal	6(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm9, %xmm9
	movslq	%edi, %rdi
	leal	12(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm8
	leal	7(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm8, %xmm8
	movslq	%edi, %rdi
	leal	13(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm7
	leal	2(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm10, %xmm15
	vsubsd	(%rdx,%rsi,8), %xmm7, %xmm7
	movslq	%edi, %rdi
	leal	20(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm6
	leal	3(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm9, %xmm13
	vaddsd	(%rdx,%rsi,8), %xmm6, %xmm6
	movslq	%edi, %rdi
	leal	21(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm5
	leal	8(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm5, %xmm5
	movslq	%edi, %rdi
	leal	14(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm4
	leal	9(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm4, %xmm4
	movslq	%edi, %rdi
	leal	15(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm3
	leal	4(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm3, %xmm3
	movslq	%edi, %rdi
	leal	22(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm2
	leal	5(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm2, %xmm2
	movslq	%edi, %rdi
	leal	23(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm1
	leal	10(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm1, %xmm1
	movslq	%edi, %rdi
	leal	16(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm0
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm0, %xmm0
	leal	11(%rax), %esi
	addl	$17, %eax
	cltq
	movslq	%esi, %rsi
	vmovsd	(%rdx,%rsi,8), %xmm11
	movq	-256(%rbp), %rsi
	vsubsd	(%rdx,%rax,8), %xmm11, %xmm11
	movslq	%r12d, %rdx
	movq	8(%rsi), %rax
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	%xmm11, -240(%rbp)
	vmovsd	-208(%rbp), %xmm11
	vmulsd	%xmm11, %xmm10, %xmm14
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-192(%rbp), %xmm11
	vmovsd	-184(%rbp), %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm15
	vmulsd	%xmm12, %xmm6, %xmm14
	vmulsd	%xmm11, %xmm5, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	-216(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-176(%rbp), %xmm11
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-224(%rbp), %xmm13, %xmm13
	vaddsd	-232(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-168(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm1, %xmm14
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm13, %xmm12
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vmovsd	%xmm12, (%rdx)
	leal	1(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm15, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	6(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	7(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	12(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm14, %xmm12, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	13(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	18(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	19(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm15, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	2(%r12), %edx
	vmovsd	-160(%rbp), %xmm11
	vmovsd	-152(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm10, %xmm14
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm12, %xmm9, %xmm13
	vmulsd	%xmm12, %xmm10, %xmm15
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-144(%rbp), %xmm11
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-136(%rbp), %xmm12
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm15
	vmulsd	%xmm12, %xmm6, %xmm14
	vmulsd	%xmm11, %xmm5, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	-216(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-128(%rbp), %xmm11
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-224(%rbp), %xmm13, %xmm13
	vaddsd	-232(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-120(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm1, %xmm14
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm15
	vaddsd	%xmm14, %xmm15, %xmm14
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vaddsd	-216(%rbp), %xmm14, %xmm14
	vmovsd	%xmm14, -216(%rbp)
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm13, %xmm12
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vmovsd	%xmm12, (%rdx)
	leal	3(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm15, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	8(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	9(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	14(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm14, %xmm12, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	15(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm12
	vsubsd	%xmm11, %xmm12, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	20(%r12), %edx
	vmovsd	-104(%rbp), %xmm12
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	21(%r12), %edx
	vmulsd	%xmm12, %xmm9, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm15, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	%xmm11, (%rdx)
	leal	4(%r12), %edx
	vmovsd	-112(%rbp), %xmm11
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm10, %xmm14
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm12, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm9, %xmm9
	vsubsd	%xmm13, %xmm14, %xmm13
	vxorpd	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm10, %xmm9
	vmulsd	%xmm12, %xmm7, %xmm10
	vmulsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm9, %xmm9
	vmulsd	%xmm11, %xmm8, %xmm14
	vmulsd	%xmm12, %xmm8, %xmm8
	vmovsd	-96(%rbp), %xmm11
	vsubsd	%xmm10, %xmm14, %xmm14
	vmovsd	-88(%rbp), %xmm10
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	%xmm11, %xmm6, %xmm12
	vmulsd	%xmm10, %xmm5, %xmm8
	vmulsd	%xmm10, %xmm6, %xmm6
	vmulsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm15, %xmm14, %xmm14
	vaddsd	%xmm15, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm15
	vsubsd	%xmm8, %xmm12, %xmm8
	vaddsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm11, %xmm4, %xmm6
	vmulsd	%xmm10, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm13, %xmm13
	vaddsd	%xmm5, %xmm9, %xmm9
	vmulsd	%xmm10, %xmm3, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm5, %xmm14, %xmm14
	vmulsd	%xmm15, %xmm2, %xmm5
	vaddsd	%xmm3, %xmm7, %xmm7
	vmovsd	-72(%rbp), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm15, %xmm0, %xmm2
	vmulsd	-240(%rbp), %xmm15, %xmm15
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm1, %xmm9, %xmm9
	vmulsd	-240(%rbp), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vaddsd	(%rdx), %xmm13, %xmm0
	vaddsd	%xmm1, %xmm14, %xmm14
	vmovsd	%xmm0, (%rdx)
	leal	5(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm9, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm14, %xmm0, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	17(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm7, %xmm0, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	22(%r12), %edx
	addl	$23, %r12d
	movslq	%r12d, %r12
	cmpq	%r14, %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r12,8), %rax
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	vaddsd	(%rax), %xmm9, %xmm9
	vmovsd	%xmm9, (%rax)
	jne	.L442
.L393:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L449
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L445:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB17:
	call	_Unwind_Resume
.LEHE17:
.L449:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2812:
	.section	.gcc_except_table
.LLSDA2812:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2812-.LLSDACSB2812
.LLSDACSB2812:
	.uleb128 .LEHB16-.LFB2812
	.uleb128 .LEHE16-.LEHB16
	.uleb128 .L445-.LFB2812
	.uleb128 0
	.uleb128 .LEHB17-.LFB2812
	.uleb128 .LEHE17-.LEHB17
	.uleb128 0
	.uleb128 0
.LLSDACSE2812:
	.text
	.size	_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_, @function
_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_:
.LFB2811:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2811
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	subq	$264, %rsp
	movq	%rsi, -256(%rbp)
	movq	%rdx, -264(%rbp)
	movq	%fs:40, %rax
	movq	%rax, -56(%rbp)
	xorl	%eax, %eax
	movl	32(%rdi), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -268(%rbp)
	movq	120(%rdi), %rax
	movl	-268(%rbp), %esi
	movq	(%rax), %rax
	movq	(%rax), %r10
	movq	8(%rax), %r14
	movq	144(%rdi), %rax
	subq	%r10, %r14
	movq	(%rax), %rax
	sarq	$2, %r14
	imull	%r14d, %esi
	movl	%r14d, %r15d
	movq	(%rax), %rdx
	movq	8(%rax), %rax
	movl	%esi, -240(%rbp)
	movq	%rax, -296(%rbp)
	movslq	%esi, %rax
	movq	%rdx, -288(%rbp)
	leaq	46(,%rax,8), %rax
	movq	%rax, -216(%rbp)
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r13
	andq	$-32, %r13
	testl	%r14d, %r14d
	jle	.L451
	movq	-264(%rbp), %rax
	movl	52(%rdi), %r11d
	xorl	%esi, %esi
	movl	-268(%rbp), %r15d
	xorl	%ecx, %ecx
	movl	%r14d, -224(%rbp)
	movq	8(%rax), %rdx
	leal	-1(%r14), %eax
	leaq	4(,%rax,4), %r12
	.p2align 4,,10
	.p2align 3
.L452:
	movl	(%r10,%rsi), %eax
	movslq	%ecx, %rdi
	addq	$4, %rsi
	imull	%r11d, %eax
	movslq	%eax, %r9
	leal	19(%rax), %r8d
	vmovsd	(%rdx,%r9,8), %xmm0
	leal	1(%rax), %r9d
	movslq	%r8d, %r8
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	movslq	%r9d, %r9
	leal	18(%rax), %r8d
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	1(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	6(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	13(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	6(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	7(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	12(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	7(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	2(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	21(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	2(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	3(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	20(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	3(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	8(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	15(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	8(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	9(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	14(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	9(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	4(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	23(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	4(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	5(%rax), %r9d
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	22(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	5(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	leal	10(%rax), %r9d
	vaddsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	17(%rax), %r8d
	movslq	%r9d, %r9
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	10(%rcx), %edi
	vmovsd	(%rdx,%r9,8), %xmm0
	movslq	%edi, %rdi
	vsubsd	(%rdx,%r8,8), %xmm0, %xmm0
	leal	11(%rax), %r8d
	addl	$16, %eax
	cltq
	movslq	%r8d, %r8
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	leal	11(%rcx), %edi
	addl	%r15d, %ecx
	vmovsd	(%rdx,%r8,8), %xmm0
	cmpq	%r12, %rsi
	movslq	%edi, %rdi
	vaddsd	(%rdx,%rax,8), %xmm0, %xmm0
	vmovsd	%xmm0, 0(%r13,%rdi,8)
	jne	.L452
	movl	-224(%rbp), %r15d
.L451:
	movq	-216(%rbp), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	leaq	31(%rsp), %r12
	andq	$-32, %r12
.LEHB18:
	call	_ZN12Communicator8instanceEv
	movl	-240(%rbp), %ecx
	xorl	%r8d, %r8d
	movq	%r13, %rdx
	movq	%r12, %rsi
	movq	%rax, %rdi
	call	_ZN12Communicator11transfer_fwEPdS0_ii
	testl	%r15d, %r15d
	jle	.L453
	subl	$1, %r14d
	movq	$0, -240(%rbp)
	leaq	4(,%r14,4), %r14
	movq	%r14, -280(%rbp)
	xorl	%r14d, %r14d
	.p2align 4,,10
	.p2align 3
.L473:
	movq	112(%rbx), %rax
	movq	-240(%rbp), %r15
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	(%rax), %rax
	movl	(%rax,%r15), %r13d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L455
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L455:
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L456
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L456:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L457
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L457:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L458
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L458:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L459
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L459:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L460
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L460:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L461
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L461:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L462
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L462:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L463
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L463:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L464
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L464:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L465
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L465:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L466
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L466:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L467
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L467:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L468
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L468:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L469
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L469:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L470
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L470:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L471
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L471:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L472
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L472:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r13d, %esi
	call	*%rax
	leal	5(%r14), %r11d
	leal	1(%r14), %r10d
	leal	6(%r14), %r9d
	vmovsd	-200(%rbp), %xmm2
	leal	7(%r14), %r8d
	movl	%r11d, -224(%rbp)
	leal	10(%r14), %r11d
	movslq	%r10d, %r10
	vmovsd	-208(%rbp), %xmm1
	movslq	%r9d, %r9
	movl	%r11d, -232(%rbp)
	leal	11(%r14), %r11d
	movslq	%r8d, %r8
	vmovsd	(%r12,%r10,8), %xmm4
	movl	60(%rbx), %ecx
	movl	%r11d, -248(%rbp)
	movslq	%r14d, %r11
	movq	8(%rbx), %rdx
	vmovsd	(%r12,%r11,8), %xmm5
	movq	-256(%rbp), %rsi
	vmulsd	%xmm4, %xmm2, %xmm3
	leal	2(%r14), %edi
	leal	4(%r14), %r15d
	vmulsd	%xmm5, %xmm1, %xmm6
	movq	8(%rdx), %rdx
	vmulsd	%xmm2, %xmm5, %xmm5
	movslq	%edi, %rdi
	movslq	%r15d, %r15
	vmulsd	%xmm4, %xmm1, %xmm4
	movq	%r15, -216(%rbp)
	vxorpd	%xmm8, %xmm8, %xmm8
	vsubsd	%xmm3, %xmm6, %xmm3
	vmovsd	(%r12,%r9,8), %xmm7
	vmovsd	(%r12,%r8,8), %xmm6
	vaddsd	%xmm4, %xmm5, %xmm4
	vxorpd	%xmm10, %xmm10, %xmm10
	imull	68(%rbx), %eax
	vaddsd	%xmm8, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm2, %xmm5
	imull	52(%rbx), %r13d
	vaddsd	%xmm8, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	leal	5(%rax,%rcx,4), %eax
	leal	8(%r14), %ecx
	vsubsd	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm7, %xmm2, %xmm8
	cltq
	vmovsd	-184(%rbp), %xmm2
	vmovsd	(%rdx,%rax,8), %xmm0
	movq	8(%rsi), %rax
	leal	3(%r14), %esi
	vmovsd	(%r12,%rdi,8), %xmm7
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-192(%rbp), %xmm1
	movslq	%esi, %rsi
	vaddsd	%xmm10, %xmm5, %xmm5
	leal	9(%r14), %edx
	vmovsd	(%r12,%rsi,8), %xmm6
	movslq	%ecx, %rcx
	movslq	%edx, %rdx
	vmovsd	%xmm0, -72(%rbp)
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm3, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-224(%rbp), %r15
	vsubsd	%xmm3, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	-168(%rbp), %xmm2
	vmovsd	(%r12,%r15,8), %xmm6
	movq	%r15, -224(%rbp)
	vaddsd	%xmm9, %xmm5, %xmm9
	movslq	-232(%rbp), %r15
	vmulsd	%xmm6, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-176(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm5
	movq	%r15, -232(%rbp)
	vmulsd	%xmm2, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm1, %xmm6
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movslq	-248(%rbp), %r15
	vaddsd	%xmm3, %xmm10, %xmm3
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	movq	%r15, -248(%rbp)
	vmovsd	(%r12,%r15,8), %xmm6
	movslq	%r13d, %r15
	vmulsd	%xmm6, %xmm2, %xmm5
	leaq	(%rax,%r15,8), %r15
	vmulsd	%xmm7, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm10, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm3, %xmm2
	vaddsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	1(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	6(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	7(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	12(%r13), %r15d
	vmovsd	-152(%rbp), %xmm2
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm1
	vmovsd	%xmm1, (%r15)
	leal	13(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm1
	vsubsd	%xmm5, %xmm1, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	18(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	19(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm1
	vsubsd	%xmm3, %xmm1, %xmm3
	vmovsd	-160(%rbp), %xmm1
	vmovsd	%xmm3, (%r15)
	movq	-216(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm5
	vmovsd	(%r12,%r10,8), %xmm4
	vmulsd	%xmm5, %xmm1, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm3
	vmulsd	%xmm2, %xmm5, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm4
	vmovsd	(%r12,%r9,8), %xmm7
	vsubsd	%xmm3, %xmm6, %xmm3
	vmovsd	(%r12,%r8,8), %xmm6
	vaddsd	%xmm4, %xmm5, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm5
	vaddsd	%xmm8, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm6, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm6
	vsubsd	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm7, %xmm2, %xmm8
	vmovsd	-136(%rbp), %xmm2
	vmovsd	(%r12,%rdi,8), %xmm7
	vmulsd	%xmm6, %xmm2, %xmm9
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-144(%rbp), %xmm1
	vaddsd	%xmm10, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm6
	vaddsd	%xmm10, %xmm8, %xmm8
	vmulsd	%xmm7, %xmm1, %xmm10
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%rcx,8), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm10, %xmm3, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vmovsd	(%r12,%rdx,8), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-224(%rbp), %r15
	vsubsd	%xmm3, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	(%r12,%r15,8), %xmm6
	vmovsd	-120(%rbp), %xmm2
	movq	-232(%rbp), %r15
	vaddsd	%xmm9, %xmm5, %xmm9
	vmulsd	%xmm6, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm8, %xmm8
	vmovsd	-128(%rbp), %xmm1
	vmulsd	%xmm7, %xmm1, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm2, %xmm7, %xmm7
	vsubsd	%xmm3, %xmm5, %xmm3
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovsd	(%r12,%r15,8), %xmm7
	movq	-248(%rbp), %r15
	vaddsd	%xmm3, %xmm10, %xmm3
	vmulsd	%xmm7, %xmm1, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vmovsd	(%r12,%r15,8), %xmm6
	leal	2(%r13), %r15d
	vmulsd	%xmm6, %xmm2, %xmm5
	vmulsd	%xmm6, %xmm1, %xmm1
	movslq	%r15d, %r15
	vmulsd	%xmm7, %xmm2, %xmm2
	leaq	(%rax,%r15,8), %r15
	vsubsd	%xmm5, %xmm10, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	(%r15), %xmm3, %xmm2
	vaddsd	%xmm5, %xmm9, %xmm5
	vaddsd	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	%xmm2, (%r15)
	leal	3(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	8(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm5, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	9(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm2
	vmovsd	%xmm2, (%r15)
	leal	14(%r13), %r15d
	vmovsd	-104(%rbp), %xmm2
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm1, %xmm1
	vmovsd	%xmm1, (%r15)
	leal	15(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm1
	vsubsd	%xmm5, %xmm1, %xmm5
	vmovsd	%xmm5, (%r15)
	leal	20(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vaddsd	(%r15), %xmm4, %xmm4
	vmovsd	%xmm4, (%r15)
	leal	21(%r13), %r15d
	movslq	%r15d, %r15
	leaq	(%rax,%r15,8), %r15
	vmovsd	(%r15), %xmm1
	vsubsd	%xmm3, %xmm1, %xmm3
	vmovsd	-112(%rbp), %xmm1
	vmovsd	%xmm3, (%r15)
	movq	-248(%rbp), %r15
	vmovsd	(%r12,%r11,8), %xmm4
	vmovsd	(%r12,%r10,8), %xmm3
	movq	-232(%rbp), %r11
	vmulsd	%xmm4, %xmm1, %xmm10
	vmulsd	%xmm3, %xmm2, %xmm5
	vmulsd	%xmm2, %xmm4, %xmm4
	vmulsd	%xmm3, %xmm1, %xmm3
	vmovsd	(%r12,%r9,8), %xmm6
	vsubsd	%xmm5, %xmm10, %xmm10
	vmovsd	(%r12,%r8,8), %xmm5
	vmulsd	%xmm6, %xmm1, %xmm7
	vaddsd	%xmm3, %xmm4, %xmm3
	vmulsd	%xmm5, %xmm2, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovsd	(%r12,%rsi,8), %xmm5
	vaddsd	%xmm8, %xmm10, %xmm10
	movq	-224(%rbp), %rsi
	vsubsd	%xmm4, %xmm7, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm7
	vmovsd	-88(%rbp), %xmm2
	vaddsd	%xmm8, %xmm3, %xmm3
	vmovsd	(%r12,%rdi,8), %xmm6
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-96(%rbp), %xmm1
	vaddsd	%xmm8, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm1, %xmm9
	vmulsd	%xmm2, %xmm6, %xmm6
	vaddsd	%xmm8, %xmm7, %xmm7
	vmulsd	%xmm5, %xmm2, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%rcx,8), %xmm6
	vaddsd	%xmm9, %xmm10, %xmm9
	vmulsd	%xmm6, %xmm1, %xmm10
	vaddsd	%xmm5, %xmm3, %xmm3
	vmovsd	(%r12,%rdx,8), %xmm5
	movq	-216(%rbp), %rdx
	vmulsd	%xmm5, %xmm2, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm2, %xmm2
	vmovsd	(%r12,%rsi,8), %xmm5
	vsubsd	%xmm8, %xmm10, %xmm8
	vmovsd	(%r12,%rdx,8), %xmm6
	leal	4(%r13), %edx
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm5, %xmm0, %xmm2
	movslq	%edx, %rdx
	vaddsd	%xmm8, %xmm4, %xmm8
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	%xmm1, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm1
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm4, %xmm2
	vaddsd	%xmm5, %xmm6, %xmm5
	vmovsd	(%r12,%r11,8), %xmm6
	vaddsd	%xmm2, %xmm9, %xmm2
	vmulsd	%xmm6, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm3, %xmm3
	vmovsd	(%r12,%r15,8), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm9, %xmm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	(%rdx), %xmm2, %xmm1
	vaddsd	%xmm4, %xmm8, %xmm4
	vaddsd	%xmm0, %xmm7, %xmm0
	vmovsd	%xmm1, (%rdx)
	leal	5(%r13), %edx
	addq	$4, -240(%rbp)
	addl	-268(%rbp), %r14d
	movq	-280(%rbp), %r15
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm3, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	10(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm4, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	11(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm0, %xmm1
	vmovsd	%xmm1, (%rdx)
	leal	16(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm0, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	17(%r13), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm4, %xmm0, %xmm4
	vmovsd	%xmm4, (%rdx)
	leal	22(%r13), %edx
	addl	$23, %r13d
	movslq	%r13d, %r13
	cmpq	%r15, -240(%rbp)
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r13,8), %rax
	vaddsd	(%rdx), %xmm3, %xmm3
	vmovsd	%xmm3, (%rdx)
	vmovsd	(%rax), %xmm0
	vsubsd	%xmm2, %xmm0, %xmm2
	vmovsd	%xmm2, (%rax)
	jne	.L473
.L453:
	movq	-296(%rbp), %rax
	subq	-288(%rbp), %rax
	sarq	$2, %rax
	testl	%eax, %eax
	jle	.L450
	subl	$1, %eax
	xorl	%r13d, %r13d
	leaq	4(,%rax,4), %r14
	.p2align 4,,10
	.p2align 3
.L495:
	movq	144(%rbx), %rax
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movq	(%rax), %rax
	movq	(%rax), %rax
	movl	(%rax,%r13), %r12d
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L476
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L476:
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -208(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L477
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L477:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$1, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -200(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L478
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L478:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$2, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -192(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L479
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L479:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$3, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -184(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L480
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L480:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$4, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -176(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L481
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L481:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movq	8(%rdx), %rdx
	addl	$5, %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -168(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L482
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L482:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -160(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L483
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L483:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -152(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L484
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L484:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -144(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L485
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L485:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -136(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L486
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L486:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -128(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L487
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L487:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,2), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -120(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L488
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L488:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -112(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L489
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L489:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	1(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -104(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L490
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L490:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	2(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -96(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L491
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L491:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	3(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -88(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L492
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L492:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	4(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -80(%rbp)
	movq	160(%rbx), %rax
	testb	$1, %al
	je	.L493
	movq	168(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L493:
	movq	%rbx, %rdi
	addq	168(%rbx), %rdi
	movl	%r12d, %esi
	call	*%rax
	imull	68(%rbx), %eax
	movq	8(%rbx), %rdx
	movl	60(%rbx), %ecx
	movq	8(%rdx), %rdx
	leal	5(%rax,%rcx,4), %eax
	cltq
	movq	(%rdx,%rax,8), %rax
	movq	%rax, -72(%rbp)
	movq	192(%rbx), %rax
	testb	$1, %al
	je	.L494
	movq	200(%rbx), %rdx
	movq	(%rbx,%rdx), %rdx
	movq	-1(%rdx,%rax), %rax
.L494:
	movq	%rbx, %rdi
	addq	200(%rbx), %rdi
	xorl	%edx, %edx
	movl	%r12d, %esi
	call	*%rax
.LEHE18:
	movl	52(%rbx), %ecx
	movq	-264(%rbp), %rsi
	addq	$4, %r13
	vmovsd	-200(%rbp), %xmm12
	imull	%ecx, %eax
	movq	8(%rsi), %rdx
	imull	%ecx, %r12d
	movslq	%eax, %rdi
	leal	19(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm10
	leal	1(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm10, %xmm10
	movslq	%edi, %rdi
	leal	18(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm9
	leal	6(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm9, %xmm9
	movslq	%edi, %rdi
	leal	13(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm8
	leal	7(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm8, %xmm8
	movslq	%edi, %rdi
	leal	12(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm7
	leal	2(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm10, %xmm15
	vaddsd	(%rdx,%rsi,8), %xmm7, %xmm7
	movslq	%edi, %rdi
	leal	21(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm6
	leal	3(%rax), %edi
	movslq	%esi, %rsi
	vmulsd	%xmm12, %xmm9, %xmm13
	vsubsd	(%rdx,%rsi,8), %xmm6, %xmm6
	movslq	%edi, %rdi
	leal	20(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm5
	leal	8(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm5, %xmm5
	movslq	%edi, %rdi
	leal	15(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm4
	leal	9(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm4, %xmm4
	movslq	%edi, %rdi
	leal	14(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm3
	leal	4(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm3, %xmm3
	movslq	%edi, %rdi
	leal	23(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm2
	leal	5(%rax), %edi
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm2, %xmm2
	movslq	%edi, %rdi
	leal	22(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm1
	leal	10(%rax), %edi
	movslq	%esi, %rsi
	vaddsd	(%rdx,%rsi,8), %xmm1, %xmm1
	movslq	%edi, %rdi
	leal	17(%rax), %esi
	vmovsd	(%rdx,%rdi,8), %xmm0
	movslq	%esi, %rsi
	vsubsd	(%rdx,%rsi,8), %xmm0, %xmm0
	leal	11(%rax), %esi
	addl	$16, %eax
	cltq
	movslq	%esi, %rsi
	vmovsd	(%rdx,%rsi,8), %xmm11
	movq	-256(%rbp), %rsi
	vaddsd	(%rdx,%rax,8), %xmm11, %xmm11
	movslq	%r12d, %rdx
	movq	8(%rsi), %rax
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	%xmm11, -240(%rbp)
	vmovsd	-208(%rbp), %xmm11
	vmulsd	%xmm11, %xmm10, %xmm14
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm15, -216(%rbp)
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-192(%rbp), %xmm11
	vmovsd	-184(%rbp), %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm6, %xmm15
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmovsd	%xmm13, -248(%rbp)
	vmulsd	%xmm11, %xmm5, %xmm13
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-176(%rbp), %xmm11
	vaddsd	-216(%rbp), %xmm15, %xmm15
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-224(%rbp), %xmm13, %xmm13
	vaddsd	-232(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-168(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm14
	vaddsd	-248(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm11, %xmm1, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm15, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vmovsd	%xmm12, (%rdx)
	leal	1(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	6(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	7(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	12(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	13(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm11
	vsubsd	%xmm14, %xmm11, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	18(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	19(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm11
	vsubsd	%xmm15, %xmm11, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	2(%r12), %edx
	vmovsd	-160(%rbp), %xmm11
	vmovsd	-152(%rbp), %xmm12
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm10, %xmm14
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm12, %xmm9, %xmm13
	vmulsd	%xmm12, %xmm10, %xmm15
	vsubsd	%xmm13, %xmm14, %xmm14
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm11, %xmm9, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vxorpd	%xmm13, %xmm13, %xmm13
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm7, %xmm13
	vmulsd	%xmm12, %xmm8, %xmm12
	vmovsd	%xmm15, -224(%rbp)
	vmulsd	%xmm11, %xmm8, %xmm15
	vmulsd	%xmm11, %xmm7, %xmm11
	vsubsd	%xmm13, %xmm15, %xmm13
	vxorpd	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-144(%rbp), %xmm11
	vaddsd	%xmm15, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm12, %xmm12
	vmulsd	%xmm11, %xmm6, %xmm15
	vmovsd	%xmm13, -232(%rbp)
	vmovsd	%xmm12, -216(%rbp)
	vmovsd	-136(%rbp), %xmm12
	vmulsd	%xmm12, %xmm5, %xmm13
	vsubsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm12, %xmm6, %xmm15
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm11, %xmm4, %xmm14
	vmovsd	%xmm13, -248(%rbp)
	vmulsd	%xmm11, %xmm5, %xmm13
	vmulsd	%xmm11, %xmm3, %xmm11
	vaddsd	%xmm13, %xmm15, %xmm15
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm4, %xmm12
	vsubsd	%xmm13, %xmm14, %xmm13
	vaddsd	%xmm11, %xmm12, %xmm12
	vmovsd	-128(%rbp), %xmm11
	vaddsd	-224(%rbp), %xmm15, %xmm15
	vmulsd	%xmm11, %xmm2, %xmm14
	vaddsd	-232(%rbp), %xmm13, %xmm13
	vaddsd	-216(%rbp), %xmm12, %xmm12
	vmovsd	%xmm13, -224(%rbp)
	vmovsd	%xmm12, -232(%rbp)
	vmovsd	-120(%rbp), %xmm12
	vmulsd	%xmm12, %xmm1, %xmm13
	vsubsd	%xmm13, %xmm14, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm14
	vaddsd	-248(%rbp), %xmm13, %xmm13
	vmovsd	%xmm13, -216(%rbp)
	vmulsd	%xmm11, %xmm1, %xmm13
	vaddsd	%xmm13, %xmm14, %xmm13
	vmulsd	-240(%rbp), %xmm12, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm12
	vaddsd	%xmm13, %xmm15, %xmm13
	vmulsd	%xmm11, %xmm0, %xmm15
	vmulsd	-240(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm15, %xmm14
	vmovsd	-216(%rbp), %xmm15
	vaddsd	%xmm11, %xmm12, %xmm11
	vaddsd	(%rdx), %xmm15, %xmm12
	vaddsd	-224(%rbp), %xmm14, %xmm14
	vaddsd	-232(%rbp), %xmm11, %xmm11
	vmovsd	%xmm12, (%rdx)
	leal	3(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	8(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	9(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm12
	vmovsd	%xmm12, (%rdx)
	leal	14(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm11, %xmm11
	vmovsd	%xmm11, (%rdx)
	leal	15(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm11
	vsubsd	%xmm14, %xmm11, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	20(%r12), %edx
	vmovsd	-104(%rbp), %xmm12
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm13, %xmm13
	vmovsd	%xmm13, (%rdx)
	leal	21(%r12), %edx
	vmulsd	%xmm12, %xmm9, %xmm13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm11
	vsubsd	%xmm15, %xmm11, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	%xmm11, (%rdx)
	leal	4(%r12), %edx
	vmovsd	-112(%rbp), %xmm11
	movslq	%edx, %rdx
	vmulsd	%xmm11, %xmm10, %xmm14
	leaq	(%rax,%rdx,8), %rdx
	vmulsd	%xmm12, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm9, %xmm9
	vsubsd	%xmm13, %xmm14, %xmm13
	vxorpd	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm10, %xmm9
	vmulsd	%xmm12, %xmm7, %xmm10
	vmulsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm9, %xmm9
	vmulsd	%xmm11, %xmm8, %xmm14
	vmulsd	%xmm12, %xmm8, %xmm8
	vmovsd	-96(%rbp), %xmm11
	vsubsd	%xmm10, %xmm14, %xmm14
	vmovsd	-88(%rbp), %xmm10
	vaddsd	%xmm7, %xmm8, %xmm7
	vmulsd	%xmm11, %xmm6, %xmm12
	vmulsd	%xmm10, %xmm5, %xmm8
	vmulsd	%xmm10, %xmm6, %xmm6
	vmulsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm15, %xmm14, %xmm14
	vaddsd	%xmm15, %xmm7, %xmm7
	vmovsd	-80(%rbp), %xmm15
	vsubsd	%xmm8, %xmm12, %xmm8
	vaddsd	%xmm5, %xmm6, %xmm5
	vmulsd	%xmm11, %xmm4, %xmm6
	vmulsd	%xmm10, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm13, %xmm13
	vaddsd	%xmm5, %xmm9, %xmm9
	vmulsd	%xmm10, %xmm3, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm3, %xmm4, %xmm3
	vaddsd	%xmm5, %xmm14, %xmm14
	vmulsd	%xmm15, %xmm2, %xmm5
	vaddsd	%xmm3, %xmm7, %xmm7
	vmovsd	-72(%rbp), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm15, %xmm0, %xmm2
	vmulsd	-240(%rbp), %xmm15, %xmm15
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm1, %xmm9, %xmm9
	vmulsd	-240(%rbp), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vsubsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm7, %xmm7
	vaddsd	(%rdx), %xmm13, %xmm0
	vaddsd	%xmm1, %xmm14, %xmm14
	vmovsd	%xmm0, (%rdx)
	leal	5(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm9, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	10(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm14, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	11(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm0
	vmovsd	%xmm0, (%rdx)
	leal	16(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vaddsd	(%rdx), %xmm7, %xmm7
	vmovsd	%xmm7, (%rdx)
	leal	17(%r12), %edx
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovsd	(%rdx), %xmm0
	vsubsd	%xmm14, %xmm0, %xmm14
	vmovsd	%xmm14, (%rdx)
	leal	22(%r12), %edx
	addl	$23, %r12d
	movslq	%r12d, %r12
	cmpq	%r14, %r13
	movslq	%edx, %rdx
	leaq	(%rax,%rdx,8), %rdx
	leaq	(%rax,%r12,8), %rax
	vaddsd	(%rdx), %xmm9, %xmm9
	vmovsd	%xmm9, (%rdx)
	vmovsd	(%rax), %xmm0
	vsubsd	%xmm13, %xmm0, %xmm13
	vmovsd	%xmm13, (%rax)
	jne	.L495
.L450:
	movq	-56(%rbp), %rax
	xorq	%fs:40, %rax
	jne	.L502
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L498:
	.cfi_restore_state
	movq	%rax, %rdi
.LEHB19:
	call	_Unwind_Resume
.LEHE19:
.L502:
	call	__stack_chk_fail
	.cfi_endproc
.LFE2811:
	.section	.gcc_except_table
.LLSDA2811:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2811-.LLSDACSB2811
.LLSDACSB2811:
	.uleb128 .LEHB18-.LFB2811
	.uleb128 .LEHE18-.LEHB18
	.uleb128 .L498-.LFB2811
	.uleb128 0
	.uleb128 .LEHB19-.LFB2811
	.uleb128 .LEHE19-.LEHB19
	.uleb128 0
	.uleb128 0
.LLSDACSE2811:
	.text
	.size	_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_, .-_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_
	.section	.text._ZNK12Dirac_Wilson9mult_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson9mult_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson9mult_precERK5Field
	.type	_ZNK12Dirac_Wilson9mult_precERK5Field, @function
_ZNK12Dirac_Wilson9mult_precERK5Field:
.LFB2708:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2708:
	.size	_ZNK12Dirac_Wilson9mult_precERK5Field, .-_ZNK12Dirac_Wilson9mult_precERK5Field
	.text
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson4multERK5Field
	.type	_ZNK12Dirac_Wilson4multERK5Field, @function
_ZNK12Dirac_Wilson4multERK5Field:
.LFB2824:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2824
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	movq	%rdx, %r14
	.cfi_offset 14, -24
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	.cfi_offset 12, -40
	.cfi_offset 13, -32
	pushq	%rbx
	movq	80(%rsi), %r13
	movq	%rdi, %rbx
	.cfi_offset 3, -48
	movq	%r13, (%rdi)
	salq	$3, %r13
	movq	%r13, %rdi
.LEHB20:
	call	_Znwm
.LEHE20:
	addq	%rax, %r13
	movq	%rax, 8(%rbx)
	cmpq	%r13, %rax
	je	.L505
	leaq	8(%rax), %rdx
	movq	%rax, %rsi
	movq	%r13, %rdi
	andl	$31, %esi
	subq	%rdx, %rdi
	shrq	$3, %rsi
	movq	%rax, %rdx
	shrq	$3, %rdi
	negq	%rsi
	addq	$1, %rdi
	andl	$3, %esi
	cmpq	%rsi, %rdi
	cmovbe	%rdi, %rsi
	testq	%rsi, %rsi
	je	.L506
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L507:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rcx, %rsi
	ja	.L507
	cmpq	%rsi, %rdi
	je	.L505
.L506:
	movq	%rdi, %r9
	subq	%rsi, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L520
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rcx
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L510:
	addq	$1, %rax
	vmovapd	%ymm0, (%rcx)
	addq	$32, %rcx
	cmpq	%rdi, %rax
	jb	.L510
	cmpq	%r8, %r9
	leaq	(%rdx,%r8,8), %rdx
	je	.L505
	.p2align 4,,10
	.p2align 3
.L520:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r13
	jne	.L520
.L505:
	movq	224(%r12), %rax
	movq	%r12, %rdi
	addq	232(%r12), %rdi
	testb	$1, %al
	je	.L514
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L514:
	movq	%r14, %rdx
	movq	%rbx, %rsi
	vzeroupper
.LEHB21:
	call	*%rax
.LEHE21:
	movq	%rbx, %rax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
.L519:
	.cfi_restore_state
	movq	8(%rbx), %rdi
	movq	%rax, %r12
	call	_ZdlPv
	movq	%r12, %rdi
.LEHB22:
	call	_Unwind_Resume
.LEHE22:
	.cfi_endproc
.LFE2824:
	.section	.gcc_except_table
.LLSDA2824:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2824-.LLSDACSB2824
.LLSDACSB2824:
	.uleb128 .LEHB20-.LFB2824
	.uleb128 .LEHE20-.LEHB20
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB21-.LFB2824
	.uleb128 .LEHE21-.LEHB21
	.uleb128 .L519-.LFB2824
	.uleb128 0
	.uleb128 .LEHB22-.LFB2824
	.uleb128 .LEHE22-.LEHB22
	.uleb128 0
	.uleb128 0
.LLSDACSE2824:
	.text
	.size	_ZNK12Dirac_Wilson4multERK5Field, .-_ZNK12Dirac_Wilson4multERK5Field
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson6gamma5ERK5Field
	.type	_ZNK12Dirac_Wilson6gamma5ERK5Field, @function
_ZNK12Dirac_Wilson6gamma5ERK5Field:
.LFB2819:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	movq	%rdx, %r14
	.cfi_offset 14, -24
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	.cfi_offset 12, -40
	.cfi_offset 13, -32
	pushq	%rbx
	movq	80(%rsi), %r13
	movq	%rdi, %rbx
	.cfi_offset 3, -48
	movq	%r13, (%rdi)
	salq	$3, %r13
	movq	%r13, %rdi
	call	_Znwm
	addq	%rax, %r13
	movq	%rax, 8(%rbx)
	cmpq	%r13, %rax
	je	.L526
	leaq	8(%rax), %rdx
	movq	%rax, %rsi
	movq	%r13, %rdi
	andl	$31, %esi
	subq	%rdx, %rdi
	shrq	$3, %rsi
	movq	%rax, %rdx
	shrq	$3, %rdi
	negq	%rsi
	addq	$1, %rdi
	andl	$3, %esi
	cmpq	%rsi, %rdi
	cmovbe	%rdi, %rsi
	testq	%rsi, %rsi
	je	.L527
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L528:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rsi, %rcx
	jb	.L528
	cmpq	%rdi, %rsi
	je	.L526
.L527:
	movq	%rdi, %r9
	subq	%rsi, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L537
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rsi
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L531:
	addq	$1, %rcx
	vmovapd	%ymm0, (%rsi)
	addq	$32, %rsi
	cmpq	%rcx, %rdi
	ja	.L531
	cmpq	%r9, %r8
	leaq	(%rdx,%r8,8), %rdx
	je	.L526
	.p2align 4,,10
	.p2align 3
.L537:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r13
	jne	.L537
.L526:
	movl	24(%r12), %r8d
	testl	%r8d, %r8d
	jle	.L525
	movslq	52(%r12), %r9
	movq	8(%r14), %rcx
	xorl	%edx, %edx
	xorl	%esi, %esi
	salq	$3, %r9
	.p2align 4,,10
	.p2align 3
.L535:
	movq	96(%rcx,%rdx), %rdi
	addl	$1, %esi
	movq	%rdi, (%rax,%rdx)
	movq	104(%rcx,%rdx), %rdi
	movq	%rdi, 8(%rax,%rdx)
	movq	144(%rcx,%rdx), %rdi
	movq	%rdi, 48(%rax,%rdx)
	movq	152(%rcx,%rdx), %rdi
	movq	%rdi, 56(%rax,%rdx)
	movq	(%rcx,%rdx), %rdi
	movq	%rdi, 96(%rax,%rdx)
	movq	8(%rcx,%rdx), %rdi
	movq	%rdi, 104(%rax,%rdx)
	movq	48(%rcx,%rdx), %rdi
	movq	%rdi, 144(%rax,%rdx)
	movq	56(%rcx,%rdx), %rdi
	movq	%rdi, 152(%rax,%rdx)
	movq	112(%rcx,%rdx), %rdi
	movq	%rdi, 16(%rax,%rdx)
	movq	120(%rcx,%rdx), %rdi
	movq	%rdi, 24(%rax,%rdx)
	movq	160(%rcx,%rdx), %rdi
	movq	%rdi, 64(%rax,%rdx)
	movq	168(%rcx,%rdx), %rdi
	movq	%rdi, 72(%rax,%rdx)
	movq	16(%rcx,%rdx), %rdi
	movq	%rdi, 112(%rax,%rdx)
	movq	24(%rcx,%rdx), %rdi
	movq	%rdi, 120(%rax,%rdx)
	movq	64(%rcx,%rdx), %rdi
	movq	%rdi, 160(%rax,%rdx)
	movq	72(%rcx,%rdx), %rdi
	movq	%rdi, 168(%rax,%rdx)
	movq	128(%rcx,%rdx), %rdi
	movq	%rdi, 32(%rax,%rdx)
	movq	136(%rcx,%rdx), %rdi
	movq	%rdi, 40(%rax,%rdx)
	movq	176(%rcx,%rdx), %rdi
	movq	%rdi, 80(%rax,%rdx)
	movq	184(%rcx,%rdx), %rdi
	movq	%rdi, 88(%rax,%rdx)
	movq	32(%rcx,%rdx), %rdi
	movq	%rdi, 128(%rax,%rdx)
	movq	40(%rcx,%rdx), %rdi
	movq	%rdi, 136(%rax,%rdx)
	movq	80(%rcx,%rdx), %rdi
	movq	%rdi, 176(%rax,%rdx)
	movq	88(%rcx,%rdx), %rdi
	movq	%rdi, 184(%rax,%rdx)
	addq	%r9, %rdx
	cmpl	%r8d, %esi
	jne	.L535
.L525:
	movq	%rbx, %rax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
	.cfi_endproc
.LFE2819:
	.size	_ZNK12Dirac_Wilson6gamma5ERK5Field, .-_ZNK12Dirac_Wilson6gamma5ERK5Field
	.section	.text._ZNK12Dirac_Wilson13mult_dag_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson13mult_dag_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson13mult_dag_precERK5Field
	.type	_ZNK12Dirac_Wilson13mult_dag_precERK5Field, @function
_ZNK12Dirac_Wilson13mult_dag_precERK5Field:
.LFB2712:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2712:
	.size	_ZNK12Dirac_Wilson13mult_dag_precERK5Field, .-_ZNK12Dirac_Wilson13mult_dag_precERK5Field
	.section	.text._ZNK12Dirac_Wilson14right_dag_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson14right_dag_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson14right_dag_precERK5Field
	.type	_ZNK12Dirac_Wilson14right_dag_precERK5Field, @function
_ZNK12Dirac_Wilson14right_dag_precERK5Field:
.LFB2716:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2716:
	.size	_ZNK12Dirac_Wilson14right_dag_precERK5Field, .-_ZNK12Dirac_Wilson14right_dag_precERK5Field
	.section	.text._ZNK12Dirac_Wilson13left_dag_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson13left_dag_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson13left_dag_precERK5Field
	.type	_ZNK12Dirac_Wilson13left_dag_precERK5Field, @function
_ZNK12Dirac_Wilson13left_dag_precERK5Field:
.LFB2715:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2715:
	.size	_ZNK12Dirac_Wilson13left_dag_precERK5Field, .-_ZNK12Dirac_Wilson13left_dag_precERK5Field
	.section	.text._ZNK12Dirac_Wilson10right_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson10right_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson10right_precERK5Field
	.type	_ZNK12Dirac_Wilson10right_precERK5Field, @function
_ZNK12Dirac_Wilson10right_precERK5Field:
.LFB2714:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2714:
	.size	_ZNK12Dirac_Wilson10right_precERK5Field, .-_ZNK12Dirac_Wilson10right_precERK5Field
	.section	.text._ZNK12Dirac_Wilson9left_precERK5Field,"axG",@progbits,_ZNK12Dirac_Wilson9left_precERK5Field,comdat
	.align 2
	.p2align 4,,15
	.weak	_ZNK12Dirac_Wilson9left_precERK5Field
	.type	_ZNK12Dirac_Wilson9left_precERK5Field, @function
_ZNK12Dirac_Wilson9left_precERK5Field:
.LFB2713:
	.cfi_startproc
	movq	%rbx, -16(%rsp)
	movq	%rbp, -8(%rsp)
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -16
	.cfi_offset 3, -24
	movq	(%rdx), %rax
	movq	%rdi, %rbx
	movq	%rdx, %rbp
	movq	%rax, (%rdi)
	movq	(%rdx), %rdi
	salq	$3, %rdi
	call	_Znwm
	movq	(%rbx), %rdx
	movq	%rax, 8(%rbx)
	movq	%rax, %rdi
	movq	8(%rbp), %rsi
	salq	$3, %rdx
	call	memcpy
	movq	%rbx, %rax
	movq	16(%rsp), %rbp
	movq	8(%rsp), %rbx
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2713:
	.size	_ZNK12Dirac_Wilson9left_precERK5Field, .-_ZNK12Dirac_Wilson9left_precERK5Field
	.text
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson6proj_pERK5Field
	.type	_ZNK12Dirac_Wilson6proj_pERK5Field, @function
_ZNK12Dirac_Wilson6proj_pERK5Field:
.LFB2820:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	movq	%rdx, %r14
	.cfi_offset 14, -24
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	.cfi_offset 12, -40
	.cfi_offset 13, -32
	pushq	%rbx
	movq	80(%rsi), %r13
	movq	%rdi, %rbx
	.cfi_offset 3, -48
	movq	%r13, (%rdi)
	salq	$3, %r13
	movq	%r13, %rdi
	call	_Znwm
	addq	%rax, %r13
	movq	%rax, 8(%rbx)
	cmpq	%r13, %rax
	je	.L548
	leaq	8(%rax), %rdx
	movq	%rax, %rsi
	movq	%r13, %rdi
	andl	$31, %esi
	subq	%rdx, %rdi
	shrq	$3, %rsi
	movq	%rax, %rdx
	shrq	$3, %rdi
	negq	%rsi
	addq	$1, %rdi
	andl	$3, %esi
	cmpq	%rsi, %rdi
	cmovbe	%rdi, %rsi
	testq	%rsi, %rsi
	je	.L549
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L550:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rsi, %rcx
	jb	.L550
	cmpq	%rdi, %rsi
	je	.L548
.L549:
	movq	%rdi, %r9
	subq	%rsi, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L559
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rsi
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L553:
	addq	$1, %rcx
	vmovapd	%ymm0, (%rsi)
	addq	$32, %rsi
	cmpq	%rcx, %rdi
	ja	.L553
	cmpq	%r9, %r8
	leaq	(%rdx,%r8,8), %rdx
	je	.L548
	.p2align 4,,10
	.p2align 3
.L559:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r13
	jne	.L559
.L548:
	movl	24(%r12), %edi
	testl	%edi, %edi
	jle	.L547
	movslq	52(%r12), %r8
	movq	8(%r14), %rcx
	xorl	%edx, %edx
	vmovsd	.LC1(%rip), %xmm0
	xorl	%esi, %esi
	salq	$3, %r8
	.p2align 4,,10
	.p2align 3
.L557:
	vmovsd	(%rcx,%rdx), %xmm4
	addl	$1, %esi
	vmovsd	8(%rcx,%rdx), %xmm3
	vmovsd	48(%rcx,%rdx), %xmm2
	vaddsd	96(%rcx,%rdx), %xmm4, %xmm4
	vmovsd	56(%rcx,%rdx), %xmm1
	vaddsd	104(%rcx,%rdx), %xmm3, %xmm3
	vaddsd	144(%rcx,%rdx), %xmm2, %xmm2
	vaddsd	152(%rcx,%rdx), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm4, (%rax,%rdx)
	vmovsd	%xmm3, 8(%rax,%rdx)
	vmovsd	%xmm2, 48(%rax,%rdx)
	vmovsd	%xmm1, 56(%rax,%rdx)
	vmovsd	%xmm4, 96(%rax,%rdx)
	vmovsd	%xmm3, 104(%rax,%rdx)
	vmovsd	%xmm2, 144(%rax,%rdx)
	vmovsd	%xmm1, 152(%rax,%rdx)
	vmovsd	16(%rcx,%rdx), %xmm4
	vmovsd	24(%rcx,%rdx), %xmm3
	vmovsd	64(%rcx,%rdx), %xmm2
	vaddsd	112(%rcx,%rdx), %xmm4, %xmm4
	vmovsd	72(%rcx,%rdx), %xmm1
	vaddsd	120(%rcx,%rdx), %xmm3, %xmm3
	vaddsd	160(%rcx,%rdx), %xmm2, %xmm2
	vaddsd	168(%rcx,%rdx), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm4, 16(%rax,%rdx)
	vmovsd	%xmm3, 24(%rax,%rdx)
	vmovsd	%xmm2, 64(%rax,%rdx)
	vmovsd	%xmm1, 72(%rax,%rdx)
	vmovsd	%xmm4, 112(%rax,%rdx)
	vmovsd	%xmm3, 120(%rax,%rdx)
	vmovsd	%xmm2, 160(%rax,%rdx)
	vmovsd	%xmm1, 168(%rax,%rdx)
	vmovsd	32(%rcx,%rdx), %xmm4
	vmovsd	40(%rcx,%rdx), %xmm3
	vmovsd	80(%rcx,%rdx), %xmm2
	vaddsd	128(%rcx,%rdx), %xmm4, %xmm4
	vmovsd	88(%rcx,%rdx), %xmm1
	vaddsd	136(%rcx,%rdx), %xmm3, %xmm3
	vaddsd	176(%rcx,%rdx), %xmm2, %xmm2
	vaddsd	184(%rcx,%rdx), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmovsd	%xmm4, 32(%rax,%rdx)
	vmovsd	%xmm3, 40(%rax,%rdx)
	vmovsd	%xmm2, 80(%rax,%rdx)
	vmovsd	%xmm1, 88(%rax,%rdx)
	vmovsd	%xmm4, 128(%rax,%rdx)
	vmovsd	%xmm3, 136(%rax,%rdx)
	vmovsd	%xmm2, 176(%rax,%rdx)
	vmovsd	%xmm1, 184(%rax,%rdx)
	addq	%r8, %rdx
	cmpl	%edi, %esi
	jne	.L557
.L547:
	movq	%rbx, %rax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
	.cfi_endproc
.LFE2820:
	.size	_ZNK12Dirac_Wilson6proj_pERK5Field, .-_ZNK12Dirac_Wilson6proj_pERK5Field
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson6proj_mERK5Field
	.type	_ZNK12Dirac_Wilson6proj_mERK5Field, @function
_ZNK12Dirac_Wilson6proj_mERK5Field:
.LFB2821:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r14
	movq	%rdx, %r14
	.cfi_offset 14, -24
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	.cfi_offset 12, -40
	.cfi_offset 13, -32
	pushq	%rbx
	movq	80(%rsi), %r13
	movq	%rdi, %rbx
	.cfi_offset 3, -48
	movq	%r13, (%rdi)
	salq	$3, %r13
	movq	%r13, %rdi
	call	_Znwm
	addq	%rax, %r13
	movq	%rax, 8(%rbx)
	cmpq	%r13, %rax
	je	.L565
	leaq	8(%rax), %rdx
	movq	%rax, %rsi
	movq	%r13, %rdi
	andl	$31, %esi
	subq	%rdx, %rdi
	shrq	$3, %rsi
	movq	%rax, %rdx
	shrq	$3, %rdi
	negq	%rsi
	addq	$1, %rdi
	andl	$3, %esi
	cmpq	%rsi, %rdi
	cmovbe	%rdi, %rsi
	testq	%rsi, %rsi
	je	.L566
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L567:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rsi, %rcx
	jb	.L567
	cmpq	%rdi, %rsi
	je	.L565
.L566:
	movq	%rdi, %r9
	subq	%rsi, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L576
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rsi
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L570:
	addq	$1, %rcx
	vmovapd	%ymm0, (%rsi)
	addq	$32, %rsi
	cmpq	%rdi, %rcx
	jb	.L570
	cmpq	%r9, %r8
	leaq	(%rdx,%r8,8), %rdx
	je	.L565
	.p2align 4,,10
	.p2align 3
.L576:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r13
	jne	.L576
.L565:
	movl	24(%r12), %edi
	testl	%edi, %edi
	jle	.L564
	movslq	52(%r12), %r8
	movq	8(%r14), %rcx
	xorl	%edx, %edx
	vmovsd	.LC1(%rip), %xmm1
	xorl	%esi, %esi
	vmovsd	.LC2(%rip), %xmm0
	salq	$3, %r8
	.p2align 4,,10
	.p2align 3
.L574:
	vmovsd	(%rcx,%rdx), %xmm5
	addl	$1, %esi
	vmovsd	8(%rcx,%rdx), %xmm4
	vmovsd	48(%rcx,%rdx), %xmm3
	vsubsd	96(%rcx,%rdx), %xmm5, %xmm5
	vmovsd	56(%rcx,%rdx), %xmm2
	vsubsd	104(%rcx,%rdx), %xmm4, %xmm4
	vsubsd	144(%rcx,%rdx), %xmm3, %xmm3
	vsubsd	152(%rcx,%rdx), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm5, %xmm5
	vmulsd	%xmm1, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vmovsd	%xmm5, (%rax,%rdx)
	vxorpd	%xmm0, %xmm5, %xmm5
	vmovsd	%xmm4, 8(%rax,%rdx)
	vxorpd	%xmm0, %xmm4, %xmm4
	vmovsd	%xmm3, 48(%rax,%rdx)
	vxorpd	%xmm0, %xmm3, %xmm3
	vmovsd	%xmm2, 56(%rax,%rdx)
	vxorpd	%xmm0, %xmm2, %xmm2
	vmovsd	%xmm5, 96(%rax,%rdx)
	vmovsd	%xmm4, 104(%rax,%rdx)
	vmovsd	%xmm3, 144(%rax,%rdx)
	vmovsd	%xmm2, 152(%rax,%rdx)
	vmovsd	16(%rcx,%rdx), %xmm5
	vmovsd	24(%rcx,%rdx), %xmm4
	vmovsd	64(%rcx,%rdx), %xmm3
	vsubsd	112(%rcx,%rdx), %xmm5, %xmm5
	vmovsd	72(%rcx,%rdx), %xmm2
	vsubsd	120(%rcx,%rdx), %xmm4, %xmm4
	vsubsd	160(%rcx,%rdx), %xmm3, %xmm3
	vsubsd	168(%rcx,%rdx), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm5, %xmm5
	vmulsd	%xmm1, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vmovsd	%xmm5, 16(%rax,%rdx)
	vxorpd	%xmm0, %xmm5, %xmm5
	vmovsd	%xmm4, 24(%rax,%rdx)
	vxorpd	%xmm0, %xmm4, %xmm4
	vmovsd	%xmm3, 64(%rax,%rdx)
	vxorpd	%xmm0, %xmm3, %xmm3
	vmovsd	%xmm2, 72(%rax,%rdx)
	vxorpd	%xmm0, %xmm2, %xmm2
	vmovsd	%xmm5, 112(%rax,%rdx)
	vmovsd	%xmm4, 120(%rax,%rdx)
	vmovsd	%xmm3, 160(%rax,%rdx)
	vmovsd	%xmm2, 168(%rax,%rdx)
	vmovsd	32(%rcx,%rdx), %xmm5
	vmovsd	40(%rcx,%rdx), %xmm4
	vmovsd	80(%rcx,%rdx), %xmm3
	vsubsd	128(%rcx,%rdx), %xmm5, %xmm5
	vmovsd	88(%rcx,%rdx), %xmm2
	vsubsd	136(%rcx,%rdx), %xmm4, %xmm4
	vsubsd	176(%rcx,%rdx), %xmm3, %xmm3
	vsubsd	184(%rcx,%rdx), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm5, %xmm5
	vmulsd	%xmm1, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vmovsd	%xmm5, 32(%rax,%rdx)
	vxorpd	%xmm0, %xmm5, %xmm5
	vmovsd	%xmm4, 40(%rax,%rdx)
	vxorpd	%xmm0, %xmm4, %xmm4
	vmovsd	%xmm3, 80(%rax,%rdx)
	vxorpd	%xmm0, %xmm3, %xmm3
	vmovsd	%xmm2, 88(%rax,%rdx)
	vxorpd	%xmm0, %xmm2, %xmm2
	vmovsd	%xmm5, 128(%rax,%rdx)
	vmovsd	%xmm4, 136(%rax,%rdx)
	vmovsd	%xmm3, 176(%rax,%rdx)
	vmovsd	%xmm2, 184(%rax,%rdx)
	addq	%r8, %rdx
	cmpl	%edi, %esi
	jne	.L574
.L564:
	movq	%rbx, %rax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
	.cfi_endproc
.LFE2821:
	.size	_ZNK12Dirac_Wilson6proj_mERK5Field, .-_ZNK12Dirac_Wilson6proj_mERK5Field
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_, @function
_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_:
.LFB2822:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	movq	%rdx, %r15
	.cfi_offset 15, -24
	pushq	%r14
	movq	%rsi, %r14
	.cfi_offset 14, -32
	pushq	%r13
	pushq	%r12
	movq	%rdi, %r12
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	pushq	%rbx
	movl	28(%rdi), %r11d
	andq	$-32, %rsp
	testl	%r11d, %r11d
	jle	.L582
	.cfi_offset 3, -56
	xorl	%ebx, %ebx
	xorl	%r13d, %r13d
.L587:
	movq	_ZN12Dirac_Wilson6mult_pE(%rbx), %rax
	movq	%r12, %rdi
	addq	_ZN12Dirac_Wilson6mult_pE+8(%rbx), %rdi
	testb	$1, %al
	je	.L584
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L584:
	movq	%r15, %rdx
	movq	%r14, %rsi
	call	*%rax
	movq	_ZN12Dirac_Wilson6mult_mE(%rbx), %rax
	movq	%r12, %rdi
	addq	_ZN12Dirac_Wilson6mult_mE+8(%rbx), %rdi
	testb	$1, %al
	je	.L586
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L586:
	movq	%r15, %rdx
	movq	%r14, %rsi
	call	*%rax
	addl	$1, %r13d
	addq	$16, %rbx
	cmpl	%r13d, 28(%r12)
	jg	.L587
.L582:
	movq	8(%r14), %rsi
	movq	(%r14), %rax
	vmovsd	16(%r12), %xmm0
	vmovsd	.LC2(%rip), %xmm1
	leaq	(%rsi,%rax,8), %rdi
	vxorpd	%xmm1, %xmm0, %xmm0
	cmpq	%rdi, %rsi
	jae	.L581
	leaq	1(%rsi), %rax
	leaq	8(%rsi), %rdx
	movl	$1, %r8d
	cmpq	%rdi, %rax
	jbe	.L602
.L590:
	movq	%rsi, %rcx
	movq	%rsi, %rax
	andl	$31, %ecx
	shrq	$3, %rcx
	negq	%rcx
	andl	$3, %ecx
	cmpq	%rcx, %r8
	cmovbe	%r8, %rcx
	testq	%rcx, %rcx
	je	.L591
	xorl	%edx, %edx
	.p2align 4,,10
	.p2align 3
.L592:
	vmulsd	(%rax), %xmm0, %xmm1
	addq	$1, %rdx
	vmovsd	%xmm1, (%rax)
	addq	$8, %rax
	cmpq	%rcx, %rdx
	jb	.L592
	cmpq	%rcx, %r8
	je	.L581
.L591:
	movq	%r8, %r10
	subq	%rcx, %r10
	movq	%r10, %r8
	shrq	$2, %r8
	leaq	0(,%r8,4), %r9
	testq	%r9, %r9
	je	.L597
	vmovddup	%xmm0, %xmm2
	leaq	(%rsi,%rcx,8), %rsi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	vinsertf128	$1, %xmm2, %ymm2, %ymm2
	.p2align 4,,10
	.p2align 3
.L594:
	vmulpd	(%rsi,%rdx), %ymm2, %ymm1
	addq	$1, %rcx
	vmovapd	%ymm1, (%rsi,%rdx)
	addq	$32, %rdx
	cmpq	%r8, %rcx
	jb	.L594
	cmpq	%r9, %r10
	leaq	(%rax,%r9,8), %rax
	je	.L581
	.p2align 4,,10
	.p2align 3
.L597:
	vmulsd	(%rax), %xmm0, %xmm1
	vmovsd	%xmm1, (%rax)
	addq	$8, %rax
	cmpq	%rax, %rdi
	ja	.L597
.L581:
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
	.p2align 4,,10
	.p2align 3
.L602:
	.cfi_restore_state
	movq	%rdi, %r8
	subq	%rdx, %r8
	addq	$7, %r8
	shrq	$3, %r8
	addq	$1, %r8
	jmp	.L590
	.cfi_endproc
.LFE2822:
	.size	_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_, .-_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson9mult_fullER5FieldRKS0_
	.type	_ZNK12Dirac_Wilson9mult_fullER5FieldRKS0_, @function
_ZNK12Dirac_Wilson9mult_fullER5FieldRKS0_:
.LFB2823:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r12
	movq	%rdx, %r12
	.cfi_offset 12, -24
	pushq	%rbx
	movq	%rsi, %rbx
	.cfi_offset 3, -32
	andq	$-32, %rsp
	call	_ZNK12Dirac_Wilson12mult_offdiagER5FieldRKS0_
	movq	8(%r12), %rsi
	movq	(%rbx), %rax
	movq	8(%rbx), %rdx
	leaq	(%rsi,%rax,8), %rdi
	cmpq	%rdi, %rsi
	jae	.L603
	movq	%rsi, %rax
	leaq	1(%rsi), %rcx
	movl	$1, %r9d
	notq	%rax
	addq	%rdi, %rax
	shrq	$3, %rax
	addq	$1, %rax
	cmpq	%rdi, %rcx
	cmovbe	%rax, %r9
	movq	%r9, %r8
	shrq	$2, %r8
	leaq	0(,%r8,4), %r10
	testq	%r10, %r10
	je	.L612
	cmpq	%rdi, %rcx
	leaq	32(%rdx), %r11
	setbe	%cl
	cmpq	$6, %rax
	seta	%al
	andl	%eax, %ecx
	leaq	32(%rsi), %rax
	cmpq	%rax, %rdx
	seta	%al
	cmpq	%r11, %rsi
	seta	%r11b
	orl	%r11d, %eax
	testb	%al, %cl
	je	.L612
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L610:
	vmovupd	(%rsi,%rax), %xmm0
	addq	$1, %rcx
	vmovupd	(%rdx,%rax), %xmm1
	vinsertf128	$0x1, 16(%rsi,%rax), %ymm0, %ymm0
	vinsertf128	$0x1, 16(%rdx,%rax), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vmovupd	%xmm0, (%rdx,%rax)
	vextractf128	$0x1, %ymm0, 16(%rdx,%rax)
	addq	$32, %rax
	cmpq	%rcx, %r8
	ja	.L610
	leaq	0(,%r10,8), %rax
	addq	%rax, %rdx
	addq	%rsi, %rax
	cmpq	%r10, %r9
	je	.L603
	.p2align 4,,10
	.p2align 3
.L613:
	vmovsd	(%rdx), %xmm0
	vaddsd	(%rax), %xmm0, %xmm0
	addq	$8, %rax
	vmovsd	%xmm0, (%rdx)
	addq	$8, %rdx
	cmpq	%rax, %rdi
	ja	.L613
.L603:
	leaq	-16(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
.L612:
	.cfi_restore_state
	movq	%rsi, %rax
	jmp	.L613
	.cfi_endproc
.LFE2823:
	.size	_ZNK12Dirac_Wilson9mult_fullER5FieldRKS0_, .-_ZNK12Dirac_Wilson9mult_fullER5FieldRKS0_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_
	.type	_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_, @function
_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_:
.LFB2826:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2826
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	addq	$-128, %rsp
	movq	%rdi, 104(%rsp)
	movl	$144, %edi
	movq	%rsi, 40(%rsp)
	movq	%rdx, 16(%rsp)
	movq	%rcx, 72(%rsp)
.LEHB23:
	.cfi_offset 3, -56
	.cfi_offset 12, -48
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	call	_Znwm
.LEHE23:
	movq	%rax, %rbx
	andl	$31, %eax
	shrq	$3, %rax
	negq	%rax
	movq	%rax, %rdi
	movq	%rbx, %rax
	andl	$3, %edi
	je	.L666
	movl	$18, %ecx
	movl	$19, %esi
	jmp	.L618
	.p2align 4,,10
	.p2align 3
.L667:
	movq	%rdx, %rcx
.L618:
	movq	%rsi, %r8
	movq	$0, (%rax)
	addq	$8, %rax
	subq	%rcx, %r8
	leaq	-1(%rcx), %rdx
	cmpq	%r8, %rdi
	ja	.L667
	jmp	.L617
.L666:
	movl	$18, %edx
.L617:
	movl	$18, %esi
	subq	%rdi, %rsi
	movq	%rsi, %r9
	shrq	$2, %r9
	leaq	0(,%r9,4), %rcx
	testq	%rcx, %rcx
	je	.L679
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rbx,%rdi,8), %r8
	xorl	%edi, %edi
.L620:
	addq	$1, %rdi
	vmovapd	%ymm0, (%r8)
	addq	$32, %r8
	cmpq	%r9, %rdi
	jb	.L620
	subq	%rcx, %rdx
	cmpq	%rcx, %rsi
	leaq	(%rax,%rcx,8), %rax
	je	.L621
.L679:
	movq	$0, (%rax)
	addq	$8, %rax
	subq	$1, %rdx
	jne	.L679
.L621:
	movq	104(%rsp), %rax
	movl	28(%rax), %r14d
	testl	%r14d, %r14d
	jle	.L623
	movq	%rbx, %rdx
	movq	$_ZN12Dirac_Wilson6mult_pE+8, 24(%rsp)
	movl	$0, 88(%rsp)
	andl	$31, %edx
	shrq	$3, %rdx
	negq	%rdx
	andl	$3, %edx
	movq	%rdx, 56(%rsp)
.L657:
	movq	104(%rsp), %rcx
	movq	80(%rcx), %r12
	movq	%r12, 112(%rsp)
	salq	$3, %r12
	movq	%r12, %rdi
	vzeroupper
.LEHB24:
	call	_Znwm
.LEHE24:
	addq	%rax, %r12
	movq	%rax, 120(%rsp)
	cmpq	%r12, %rax
	je	.L624
	leaq	8(%rax), %rdx
	movq	%rax, %rcx
	movq	%r12, %r8
	andl	$31, %ecx
	subq	%rdx, %r8
	shrq	$3, %rcx
	movq	%rax, %rdx
	shrq	$3, %r8
	negq	%rcx
	addq	$1, %r8
	andl	$3, %ecx
	cmpq	%rcx, %r8
	cmovbe	%r8, %rcx
	testq	%rcx, %rcx
	je	.L625
	xorl	%esi, %esi
.L626:
	addq	$1, %rsi
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rsi, %rcx
	ja	.L626
	cmpq	%rcx, %r8
	je	.L624
.L625:
	subq	%rcx, %r8
	movq	%r8, %rsi
	shrq	$2, %rsi
	leaq	0(,%rsi,4), %rdi
	testq	%rdi, %rdi
	je	.L678
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rcx,8), %rcx
	xorl	%eax, %eax
.L629:
	addq	$1, %rax
	vmovapd	%ymm0, (%rcx)
	addq	$32, %rcx
	cmpq	%rsi, %rax
	jb	.L629
	cmpq	%rdi, %r8
	leaq	(%rdx,%rdi,8), %rdx
	je	.L624
.L678:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r12
	jne	.L678
.L624:
	movq	24(%rsp), %rsi
	movq	-8(%rsi), %rax
	testb	$1, %al
	je	.L696
	movq	104(%rsp), %rdi
	addq	(%rsi), %rdi
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L633:
	movq	16(%rsp), %rdx
	leaq	112(%rsp), %rsi
	vzeroupper
.LEHB25:
	call	*%rax
	movq	104(%rsp), %rax
	xorl	%r12d, %r12d
	movl	$0, 92(%rsp)
	movl	24(%rax), %r13d
	testl	%r13d, %r13d
	jle	.L635
	movq	$18, 48(%rsp)
	movq	56(%rsp), %rdx
	subq	%rdx, 48(%rsp)
	movq	48(%rsp), %rcx
	shrq	$2, %rcx
	movq	%rcx, 32(%rsp)
	salq	$2, %rcx
	movq	%rcx, 64(%rsp)
	.p2align 4,,10
	.p2align 3
.L691:
	cmpq	$0, 56(%rsp)
	movq	%rbx, %rax
	je	.L672
	movl	$18, %ecx
	movl	$19, %esi
	movq	56(%rsp), %rdi
	jmp	.L653
	.p2align 4,,10
	.p2align 3
.L673:
	movq	%rdx, %rcx
.L653:
	movq	%rsi, %r8
	movq	$0, (%rax)
	addq	$8, %rax
	subq	%rcx, %r8
	leaq	-1(%rcx), %rdx
	cmpq	%r8, %rdi
	ja	.L673
.L652:
	cmpq	$0, 64(%rsp)
	je	.L677
	movq	56(%rsp), %rcx
	vxorpd	%xmm0, %xmm0, %xmm0
	movq	32(%rsp), %r8
	leaq	(%rbx,%rcx,8), %rdi
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L636:
	movq	%rcx, %rsi
	addq	$1, %rcx
	salq	$5, %rsi
	cmpq	%rcx, %r8
	vmovapd	%ymm0, (%rdi,%rsi)
	ja	.L636
	movq	64(%rsp), %rcx
	subq	%rcx, %rdx
	cmpq	%rcx, 48(%rsp)
	leaq	(%rax,%rcx,8), %rax
	je	.L637
	.p2align 4,,10
	.p2align 3
.L677:
	movq	$0, (%rax)
	addq	$8, %rax
	subq	$1, %rdx
	jne	.L677
.L637:
	movq	104(%rsp), %rsi
	movq	120(%rsp), %r9
	movl	$0, 100(%rsp)
	movq	$0, 80(%rsp)
	movl	$0, 96(%rsp)
	movl	32(%rsi), %r8d
.L639:
	movq	80(%rsp), %r15
	movl	96(%rsp), %r14d
	xorl	%r13d, %r13d
.L644:
	movq	%r12, 8(%rsp)
	testl	%r8d, %r8d
	vmovsd	8(%rsp), %xmm5
	vmovapd	%xmm5, %xmm4
	jle	.L642
	movq	104(%rsp), %rax
	movl	92(%rsp), %r10d
	leaq	(%r9,%r15), %r11
	movq	72(%rsp), %rcx
	imull	52(%rax), %r10d
	movq	8(%rcx), %rsi
	xorl	%ecx, %ecx
	movslq	%r10d, %rdx
	leal	(%r10,%r13), %edi
	addl	100(%rsp), %r10d
	addq	%r13, %rdx
	movslq	%edi, %rdi
	salq	$3, %rdx
	leaq	8(,%rdi,8), %rax
	movslq	%r10d, %r10
	subq	%rdi, %r10
	salq	$3, %r10
	.p2align 4,,10
	.p2align 3
.L640:
	leaq	(%r10,%rax), %rdi
	vmovsd	(%rsi,%rdx), %xmm2
	vmovsd	(%r11,%rdx), %xmm1
	addl	$1, %ecx
	vmovsd	(%rsi,%rax), %xmm0
	addq	$48, %rdx
	vmovsd	(%r9,%rdi), %xmm3
	addq	$48, %rax
	vmulsd	%xmm2, %xmm1, %xmm7
	cmpl	%r8d, %ecx
	vmulsd	%xmm0, %xmm3, %xmm6
	vmulsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm5, %xmm5
	jne	.L640
.L642:
	leal	(%r14,%r14), %eax
	addq	$2, %r13
	addl	$1, %r14d
	subq	$16, %r15
	movslq	%eax, %rdx
	addl	$1, %eax
	cmpq	$6, %r13
	cltq
	vmovsd	%xmm4, (%rbx,%rdx,8)
	vmovsd	%xmm5, (%rbx,%rax,8)
	jne	.L644
	addl	$3, 96(%rsp)
	addq	$16, 80(%rsp)
	addl	$2, 100(%rsp)
	cmpl	$9, 96(%rsp)
	jne	.L639
	movq	104(%rsp), %rsi
	movq	160(%rsi), %rax
	movq	%rsi, %rdi
	addq	168(%rsi), %rdi
	testb	$1, %al
	je	.L646
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L646:
	movl	92(%rsp), %esi
	vzeroupper
	call	*%rax
.LEHE25:
	movq	104(%rsp), %r8
	movl	88(%rsp), %edx
	movq	40(%rsp), %rsi
	imull	56(%r8), %edx
	movslq	68(%r8), %rcx
	addl	%edx, %eax
	imull	%ecx, %eax
	cltq
	leaq	0(,%rax,8), %rdx
	addq	8(%rsi), %rdx
	leaq	(%rdx,%rcx,8), %rsi
	cmpq	%rdx, %rsi
	jbe	.L647
	movq	%rdx, %rax
	notq	%rax
	leaq	(%rax,%rsi), %r9
	shrq	$3, %r9
	addq	$1, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L671
	leaq	32(%rdx), %rax
	leaq	32(%rbx), %rcx
	cmpq	%rax, %rbx
	seta	%al
	cmpq	%rcx, %rdx
	seta	%cl
	orl	%ecx, %eax
	cmpq	$6, %r9
	seta	%cl
	testb	%cl, %al
	je	.L671
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L649:
	vmovupd	(%rbx,%rax), %xmm0
	addq	$1, %rcx
	vmovupd	(%rdx,%rax), %xmm1
	vinsertf128	$0x1, 16(%rbx,%rax), %ymm0, %ymm0
	vinsertf128	$0x1, 16(%rdx,%rax), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vmovupd	%xmm0, (%rdx,%rax)
	vextractf128	$0x1, %ymm0, 16(%rdx,%rax)
	addq	$32, %rax
	cmpq	%rcx, %rdi
	ja	.L649
	leaq	0(,%r8,8), %rax
	addq	%rax, %rdx
	addq	%rbx, %rax
	cmpq	%r8, %r9
	je	.L647
	.p2align 4,,10
	.p2align 3
.L676:
	vmovsd	(%rdx), %xmm0
	vaddsd	(%rax), %xmm0, %xmm0
	addq	$8, %rax
	vmovsd	%xmm0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %rsi
	ja	.L676
.L647:
	movq	104(%rsp), %r8
	addl	$1, 92(%rsp)
	movl	92(%rsp), %eax
	cmpl	%eax, 24(%r8)
	jg	.L691
.L635:
	movq	120(%rsp), %rdi
	vzeroupper
	call	_ZdlPv
	movq	104(%rsp), %rsi
	addl	$1, 88(%rsp)
	addq	$16, 24(%rsp)
	movl	88(%rsp), %r8d
	cmpl	%r8d, 28(%rsi)
	jg	.L657
.L623:
	movq	%rbx, %rdi
	vzeroupper
	call	_ZdlPv
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L696:
	.cfi_restore_state
	movq	104(%rsp), %rdi
	addq	(%rsi), %rdi
	jmp	.L633
.L675:
	movq	120(%rsp), %rdi
	movq	%rax, %r12
	call	_ZdlPv
.L663:
	movq	%rbx, %rdi
	call	_ZdlPv
	movq	%r12, %rdi
.LEHB26:
	call	_Unwind_Resume
.LEHE26:
.L671:
	movq	%rbx, %rax
	jmp	.L676
.L672:
	movl	$18, %edx
	jmp	.L652
.L674:
	movq	%rax, %r12
	jmp	.L663
	.cfi_endproc
.LFE2826:
	.section	.gcc_except_table
.LLSDA2826:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2826-.LLSDACSB2826
.LLSDACSB2826:
	.uleb128 .LEHB23-.LFB2826
	.uleb128 .LEHE23-.LEHB23
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB24-.LFB2826
	.uleb128 .LEHE24-.LEHB24
	.uleb128 .L674-.LFB2826
	.uleb128 0
	.uleb128 .LEHB25-.LFB2826
	.uleb128 .LEHE25-.LEHB25
	.uleb128 .L675-.LFB2826
	.uleb128 0
	.uleb128 .LEHB26-.LFB2826
	.uleb128 .LEHE26-.LEHB26
	.uleb128 0
	.uleb128 0
.LLSDACSE2826:
	.text
	.size	_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_, .-_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_
	.type	_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_, @function
_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_:
.LFB2827:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2827
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	movq	%rdx, %r13
	.cfi_offset 13, -40
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	pushq	%r12
	movq	%rcx, %r12
	.cfi_offset 12, -48
	pushq	%rbx
	andq	$-32, %rsp
	subq	$160, %rsp
	movq	%rdi, 104(%rsp)
	movl	$144, %edi
	movq	%rsi, 48(%rsp)
.LEHB27:
	.cfi_offset 3, -56
	call	_Znwm
.LEHE27:
	movq	%rax, %rbx
	andl	$31, %eax
	shrq	$3, %rax
	negq	%rax
	movq	%rax, %rdi
	movq	%rbx, %rax
	andl	$3, %edi
	je	.L757
	movl	$18, %ecx
	movl	$19, %esi
	jmp	.L699
	.p2align 4,,10
	.p2align 3
.L758:
	movq	%rdx, %rcx
.L699:
	movq	%rsi, %r8
	movq	$0, (%rax)
	addq	$8, %rax
	subq	%rcx, %r8
	leaq	-1(%rcx), %rdx
	cmpq	%r8, %rdi
	ja	.L758
	jmp	.L698
.L757:
	movl	$18, %edx
.L698:
	movl	$18, %esi
	subq	%rdi, %rsi
	movq	%rsi, %r9
	shrq	$2, %r9
	leaq	0(,%r9,4), %rcx
	testq	%rcx, %rcx
	je	.L772
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rbx,%rdi,8), %r8
	xorl	%edi, %edi
.L701:
	addq	$1, %rdi
	vmovapd	%ymm0, (%r8)
	addq	$32, %r8
	cmpq	%r9, %rdi
	jb	.L701
	subq	%rcx, %rdx
	cmpq	%rcx, %rsi
	leaq	(%rax,%rcx,8), %rax
	je	.L702
.L772:
	movq	$0, (%rax)
	addq	$8, %rax
	subq	$1, %rdx
	jne	.L772
.L702:
	movq	104(%rsp), %rdx
	movq	104(%rsp), %rsi
	leaq	112(%rsp), %rdi
	movq	(%rdx), %rax
	movq	%r13, %rdx
	vzeroupper
.LEHB28:
	call	*120(%rax)
.LEHE28:
	movq	104(%rsp), %rcx
	leaq	128(%rsp), %rdi
	movq	%r12, %rdx
	movq	(%rcx), %rax
	movq	%rcx, %rsi
.LEHB29:
	call	*120(%rax)
.LEHE29:
	movq	104(%rsp), %rsi
	movl	28(%rsi), %eax
	testl	%eax, %eax
	jle	.L704
	movq	%rbx, %r8
	movq	$_ZN12Dirac_Wilson6mult_pE+8, 32(%rsp)
	movl	$0, 88(%rsp)
	andl	$31, %r8d
	shrq	$3, %r8
	negq	%r8
	andl	$3, %r8d
	movq	%r8, 64(%rsp)
.L738:
	movq	104(%rsp), %rax
	movq	80(%rax), %r12
	movq	%r12, 144(%rsp)
	salq	$3, %r12
	movq	%r12, %rdi
.LEHB30:
	call	_Znwm
.LEHE30:
	addq	%rax, %r12
	movq	%rax, 152(%rsp)
	cmpq	%r12, %rax
	je	.L705
	leaq	8(%rax), %rdx
	movq	%rax, %r9
	movq	%r12, %r8
	andl	$31, %r9d
	subq	%rdx, %r8
	shrq	$3, %r9
	movq	%rax, %rdx
	shrq	$3, %r8
	negq	%r9
	addq	$1, %r8
	andl	$3, %r9d
	cmpq	%r9, %r8
	cmovbe	%r8, %r9
	testq	%r9, %r9
	je	.L706
	xorl	%ecx, %ecx
.L707:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rcx, %r9
	ja	.L707
	cmpq	%r9, %r8
	je	.L705
.L706:
	subq	%r9, %r8
	movq	%r8, %rsi
	shrq	$2, %rsi
	leaq	0(,%rsi,4), %rdi
	testq	%rdi, %rdi
	je	.L771
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%r9,8), %rcx
	xorl	%eax, %eax
.L710:
	addq	$1, %rax
	vmovapd	%ymm0, (%rcx)
	addq	$32, %rcx
	cmpq	%rsi, %rax
	jb	.L710
	cmpq	%rdi, %r8
	leaq	(%rdx,%rdi,8), %rdx
	je	.L705
.L771:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r12
	jne	.L771
.L705:
	movq	32(%rsp), %rdx
	movq	-8(%rdx), %rax
	testb	$1, %al
	je	.L789
	movq	104(%rsp), %rdi
	addq	(%rdx), %rdi
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L714:
	leaq	128(%rsp), %rdx
	leaq	144(%rsp), %rsi
	vzeroupper
.LEHB31:
	call	*%rax
	movq	104(%rsp), %rsi
	xorl	%r12d, %r12d
	movl	$0, 92(%rsp)
	movl	24(%rsi), %r15d
	testl	%r15d, %r15d
	jle	.L716
	movq	$18, 56(%rsp)
	movq	64(%rsp), %rdx
	subq	%rdx, 56(%rsp)
	movq	56(%rsp), %rcx
	shrq	$2, %rcx
	movq	%rcx, 40(%rsp)
	salq	$2, %rcx
	movq	%rcx, 72(%rsp)
	.p2align 4,,10
	.p2align 3
.L784:
	cmpq	$0, 64(%rsp)
	movq	%rbx, %rax
	je	.L763
	movl	$18, %ecx
	movl	$19, %esi
	movq	64(%rsp), %rdi
	jmp	.L734
	.p2align 4,,10
	.p2align 3
.L764:
	movq	%rdx, %rcx
.L734:
	movq	%rsi, %r8
	movq	$0, (%rax)
	addq	$8, %rax
	subq	%rcx, %r8
	leaq	-1(%rcx), %rdx
	cmpq	%r8, %rdi
	ja	.L764
.L733:
	cmpq	$0, 72(%rsp)
	je	.L770
	movq	64(%rsp), %rcx
	vxorpd	%xmm0, %xmm0, %xmm0
	movq	40(%rsp), %r8
	leaq	(%rbx,%rcx,8), %rdi
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L717:
	movq	%rcx, %rsi
	addq	$1, %rcx
	salq	$5, %rsi
	cmpq	%rcx, %r8
	vmovapd	%ymm0, (%rdi,%rsi)
	ja	.L717
	movq	72(%rsp), %r8
	subq	%r8, %rdx
	cmpq	%r8, 56(%rsp)
	leaq	(%rax,%r8,8), %rax
	je	.L718
	.p2align 4,,10
	.p2align 3
.L770:
	movq	$0, (%rax)
	addq	$8, %rax
	subq	$1, %rdx
	jne	.L770
.L718:
	movq	104(%rsp), %rax
	movq	152(%rsp), %rsi
	movq	120(%rsp), %r9
	movl	$0, 100(%rsp)
	movq	$0, 80(%rsp)
	movl	$0, 96(%rsp)
	movl	32(%rax), %r8d
.L720:
	movq	80(%rsp), %r15
	movl	96(%rsp), %r14d
	xorl	%r13d, %r13d
.L725:
	movq	%r12, 24(%rsp)
	testl	%r8d, %r8d
	vmovsd	24(%rsp), %xmm5
	vmovapd	%xmm5, %xmm4
	jle	.L723
	movq	104(%rsp), %rdx
	movl	92(%rsp), %r10d
	leaq	(%r9,%r15), %r11
	xorl	%ecx, %ecx
	imull	52(%rdx), %r10d
	movslq	%r10d, %rdx
	leal	(%r10,%r13), %edi
	addl	100(%rsp), %r10d
	addq	%r13, %rdx
	movslq	%edi, %rdi
	salq	$3, %rdx
	leaq	8(,%rdi,8), %rax
	movslq	%r10d, %r10
	subq	%rdi, %r10
	salq	$3, %r10
	.p2align 4,,10
	.p2align 3
.L721:
	leaq	(%r10,%rax), %rdi
	vmovsd	(%rsi,%rdx), %xmm3
	vmovsd	(%r11,%rdx), %xmm1
	addl	$1, %ecx
	vmovsd	(%rsi,%rax), %xmm0
	addq	$48, %rdx
	vmovsd	(%r9,%rdi), %xmm2
	addq	$48, %rax
	vmulsd	%xmm1, %xmm3, %xmm7
	cmpl	%r8d, %ecx
	vmulsd	%xmm2, %xmm0, %xmm6
	vmulsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm0, %xmm2, %xmm0
	vsubsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm5, %xmm5
	jne	.L721
.L723:
	leal	(%r14,%r14), %eax
	addq	$2, %r13
	addl	$1, %r14d
	subq	$16, %r15
	movslq	%eax, %rdx
	addl	$1, %eax
	cmpq	$6, %r13
	cltq
	vmovsd	%xmm4, (%rbx,%rdx,8)
	vmovsd	%xmm5, (%rbx,%rax,8)
	jne	.L725
	addl	$3, 96(%rsp)
	addq	$16, 80(%rsp)
	addl	$2, 100(%rsp)
	cmpl	$9, 96(%rsp)
	jne	.L720
	movq	104(%rsp), %rcx
	movq	160(%rcx), %rax
	movq	%rcx, %rdi
	addq	168(%rcx), %rdi
	testb	$1, %al
	je	.L727
	movq	(%rdi), %rdx
	movq	-1(%rdx,%rax), %rax
.L727:
	movl	92(%rsp), %esi
	vzeroupper
	call	*%rax
.LEHE31:
	movq	104(%rsp), %rsi
	movl	88(%rsp), %edx
	imull	56(%rsi), %edx
	movslq	68(%rsi), %rcx
	movq	48(%rsp), %rsi
	addl	%edx, %eax
	imull	%ecx, %eax
	cltq
	leaq	0(,%rax,8), %rdx
	addq	8(%rsi), %rdx
	leaq	(%rdx,%rcx,8), %rsi
	cmpq	%rdx, %rsi
	jbe	.L728
	movq	%rdx, %rax
	notq	%rax
	leaq	(%rax,%rsi), %r9
	shrq	$3, %r9
	addq	$1, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L762
	leaq	32(%rdx), %rax
	leaq	32(%rbx), %rcx
	cmpq	%rax, %rbx
	seta	%al
	cmpq	%rcx, %rdx
	seta	%cl
	orl	%ecx, %eax
	cmpq	$6, %r9
	seta	%cl
	testb	%cl, %al
	je	.L762
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L730:
	vmovupd	(%rbx,%rax), %xmm0
	addq	$1, %rcx
	vmovupd	(%rdx,%rax), %xmm1
	vinsertf128	$0x1, 16(%rbx,%rax), %ymm0, %ymm0
	vinsertf128	$0x1, 16(%rdx,%rax), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vmovupd	%xmm0, (%rdx,%rax)
	vextractf128	$0x1, %ymm0, 16(%rdx,%rax)
	addq	$32, %rax
	cmpq	%rcx, %rdi
	ja	.L730
	leaq	0(,%r8,8), %rax
	addq	%rax, %rdx
	addq	%rbx, %rax
	cmpq	%r8, %r9
	je	.L728
	.p2align 4,,10
	.p2align 3
.L769:
	vmovsd	(%rdx), %xmm0
	vaddsd	(%rax), %xmm0, %xmm0
	addq	$8, %rax
	vmovsd	%xmm0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %rsi
	ja	.L769
.L728:
	movq	104(%rsp), %r8
	addl	$1, 92(%rsp)
	movl	92(%rsp), %eax
	cmpl	%eax, 24(%r8)
	jg	.L784
.L716:
	movq	152(%rsp), %rdi
	vzeroupper
	call	_ZdlPv
	movq	104(%rsp), %rsi
	addl	$1, 88(%rsp)
	addq	$16, 32(%rsp)
	movl	88(%rsp), %r8d
	cmpl	%r8d, 28(%rsi)
	jg	.L738
.L704:
	movq	136(%rsp), %rdi
	call	_ZdlPv
	movq	120(%rsp), %rdi
	call	_ZdlPv
	movq	%rbx, %rdi
	call	_ZdlPv
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L789:
	.cfi_restore_state
	movq	104(%rsp), %rdi
	addq	(%rdx), %rdi
	jmp	.L714
.L768:
	movq	152(%rsp), %rdi
	movq	%rax, %r12
	call	_ZdlPv
.L748:
	movq	136(%rsp), %rdi
	call	_ZdlPv
.L751:
	movq	120(%rsp), %rdi
	call	_ZdlPv
.L754:
	movq	%rbx, %rdi
	call	_ZdlPv
	movq	%r12, %rdi
.LEHB32:
	call	_Unwind_Resume
.LEHE32:
.L762:
	movq	%rbx, %rax
	jmp	.L769
.L763:
	movl	$18, %edx
	jmp	.L733
.L767:
	movq	%rax, %r12
	jmp	.L748
.L766:
	movq	%rax, %r12
	jmp	.L751
.L765:
	movq	%rax, %r12
	.p2align 4,,2
	jmp	.L754
	.cfi_endproc
.LFE2827:
	.section	.gcc_except_table
.LLSDA2827:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2827-.LLSDACSB2827
.LLSDACSB2827:
	.uleb128 .LEHB27-.LFB2827
	.uleb128 .LEHE27-.LEHB27
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB28-.LFB2827
	.uleb128 .LEHE28-.LEHB28
	.uleb128 .L765-.LFB2827
	.uleb128 0
	.uleb128 .LEHB29-.LFB2827
	.uleb128 .LEHE29-.LEHB29
	.uleb128 .L766-.LFB2827
	.uleb128 0
	.uleb128 .LEHB30-.LFB2827
	.uleb128 .LEHE30-.LEHB30
	.uleb128 .L767-.LFB2827
	.uleb128 0
	.uleb128 .LEHB31-.LFB2827
	.uleb128 .LEHE31-.LEHB31
	.uleb128 .L768-.LFB2827
	.uleb128 0
	.uleb128 .LEHB32-.LFB2827
	.uleb128 .LEHE32-.LEHB32
	.uleb128 0
	.uleb128 0
.LLSDACSE2827:
	.text
	.size	_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_, .-_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_
	.align 2
	.p2align 4,,15
	.globl	_ZNK12Dirac_Wilson8md_forceERK5FieldS2_
	.type	_ZNK12Dirac_Wilson8md_forceERK5FieldS2_, @function
_ZNK12Dirac_Wilson8md_forceERK5FieldS2_:
.LFB2828:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	.cfi_lsda 0x3,.LLSDA2828
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	movq	%rdx, %r14
	.cfi_offset 14, -32
	.cfi_offset 15, -24
	pushq	%r13
	movq	%rcx, %r13
	.cfi_offset 13, -40
	pushq	%r12
	movq	%rsi, %r12
	.cfi_offset 12, -48
	pushq	%rbx
	movl	56(%rsi), %r15d
	movq	%rdi, %rbx
	.cfi_offset 3, -56
	imull	68(%rsi), %r15d
	andq	$-32, %rsp
	imull	72(%rsi), %r15d
	movslq	%r15d, %r15
	movq	%r15, (%rdi)
	salq	$3, %r15
	movq	%r15, %rdi
.LEHB33:
	call	_Znwm
.LEHE33:
	addq	%rax, %r15
	movq	%rax, 8(%rbx)
	cmpq	%r15, %rax
	je	.L791
	leaq	8(%rax), %rdx
	movq	%rax, %rsi
	movq	%r15, %rdi
	andl	$31, %esi
	subq	%rdx, %rdi
	shrq	$3, %rsi
	movq	%rax, %rdx
	shrq	$3, %rdi
	negq	%rsi
	addq	$1, %rdi
	andl	$3, %esi
	cmpq	%rsi, %rdi
	cmovbe	%rdi, %rsi
	testq	%rsi, %rsi
	je	.L792
	xorl	%ecx, %ecx
	.p2align 4,,10
	.p2align 3
.L793:
	addq	$1, %rcx
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rsi, %rcx
	jb	.L793
	cmpq	%rsi, %rdi
	je	.L791
.L792:
	movq	%rdi, %r9
	subq	%rsi, %r9
	movq	%r9, %rdi
	shrq	$2, %rdi
	leaq	0(,%rdi,4), %r8
	testq	%r8, %r8
	je	.L814
	vxorpd	%xmm0, %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rcx
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L796:
	addq	$1, %rax
	vmovapd	%ymm0, (%rcx)
	addq	$32, %rcx
	cmpq	%rdi, %rax
	jb	.L796
	cmpq	%r8, %r9
	leaq	(%rdx,%r8,8), %rdx
	je	.L791
	.p2align 4,,10
	.p2align 3
.L814:
	movq	$0, (%rdx)
	addq	$8, %rdx
	cmpq	%rdx, %r15
	jne	.L814
.L791:
	movq	%r13, %rcx
	movq	%r14, %rdx
	movq	%rbx, %rsi
	movq	%r12, %rdi
	vzeroupper
.LEHB34:
	call	_ZNK12Dirac_Wilson10md_force_pER5FieldRKS0_S3_
	movq	%r13, %rcx
	movq	%r14, %rdx
	movq	%rbx, %rsi
	movq	%r12, %rdi
	call	_ZNK12Dirac_Wilson10md_force_mER5FieldRKS0_S3_
.LEHE34:
	movq	8(%rbx), %rsi
	movq	(%rbx), %rax
	vmovsd	16(%r12), %xmm0
	vmovsd	.LC2(%rip), %xmm1
	leaq	(%rsi,%rax,8), %rdi
	vxorpd	%xmm1, %xmm0, %xmm0
	cmpq	%rdi, %rsi
	jae	.L790
	leaq	1(%rsi), %rax
	leaq	8(%rsi), %rdx
	movl	$1, %r8d
	cmpq	%rdi, %rax
	jbe	.L821
.L801:
	movq	%rsi, %rcx
	movq	%rsi, %rax
	andl	$31, %ecx
	shrq	$3, %rcx
	negq	%rcx
	andl	$3, %ecx
	cmpq	%rcx, %r8
	cmovbe	%r8, %rcx
	testq	%rcx, %rcx
	je	.L802
	xorl	%edx, %edx
	.p2align 4,,10
	.p2align 3
.L803:
	vmulsd	(%rax), %xmm0, %xmm1
	addq	$1, %rdx
	vmovsd	%xmm1, (%rax)
	addq	$8, %rax
	cmpq	%rdx, %rcx
	ja	.L803
	cmpq	%rcx, %r8
	je	.L790
.L802:
	movq	%r8, %r10
	subq	%rcx, %r10
	movq	%r10, %r8
	shrq	$2, %r8
	leaq	0(,%r8,4), %r9
	testq	%r9, %r9
	je	.L813
	vmovddup	%xmm0, %xmm2
	leaq	(%rsi,%rcx,8), %rsi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	vinsertf128	$1, %xmm2, %ymm2, %ymm2
	.p2align 4,,10
	.p2align 3
.L805:
	vmulpd	(%rsi,%rdx), %ymm2, %ymm1
	addq	$1, %rcx
	vmovapd	%ymm1, (%rsi,%rdx)
	addq	$32, %rdx
	cmpq	%r8, %rcx
	jb	.L805
	cmpq	%r9, %r10
	leaq	(%rax,%r9,8), %rax
	je	.L790
	.p2align 4,,10
	.p2align 3
.L813:
	vmulsd	(%rax), %xmm0, %xmm1
	vmovsd	%xmm1, (%rax)
	addq	$8, %rax
	cmpq	%rax, %rdi
	ja	.L813
.L790:
	leaq	-40(%rbp), %rsp
	movq	%rbx, %rax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	vzeroupper
	ret
	.p2align 4,,10
	.p2align 3
.L821:
	.cfi_restore_state
	movq	%rdi, %r8
	subq	%rdx, %r8
	addq	$7, %r8
	shrq	$3, %r8
	addq	$1, %r8
	jmp	.L801
.L812:
	movq	8(%rbx), %rdi
	movq	%rax, %r12
	call	_ZdlPv
	movq	%r12, %rdi
.LEHB35:
	call	_Unwind_Resume
.LEHE35:
	.cfi_endproc
.LFE2828:
	.section	.gcc_except_table
.LLSDA2828:
	.byte	0xff
	.byte	0xff
	.byte	0x1
	.uleb128 .LLSDACSE2828-.LLSDACSB2828
.LLSDACSB2828:
	.uleb128 .LEHB33-.LFB2828
	.uleb128 .LEHE33-.LEHB33
	.uleb128 0
	.uleb128 0
	.uleb128 .LEHB34-.LFB2828
	.uleb128 .LEHE34-.LEHB34
	.uleb128 .L812-.LFB2828
	.uleb128 0
	.uleb128 .LEHB35-.LFB2828
	.uleb128 .LEHE35-.LEHB35
	.uleb128 0
	.uleb128 0
.LLSDACSE2828:
	.text
	.size	_ZNK12Dirac_Wilson8md_forceERK5FieldS2_, .-_ZNK12Dirac_Wilson8md_forceERK5FieldS2_
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC3:
	.string	"mass"
	.text
	.p2align 4,,15
	.globl	_ZN2Dw9read_massERKN4pugi8xml_nodeE
	.type	_ZN2Dw9read_massERKN4pugi8xml_nodeE, @function
_ZN2Dw9read_massERKN4pugi8xml_nodeE:
.LFB2830:
	.cfi_startproc
	subq	$24, %rsp
	.cfi_def_cfa_offset 32
	movq	(%rdi), %rdi
	xorl	%ecx, %ecx
	leaq	8(%rsp), %rdx
	movl	$.LC3, %esi
	call	_ZN3XML4readEN4pugi8xml_nodeEPKcRdb
	vmovsd	8(%rsp), %xmm0
	addq	$24, %rsp
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE2830:
	.size	_ZN2Dw9read_massERKN4pugi8xml_nodeE, .-_ZN2Dw9read_massERKN4pugi8xml_nodeE
	.section	.text.startup,"ax",@progbits
	.p2align 4,,15
	.type	_GLOBAL__sub_I__ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_, @function
_GLOBAL__sub_I__ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_:
.LFB3234:
	.cfi_startproc
	subq	$8, %rsp
	.cfi_def_cfa_offset 16
	movl	$_ZStL8__ioinit, %edi
	call	_ZNSt8ios_base4InitC1Ev
	movl	$__dso_handle, %edx
	movl	$_ZStL8__ioinit, %esi
	movl	$_ZNSt8ios_base4InitD1Ev, %edi
	addq	$8, %rsp
	.cfi_def_cfa_offset 8
	jmp	__cxa_atexit
	.cfi_endproc
.LFE3234:
	.size	_GLOBAL__sub_I__ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_, .-_GLOBAL__sub_I__ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_
	.section	.ctors,"aw",@progbits
	.align 8
	.quad	_GLOBAL__sub_I__ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_
	.weak	_ZTS12Dirac_Wilson
	.section	.rodata._ZTS12Dirac_Wilson,"aG",@progbits,_ZTS12Dirac_Wilson,comdat
	.type	_ZTS12Dirac_Wilson, @object
	.size	_ZTS12Dirac_Wilson, 15
_ZTS12Dirac_Wilson:
	.string	"12Dirac_Wilson"
	.weak	_ZTI12Dirac_Wilson
	.section	.rodata._ZTI12Dirac_Wilson,"aG",@progbits,_ZTI12Dirac_Wilson,comdat
	.align 16
	.type	_ZTI12Dirac_Wilson, @object
	.size	_ZTI12Dirac_Wilson, 24
_ZTI12Dirac_Wilson:
	.quad	_ZTVN10__cxxabiv120__si_class_type_infoE+16
	.quad	_ZTS12Dirac_Wilson
	.quad	_ZTI15DiracWilsonLike
	.weak	_ZTV12Dirac_Wilson
	.section	.rodata._ZTV12Dirac_Wilson,"aG",@progbits,_ZTV12Dirac_Wilson,comdat
	.align 32
	.type	_ZTV12Dirac_Wilson, @object
	.size	_ZTV12Dirac_Wilson, 152
_ZTV12Dirac_Wilson:
	.quad	0
	.quad	_ZTI12Dirac_Wilson
	.quad	_ZN12Dirac_WilsonD1Ev
	.quad	_ZN12Dirac_WilsonD0Ev
	.quad	_ZNK12Dirac_Wilson5fsizeEv
	.quad	_ZNK12Dirac_Wilson5gsizeEv
	.quad	_ZNK12Dirac_Wilson9get_gsiteEv
	.quad	_ZNK12Dirac_Wilson4multERK5Field
	.quad	_ZNK12Dirac_Wilson8mult_dagERK5Field
	.quad	_ZNK12Dirac_Wilson9mult_precERK5Field
	.quad	_ZNK12Dirac_Wilson13mult_dag_precERK5Field
	.quad	_ZNK12Dirac_Wilson9left_precERK5Field
	.quad	_ZNK12Dirac_Wilson10right_precERK5Field
	.quad	_ZNK12Dirac_Wilson13left_dag_precERK5Field
	.quad	_ZNK12Dirac_Wilson14right_dag_precERK5Field
	.quad	_ZNK12Dirac_Wilson8md_forceERK5FieldS2_
	.quad	_ZN12Dirac_Wilson21update_internal_stateEv
	.quad	_ZNK12Dirac_Wilson6gamma5ERK5Field
	.quad	_ZNK12Dirac_Wilson17get_fermionFormatEv
	.globl	_ZN12Dirac_Wilson6mult_mE
	.data
	.align 32
	.type	_ZN12Dirac_Wilson6mult_mE, @object
	.size	_ZN12Dirac_Wilson6mult_mE, 64
_ZN12Dirac_Wilson6mult_mE:
	.quad	_ZNK12Dirac_Wilson7mult_xmER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_ymER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_zmER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_tmER5FieldRKS0_
	.quad	0
	.globl	_ZN12Dirac_Wilson6mult_pE
	.align 32
	.type	_ZN12Dirac_Wilson6mult_pE, @object
	.size	_ZN12Dirac_Wilson6mult_pE, 64
_ZN12Dirac_Wilson6mult_pE:
	.quad	_ZNK12Dirac_Wilson7mult_xpER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_ypER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_zpER5FieldRKS0_
	.quad	0
	.quad	_ZNK12Dirac_Wilson7mult_tpER5FieldRKS0_
	.quad	0
	.weak	_ZTV15DiracWilsonLike
	.section	.rodata._ZTV15DiracWilsonLike,"aG",@progbits,_ZTV15DiracWilsonLike,comdat
	.align 32
	.type	_ZTV15DiracWilsonLike, @object
	.size	_ZTV15DiracWilsonLike, 152
_ZTV15DiracWilsonLike:
	.quad	0
	.quad	_ZTI15DiracWilsonLike
	.quad	_ZN15DiracWilsonLikeD1Ev
	.quad	_ZN15DiracWilsonLikeD0Ev
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.weak	_ZTV5Dirac
	.section	.rodata._ZTV5Dirac,"aG",@progbits,_ZTV5Dirac,comdat
	.align 32
	.type	_ZTV5Dirac, @object
	.size	_ZTV5Dirac, 136
_ZTV5Dirac:
	.quad	0
	.quad	_ZTI5Dirac
	.quad	_ZN5DiracD1Ev
	.quad	_ZN5DiracD0Ev
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.quad	__cxa_pure_virtual
	.local	_ZStL8__ioinit
	.comm	_ZStL8__ioinit,1,1
	.weak	_ZTI15DiracWilsonLike
	.section	.rodata._ZTI15DiracWilsonLike,"aG",@progbits,_ZTI15DiracWilsonLike,comdat
	.align 16
	.type	_ZTI15DiracWilsonLike, @object
	.size	_ZTI15DiracWilsonLike, 24
_ZTI15DiracWilsonLike:
	.quad	_ZTVN10__cxxabiv120__si_class_type_infoE+16
	.quad	_ZTS15DiracWilsonLike
	.quad	_ZTI5Dirac
	.weak	_ZTI5Dirac
	.section	.rodata._ZTI5Dirac,"aG",@progbits,_ZTI5Dirac,comdat
	.align 16
	.type	_ZTI5Dirac, @object
	.size	_ZTI5Dirac, 16
_ZTI5Dirac:
	.quad	_ZTVN10__cxxabiv117__class_type_infoE+16
	.quad	_ZTS5Dirac
	.weak	_ZTS15DiracWilsonLike
	.section	.rodata._ZTS15DiracWilsonLike,"aG",@progbits,_ZTS15DiracWilsonLike,comdat
	.align 16
	.type	_ZTS15DiracWilsonLike, @object
	.size	_ZTS15DiracWilsonLike, 18
_ZTS15DiracWilsonLike:
	.string	"15DiracWilsonLike"
	.weak	_ZTS5Dirac
	.section	.rodata._ZTS5Dirac,"aG",@progbits,_ZTS5Dirac,comdat
	.type	_ZTS5Dirac, @object
	.size	_ZTS5Dirac, 7
_ZTS5Dirac:
	.string	"5Dirac"
	.weakref	_ZL20__gthrw_pthread_oncePiPFvvE,pthread_once
	.weakref	_ZL27__gthrw_pthread_getspecificj,pthread_getspecific
	.weakref	_ZL27__gthrw_pthread_setspecificjPKv,pthread_setspecific
	.weakref	_ZL22__gthrw_pthread_createPmPK14pthread_attr_tPFPvS3_ES3_,pthread_create
	.weakref	_ZL20__gthrw_pthread_joinmPPv,pthread_join
	.weakref	_ZL21__gthrw_pthread_equalmm,pthread_equal
	.weakref	_ZL20__gthrw_pthread_selfv,pthread_self
	.weakref	_ZL22__gthrw_pthread_detachm,pthread_detach
	.weakref	_ZL22__gthrw_pthread_cancelm,pthread_cancel
	.weakref	_ZL19__gthrw_sched_yieldv,sched_yield
	.weakref	_ZL26__gthrw_pthread_mutex_lockP15pthread_mutex_t,pthread_mutex_lock
	.weakref	_ZL29__gthrw_pthread_mutex_trylockP15pthread_mutex_t,pthread_mutex_trylock
	.weakref	_ZL31__gthrw_pthread_mutex_timedlockP15pthread_mutex_tPK8timespec,pthread_mutex_timedlock
	.weakref	_ZL28__gthrw_pthread_mutex_unlockP15pthread_mutex_t,pthread_mutex_unlock
	.weakref	_ZL26__gthrw_pthread_mutex_initP15pthread_mutex_tPK19pthread_mutexattr_t,pthread_mutex_init
	.weakref	_ZL29__gthrw_pthread_mutex_destroyP15pthread_mutex_t,pthread_mutex_destroy
	.weakref	_ZL30__gthrw_pthread_cond_broadcastP14pthread_cond_t,pthread_cond_broadcast
	.weakref	_ZL27__gthrw_pthread_cond_signalP14pthread_cond_t,pthread_cond_signal
	.weakref	_ZL25__gthrw_pthread_cond_waitP14pthread_cond_tP15pthread_mutex_t,pthread_cond_wait
	.weakref	_ZL30__gthrw_pthread_cond_timedwaitP14pthread_cond_tP15pthread_mutex_tPK8timespec,pthread_cond_timedwait
	.weakref	_ZL28__gthrw_pthread_cond_destroyP14pthread_cond_t,pthread_cond_destroy
	.weakref	_ZL26__gthrw_pthread_key_createPjPFvPvE,pthread_key_create
	.weakref	_ZL26__gthrw_pthread_key_deletej,pthread_key_delete
	.weakref	_ZL30__gthrw_pthread_mutexattr_initP19pthread_mutexattr_t,pthread_mutexattr_init
	.weakref	_ZL33__gthrw_pthread_mutexattr_settypeP19pthread_mutexattr_ti,pthread_mutexattr_settype
	.weakref	_ZL33__gthrw_pthread_mutexattr_destroyP19pthread_mutexattr_t,pthread_mutexattr_destroy
	.weak	_ZN5DiracD1Ev
	.set	_ZN5DiracD1Ev,_ZN5DiracD2Ev
	.weak	_ZN15DiracWilsonLikeD1Ev
	.set	_ZN15DiracWilsonLikeD1Ev,_ZN15DiracWilsonLikeD2Ev
	.weak	_ZN12Dirac_WilsonD1Ev
	.set	_ZN12Dirac_WilsonD1Ev,_ZN12Dirac_WilsonD2Ev
	.section	.rodata.cst8,"aM",@progbits,8
	.align 8
.LC1:
	.long	0
	.long	1071644672
	.section	.rodata.cst16,"aM",@progbits,16
	.align 16
.LC2:
	.long	0
	.long	-2147483648
	.long	0
	.long	0
	.ident	"GCC: (Ubuntu/Linaro 4.6.1-9ubuntu3) 4.6.1"
	.section	.note.GNU-stack,"",@progbits
